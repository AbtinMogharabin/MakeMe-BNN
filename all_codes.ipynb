{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBXHs2m0APxl"
      },
      "source": [
        "# Implemenation of Make Me a BNN\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NW9xd8Xc3UqB"
      },
      "source": [
        "## Import libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Install Missing Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Wvc77o82xGwt"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: netcal in /home/abduallah_damash/project/ceng502/venv/lib/python3.10/site-packages (1.3.5)\n",
            "Requirement already satisfied: numpy>=1.18 in /home/abduallah_damash/project/ceng502/venv/lib/python3.10/site-packages (from netcal) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.4 in /home/abduallah_damash/project/ceng502/venv/lib/python3.10/site-packages (from netcal) (1.13.1)\n",
            "Requirement already satisfied: matplotlib>=3.3 in /home/abduallah_damash/project/ceng502/venv/lib/python3.10/site-packages (from netcal) (3.9.0)\n",
            "Requirement already satisfied: scikit-learn>=0.24 in /home/abduallah_damash/project/ceng502/venv/lib/python3.10/site-packages (from netcal) (1.5.0)\n",
            "Requirement already satisfied: torch>=1.9 in /home/abduallah_damash/project/ceng502/venv/lib/python3.10/site-packages (from netcal) (2.3.0)\n",
            "Requirement already satisfied: torchvision>=0.10.0 in /home/abduallah_damash/project/ceng502/venv/lib/python3.10/site-packages (from netcal) (0.18.0)\n",
            "Requirement already satisfied: tqdm>=4.40 in /home/abduallah_damash/project/ceng502/venv/lib/python3.10/site-packages (from netcal) (4.66.4)\n",
            "Requirement already satisfied: pyro-ppl>=1.8 in /home/abduallah_damash/project/ceng502/venv/lib/python3.10/site-packages (from netcal) (1.9.0)\n",
            "Requirement already satisfied: tikzplotlib==0.9.8 in /home/abduallah_damash/project/ceng502/venv/lib/python3.10/site-packages (from netcal) (0.9.8)\n",
            "Requirement already satisfied: tensorboard>=2.2 in /home/abduallah_damash/project/ceng502/venv/lib/python3.10/site-packages (from netcal) (2.16.2)\n",
            "Requirement already satisfied: gpytorch>=1.5.1 in /home/abduallah_damash/project/ceng502/venv/lib/python3.10/site-packages (from netcal) (1.11)\n",
            "Requirement already satisfied: Pillow in /home/abduallah_damash/project/ceng502/venv/lib/python3.10/site-packages (from tikzplotlib==0.9.8->netcal) (10.3.0)\n",
            "Requirement already satisfied: linear-operator>=0.5.0 in /home/abduallah_damash/project/ceng502/venv/lib/python3.10/site-packages (from gpytorch>=1.5.1->netcal) (0.5.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /home/abduallah_damash/project/ceng502/venv/lib/python3.10/site-packages (from matplotlib>=3.3->netcal) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /home/abduallah_damash/project/ceng502/venv/lib/python3.10/site-packages (from matplotlib>=3.3->netcal) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /home/abduallah_damash/project/ceng502/venv/lib/python3.10/site-packages (from matplotlib>=3.3->netcal) (4.52.4)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /home/abduallah_damash/project/ceng502/venv/lib/python3.10/site-packages (from matplotlib>=3.3->netcal) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/abduallah_damash/project/ceng502/venv/lib/python3.10/site-packages (from matplotlib>=3.3->netcal) (24.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /home/abduallah_damash/project/ceng502/venv/lib/python3.10/site-packages (from matplotlib>=3.3->netcal) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /home/abduallah_damash/project/ceng502/venv/lib/python3.10/site-packages (from matplotlib>=3.3->netcal) (2.9.0.post0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /home/abduallah_damash/project/ceng502/venv/lib/python3.10/site-packages (from pyro-ppl>=1.8->netcal) (3.3.0)\n",
            "Requirement already satisfied: pyro-api>=0.1.1 in /home/abduallah_damash/project/ceng502/venv/lib/python3.10/site-packages (from pyro-ppl>=1.8->netcal) (0.1.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /home/abduallah_damash/project/ceng502/venv/lib/python3.10/site-packages (from scikit-learn>=0.24->netcal) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/abduallah_damash/project/ceng502/venv/lib/python3.10/site-packages (from scikit-learn>=0.24->netcal) (3.5.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /home/abduallah_damash/project/ceng502/venv/lib/python3.10/site-packages (from tensorboard>=2.2->netcal) (2.1.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /home/abduallah_damash/project/ceng502/venv/lib/python3.10/site-packages (from tensorboard>=2.2->netcal) (1.64.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /home/abduallah_damash/project/ceng502/venv/lib/python3.10/site-packages (from tensorboard>=2.2->netcal) (3.6)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /home/abduallah_damash/project/ceng502/venv/lib/python3.10/site-packages (from tensorboard>=2.2->netcal) (5.27.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /home/abduallah_damash/project/ceng502/venv/lib/python3.10/site-packages (from tensorboard>=2.2->netcal) (68.2.0)\n",
            "Requirement already satisfied: six>1.9 in /home/abduallah_damash/project/ceng502/venv/lib/python3.10/site-packages (from tensorboard>=2.2->netcal) (1.16.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/abduallah_damash/project/ceng502/venv/lib/python3.10/site-packages (from tensorboard>=2.2->netcal) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /home/abduallah_damash/project/ceng502/venv/lib/python3.10/site-packages (from tensorboard>=2.2->netcal) (3.0.3)\n",
            "Requirement already satisfied: filelock in /home/abduallah_damash/project/ceng502/venv/lib/python3.10/site-packages (from torch>=1.9->netcal) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /home/abduallah_damash/project/ceng502/venv/lib/python3.10/site-packages (from torch>=1.9->netcal) (4.12.0)\n",
            "Requirement already satisfied: sympy in /home/abduallah_damash/project/ceng502/venv/lib/python3.10/site-packages (from torch>=1.9->netcal) (1.12)\n",
            "Requirement already satisfied: networkx in /home/abduallah_damash/project/ceng502/venv/lib/python3.10/site-packages (from torch>=1.9->netcal) (3.3)\n",
            "Requirement already satisfied: jinja2 in /home/abduallah_damash/project/ceng502/venv/lib/python3.10/site-packages (from torch>=1.9->netcal) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /home/abduallah_damash/project/ceng502/venv/lib/python3.10/site-packages (from torch>=1.9->netcal) (2024.5.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/abduallah_damash/project/ceng502/venv/lib/python3.10/site-packages (from torch>=1.9->netcal) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/abduallah_damash/project/ceng502/venv/lib/python3.10/site-packages (from torch>=1.9->netcal) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/abduallah_damash/project/ceng502/venv/lib/python3.10/site-packages (from torch>=1.9->netcal) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/abduallah_damash/project/ceng502/venv/lib/python3.10/site-packages (from torch>=1.9->netcal) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/abduallah_damash/project/ceng502/venv/lib/python3.10/site-packages (from torch>=1.9->netcal) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/abduallah_damash/project/ceng502/venv/lib/python3.10/site-packages (from torch>=1.9->netcal) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/abduallah_damash/project/ceng502/venv/lib/python3.10/site-packages (from torch>=1.9->netcal) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/abduallah_damash/project/ceng502/venv/lib/python3.10/site-packages (from torch>=1.9->netcal) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/abduallah_damash/project/ceng502/venv/lib/python3.10/site-packages (from torch>=1.9->netcal) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/abduallah_damash/project/ceng502/venv/lib/python3.10/site-packages (from torch>=1.9->netcal) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/abduallah_damash/project/ceng502/venv/lib/python3.10/site-packages (from torch>=1.9->netcal) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /home/abduallah_damash/project/ceng502/venv/lib/python3.10/site-packages (from torch>=1.9->netcal) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/abduallah_damash/project/ceng502/venv/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.9->netcal) (12.5.40)\n",
            "Requirement already satisfied: jaxtyping>=0.2.9 in /home/abduallah_damash/project/ceng502/venv/lib/python3.10/site-packages (from linear-operator>=0.5.0->gpytorch>=1.5.1->netcal) (0.2.29)\n",
            "Requirement already satisfied: typeguard~=2.13.3 in /home/abduallah_damash/project/ceng502/venv/lib/python3.10/site-packages (from linear-operator>=0.5.0->gpytorch>=1.5.1->netcal) (2.13.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/abduallah_damash/project/ceng502/venv/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard>=2.2->netcal) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /home/abduallah_damash/project/ceng502/venv/lib/python3.10/site-packages (from sympy->torch>=1.9->netcal) (1.3.0)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "Requirement already satisfied: torchsummary in /home/abduallah_damash/project/ceng502/venv/lib/python3.10/site-packages (1.5.1)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install netcal\n",
        "!pip install torchsummary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Import Used Libs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "REotBNCV3UqO"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import numpy as np\n",
        "from torchsummary import summary\n",
        "from torch.nn.functional import softmax\n",
        "import torch.nn.functional as F\n",
        "import netcal.metrics as metrics\n",
        "from netcal.metrics import ECE\n",
        "from sklearn.metrics import precision_recall_curve, auc, roc_auc_score, roc_curve\n",
        "from torch.utils.data import random_split\n",
        "import multiprocessing\n",
        "from sklearn.metrics import f1_score\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Import Custom Libs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "current dir is /home/abduallah_damash/project/ceng502/MakeMe-BNN\n",
            "ABNN dir is /home/abduallah_damash/project/ceng502/MakeMe-BNN/MakeMe-BNN/ABNN\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "# Get the current working directory of the notebook\n",
        "current_dir = os.path.dirname(os.path.abspath('__file__'))\n",
        "print(f\"current dir is {current_dir}\")\n",
        "# Add the ABNN and 'Simple CNN Demo' directories to the Python path\n",
        "abnn_dir = os.path.abspath(os.path.join(current_dir, './MakeMe-BNN/ABNN'))\n",
        "print(f\"ABNN dir is {abnn_dir}\")\n",
        "if abnn_dir not in sys.path:\n",
        "    sys.path.insert(0, abnn_dir)\n",
        "\n",
        "# Now import the necessary modules\n",
        "from ABNN import ABNN\n",
        "# from ABNN import ABNN, test_and_eval, train\n",
        "# from ABNN.datasets import dtd,imagenet,cifar10,cifar100,streethazards,svhn,bddanomaly,muad\n",
        "# from ABNN.deep_learning_models import resnet,resnet_diff_arc,wide_resnet18_10,vit,deeplabv3plus_resnet50"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Set the device Usage:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "multiprocessing.set_start_method('forkserver', force=True)\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "torch.backends.cudnn.benchmark = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AoXdUp79d8CL"
      },
      "source": [
        "##  Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFwNb7Gf3UqR"
      },
      "source": [
        "### Load and Augment CIFAR-10 and CIFAR100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uyQVyB3oCizi"
      },
      "source": [
        "Similar to the paper, only used:\n",
        "- Horizontal Flip for data augmentation.\n",
        "- Batch size as 128."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Load the CIFAR10 dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mSmx1nli3UqS",
        "outputId": "e4050327-9011-4833-8df4-9af7382bf900"
      },
      "outputs": [],
      "source": [
        "def prepare_cifar10_data(batch_size=128, train_split_ratio=0.8, num_workers=2):\n",
        "    \"\"\"\n",
        "    Prepares the CIFAR-10 dataset for training, validation, and testing.\n",
        "\n",
        "    Parameters:\n",
        "    - batch_size (int): The number of samples per batch to load. Default is 128.\n",
        "    - train_split_ratio (float): The proportion of the dataset to include in the train split. Default is 0.8 (80%).\n",
        "    - num_workers (int): How many subprocesses to use for data loading. Default is 2.\n",
        "\n",
        "    Returns:\n",
        "    - trainloader (DataLoader): DataLoader for the training set.\n",
        "    - validloader (DataLoader): DataLoader for the validation set.\n",
        "    - testloader (DataLoader): DataLoader for the test set.\n",
        "    \"\"\"\n",
        "    # Define the transform\n",
        "    transform = transforms.Compose([\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "    ])\n",
        "\n",
        "    # Load the CIFAR10 dataset\n",
        "    full_trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "\n",
        "    # Split the dataset into training and validation sets\n",
        "    train_size = int(train_split_ratio * len(full_trainset))  # 80% for training\n",
        "    valid_size = len(full_trainset) - train_size  # 20% for validation\n",
        "    train_subset, valid_subset = random_split(full_trainset, [train_size, valid_size])\n",
        "\n",
        "    # Create DataLoaders\n",
        "    trainloader = DataLoader(train_subset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
        "    validloader = DataLoader(valid_subset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
        "\n",
        "    # Load the test set\n",
        "    testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "    testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
        "\n",
        "    # Print the sizes of the datasets\n",
        "    print(f'CIFAR10 Training set size: {len(train_subset)}')\n",
        "    print(f'CIFAR10 Validation set size: {len(valid_subset)}')\n",
        "    print(f'CIFAR10 Test set size: {len(testset)}')\n",
        "\n",
        "    return trainloader, validloader, testloader\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     trainloader10, validloader10, testloader10 = prepare_cifar10_data()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Load the CIFAR100 dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def prepare_cifar100_data(batch_size=128, train_split_ratio=0.8, num_workers=2):\n",
        "    \"\"\"\n",
        "    Prepares the CIFAR-100 dataset for training, validation, and testing.\n",
        "\n",
        "    Parameters:\n",
        "    - batch_size (int): The number of samples per batch to load. Default is 128.\n",
        "    - train_split_ratio (float): The proportion of the dataset to include in the train split. Default is 0.8 (80%).\n",
        "    - num_workers (int): How many subprocesses to use for data loading. Default is 2.\n",
        "\n",
        "    Returns:\n",
        "    - trainloader (DataLoader): DataLoader for the training set.\n",
        "    - validloader (DataLoader): DataLoader for the validation set.\n",
        "    - testloader (DataLoader): DataLoader for the test set.\n",
        "    \"\"\"\n",
        "    # Define the transform\n",
        "    transform = transforms.Compose([\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "    ])\n",
        "\n",
        "    # Load the CIFAR100 dataset\n",
        "    full_trainset = datasets.CIFAR100(root='./data', train=True, download=True, transform=transform)\n",
        "\n",
        "    # Split the dataset into training and validation sets\n",
        "    train_size = int(train_split_ratio * len(full_trainset))  # 80% for training\n",
        "    valid_size = len(full_trainset) - train_size  # 20% for validation\n",
        "    train_subset, valid_subset = random_split(full_trainset, [train_size, valid_size])\n",
        "\n",
        "    # Create DataLoaders\n",
        "    trainloader = DataLoader(train_subset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
        "    validloader = DataLoader(valid_subset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
        "\n",
        "    # Load the test set\n",
        "    testset = datasets.CIFAR100(root='./data', train=False, download=True, transform=transform)\n",
        "    testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
        "\n",
        "    # Print the sizes of the datasets\n",
        "    print(f'CIFAR100 Training set size: {len(train_subset)}')\n",
        "    print(f'CIFAR100 Validation set size: {len(valid_subset)}')\n",
        "    print(f'CIFAR100 Test set size: {len(testset)}')\n",
        "\n",
        "    return trainloader, validloader, testloader\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     trainloader100, validloader100, testloader100 = prepare_cifar100_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def prepare_svhn_data(batch_size=128, train_split_ratio=0.8, num_workers=2):\n",
        "    \"\"\"\n",
        "    Prepares the SVHN dataset for training, validation, and testing.\n",
        "\n",
        "    Parameters:\n",
        "    - batch_size (int): The number of samples per batch to load. Default is 128.\n",
        "    - train_split_ratio (float): The proportion of the dataset to include in the train split. Default is 0.8 (80%).\n",
        "    - num_workers (int): How many subprocesses to use for data loading. Default is 2.\n",
        "\n",
        "    Returns:\n",
        "    - trainloader (DataLoader): DataLoader for the training set.\n",
        "    - validloader (DataLoader): DataLoader for the validation set.\n",
        "    - testloader (DataLoader): DataLoader for the test set.\n",
        "    \"\"\"\n",
        "    # Define the transform\n",
        "    transform = transforms.Compose([\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "    ])\n",
        "\n",
        "    # Load the SVHN dataset\n",
        "    full_trainset = datasets.SVHN(root='./data', split='train', download=True, transform=transform)\n",
        "\n",
        "    # Split the dataset into training and validation sets\n",
        "    train_size = int(train_split_ratio * len(full_trainset))  # 80% for training\n",
        "    valid_size = len(full_trainset) - train_size  # 20% for validation\n",
        "    train_subset, valid_subset = random_split(full_trainset, [train_size, valid_size])\n",
        "\n",
        "    # Create DataLoaders\n",
        "    trainloader = DataLoader(train_subset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
        "    validloader = DataLoader(valid_subset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
        "\n",
        "    # Load the test set\n",
        "    testset = datasets.SVHN(root='./data', split='test', download=True, transform=transform)\n",
        "    testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
        "\n",
        "    # Print the sizes of the datasets\n",
        "    print(f'SVHN Training set size: {len(train_subset)}')\n",
        "    print(f'SVHN Validation set size: {len(valid_subset)}')\n",
        "    print(f'SVHN Test set size: {len(testset)}')\n",
        "\n",
        "    return trainloader, validloader, testloader\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     trainloader_svhn, validloader_svhn, testloader_svhn = prepare_svhn_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AL7gMgK93UqT"
      },
      "source": [
        "## Define the DL model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define ResNen Model\n",
        "\n",
        "- It can be used as ResNet18, ResNet34, ResNet50, ResNet101, ResNet152\n",
        "- Added a dropout layer with P=0.3 to prevent overfitting. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "a4UNu1Ei4qr4"
      },
      "outputs": [],
      "source": [
        "\"\"\"resnet in pytorch\n",
        "[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun.\n",
        "\n",
        "    Deep Residual Learning for Image Recognition\n",
        "    https://arxiv.org/abs/1512.03385v1\n",
        "\"\"\"\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1,norm_layer=None):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d        \n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = norm_layer(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn2 = norm_layer(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                norm_layer(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1,norm_layer=None):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d        \n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = norm_layer(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=stride, padding=1, bias=False)\n",
        "        self.bn2 = norm_layer(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, self.expansion *\n",
        "                               planes, kernel_size=1, bias=False)\n",
        "        self.bn3 = norm_layer(self.expansion*planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                norm_layer(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = F.relu(self.bn2(self.conv2(out)))\n",
        "        out = self.bn3(self.conv3(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10,norm_layer=None):\n",
        "        super(ResNet, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        self._norm_layer = norm_layer\n",
        "        self.in_planes = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn1 = norm_layer(64)\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride,norm_layer=self._norm_layer))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = F.avg_pool2d(out, 4)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "    \n",
        "def resnet18():\n",
        "    \"\"\" return a ResNet 18 object\n",
        "    \"\"\"\n",
        "    return ResNet(BasicBlock, [2, 2, 2, 2])\n",
        "\n",
        "def resnet34():\n",
        "    \"\"\" return a ResNet 34 object\n",
        "    \"\"\"\n",
        "    return ResNet(BasicBlock, [3, 4, 6, 3])\n",
        "\n",
        "def resnet50():\n",
        "    \"\"\" return a ResNet 50 object\n",
        "    \"\"\"\n",
        "    return ResNet(Bottleneck, [3, 4, 6, 3])\n",
        "\n",
        "def resnet101():\n",
        "    \"\"\" return a ResNet 101 object\n",
        "    \"\"\"\n",
        "    return ResNet(Bottleneck, [3, 4, 23, 3])\n",
        "\n",
        "def resnet152():\n",
        "    \"\"\" return a ResNet 152 object\n",
        "    \"\"\"\n",
        "    return ResNet(Bottleneck, [3, 8, 36, 3])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define Training and Tesing Loops"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training Loop:\n",
        "\n",
        "The `train_model` function is a versatile training loop designed for PyTorch models, providing flexibility in selecting optimizers, loss functions, and various hyperparameters. Here's a brief description of its key features and how it utilizes different parameters:\n",
        "\n",
        "1. **Optimizer Selection**:\n",
        "   - The function allows the choice between 'SGD' (Stochastic Gradient Descent) and 'Adam' optimizers via the `Optimizer_type` parameter. This is achieved by checking the value of `Optimizer_type` and initializing the respective optimizer with the specified `learning_rate`, `Weight_decay`, and `Momentum` (for SGD).\n",
        "\n",
        "2. **Loss Function Selection**:\n",
        "   - The function supports multiple loss functions, including 'CrossEntropyLoss', 'MSELoss', and a custom loss function 'CustomMAPLoss'. The appropriate loss function is selected based on the `Loss_fn` parameter. For 'CustomMAPLoss', the `Num_classes` and `Weight_decay` parameters are used for initialization.\n",
        "\n",
        "3. **Learning Rate Scheduler**:\n",
        "   - A MultiStepLR scheduler is used to adjust the learning rate at specified milestones. The `milestones` parameter defines the epochs at which the learning rate is reduced by a factor specified by `gamma_lr`.\n",
        "\n",
        "4. **Training and Validation Loop**:\n",
        "   - The function contains a standard training loop where it iterates over the training dataset, computes the loss, performs backpropagation, and updates the model parameters.\n",
        "   - After each epoch, the model is evaluated on the validation dataset, and the average validation loss is computed and stored.\n",
        "\n",
        "5. **Hyperparameters**:\n",
        "   - Various hyperparameters such as `epochs`, `learning_rate`, `Weight_decay`, `Momentum`, and `Num_classes` can be adjusted to fine-tune the training process according to specific needs.\n",
        "\n",
        "6. **Model Saving**:\n",
        "   - The trained model's state dictionary is saved to a specified path (`save_path`) after the training is complete.\n",
        "\n",
        "7. **Loss Visualization**:\n",
        "   - The function plots the training and validation losses over epochs for easy visualization of the model's performance.\n",
        "\n",
        "\n",
        "This design provides flexibility and ease of experimentation with different training configurations, making it suitable for various deep learning tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_model(model: nn.Module, train_loader: DataLoader, val_loader: DataLoader,\n",
        "                epochs: int = 10, learning_rate: float = 0.005, gamma_lr: float = 0.1, \n",
        "                milestones: list = [5, 15], save_path: str = 'model.pth', Weight_decay: float = 5e-4,\n",
        "                Momentum: float = 0.9, Optimizer_type: str = 'SGD',  Loss_fn: str = 'CrossEntropyLoss',\n",
        "                Num_classes: int = 10, BNL_enable: bool = False, BNL_load_path: str = \"model.pth\") -> (list, list):\n",
        "    \"\"\"\n",
        "    Trains the model and evaluates it on the validation set after each epoch.\n",
        "\n",
        "    Parameters:\n",
        "        model (nn.Module): The neural network model to train.\n",
        "        train_loader (DataLoader): DataLoader for the training dataset.\n",
        "        val_loader (DataLoader): DataLoader for the validation dataset.\n",
        "        epochs (int): The number of epochs to train the model.\n",
        "        learning_rate (float): The learning rate for the optimizer.\n",
        "        gamma_lr (float): Factor by which the learning rate will be multiplied at each milestone.\n",
        "        milestones (list): List of epoch indices at which to adjust the learning rate.\n",
        "        save_path (str): Path to save the trained model state.\n",
        "        Weight_decay (float): The weight decay for the optimizer.\n",
        "        Momentum (float): The Momentum for the optimizer.\n",
        "        Optimizer_type (str): The optimizer type.\n",
        "        Loss_fn (str): The loss function type.\n",
        "        Num_classes (int): Number of classes in the dataset.\n",
        "        BNL_enable (bool): this to enable the training loop to load the trined wights of deep learning model.\n",
        "        BNL_load_path (str): the loading path of the trined wights of deep learning model.\n",
        "\n",
        "\n",
        "    Returns:\n",
        "        train_losses (tuple): A tuple containing lists of training losses per epoch.\n",
        "        val_losses (tuple): A tuple containing lists of validation losses per epoch.\n",
        "    \"\"\"    \n",
        "    if BNL_enable:\n",
        "        model.load_state_dict(torch.load(BNL_load_path),strict=False)\n",
        "        print(\"BNL model loaded from {}\".format(BNL_load_path))\n",
        "        print('Model weights loaded.')\n",
        "\n",
        "    if Optimizer_type == 'SGD':\n",
        "        optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=Momentum, weight_decay=Weight_decay)\n",
        "    elif Optimizer_type == 'Adam':\n",
        "        optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=Weight_decay)\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported optimizer type. Choose either 'SGD' or 'Adam'.\")\n",
        "\n",
        "    if Loss_fn == 'CrossEntropyLoss':\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "    elif Loss_fn == 'MSELoss':\n",
        "        criterion = nn.MSELoss()\n",
        "    elif Loss_fn == 'CustomMAPLoss':\n",
        "        criterion = CustomMAPLoss(num_classes=Num_classes, weight_decay=Weight_decay)    \n",
        "    else:\n",
        "        raise ValueError(\"Unsupported loss function. Implement additional loss functions as needed. Choose either 'CrossEntropyLoss' or 'MSELoss' or 'CustomMAPLoss'\")\n",
        "    \n",
        "\n",
        "    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=milestones, gamma=gamma_lr)\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()  # Set model to training mode\n",
        "        train_loss = 0.0\n",
        "\n",
        "        # Training loop\n",
        "        for data, target in train_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            optimizer.zero_grad()  # Zero the parameter gradients\n",
        "            output = model(data)  # Forward pass\n",
        "            loss = criterion(output, target)  # Loss calculation\n",
        "            loss.backward()  # Backward pass (backpropagation)\n",
        "            optimizer.step()  # Optimize model parameters\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        # Store average training loss\n",
        "        train_losses.append(train_loss / len(train_loader))\n",
        "\n",
        "        # Validation loop\n",
        "        val_loss = 0.0\n",
        "        model.eval()  # Set model to evaluation mode\n",
        "        with torch.no_grad():\n",
        "            for data, target in val_loader:\n",
        "                data, target = data.to(device), target.to(device)\n",
        "                output = model(data)\n",
        "                val_loss += criterion(output, target).item()\n",
        "\n",
        "        # Store average validation loss\n",
        "        val_losses.append(val_loss / len(val_loader))\n",
        "\n",
        "        # Print epoch summary\n",
        "        print(f'Epoch {epoch + 1}/{epochs}, Train Loss: {train_losses[-1]:.4f}, Val Loss: {val_losses[-1]:.4f}')\n",
        "\n",
        "        scheduler.step()  # Adjust learning rate\n",
        "\n",
        "    # Save the trained model state\n",
        "    torch.save(model.state_dict(), save_path)\n",
        "\n",
        "    # Plot training and validation losses\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    plt.plot(train_losses, label='Training Loss')\n",
        "    plt.plot(val_losses, label='Validation Loss')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    return train_losses, val_losses\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Testing Loop:\n",
        "#### Description:\n",
        "\n",
        "The `test_model_with_metrics` function evaluates a neural network model on a test dataset, providing flexibility to calculate various performance metrics and uncertainty measures based on the provided flags. This function is designed to offer a comprehensive evaluation with a single loop over the test data, ensuring efficient computation.\n",
        "\n",
        "#### Functionality:\n",
        "1. Model Loading: Loads the model state from the specified path and sets the model to evaluation mode.\n",
        "2. Single Test Loop: Iterates over the test dataset once to compute the required metrics.\n",
        "4. Uncertainty Calculation: Computes the average uncertainty (variance) for each class if `calculate_uncert` is enabled.\n",
        "5. Negative Log-Likelihood: Computes and prints the NLL if `calculate_nll_loss` is enabled.\n",
        "6. Expected Calibration Error: Computes and prints the ECE if `calculate_ece_error` is enabled.\n",
        "7. Precision-Recall AUC: Computes and prints the mean AUPR if `calculate_auprc` is enabled.\n",
        "8. ROC AUC: Computes and prints the mean AUC if `calculate_auc_roc` is enabled.\n",
        "9. FPR at 95% TPR: Computes and prints the mean FPR at 95% TPR if `calculate_fpr_95` is enabled.\n",
        "10. Parameter Counting: Counts and prints the number of trainable parameters if `count_params` is enabled.\n",
        "11. Uncertainty Plotting: Plots the uncertainty for different classes if `plot_uncert` is enabled.\n",
        "12. Ensemble Prediction: Uses an ensemble of models for prediction if `predict_uncert` is enabled, calculating accuracy and variance.\n",
        "\n",
        "This function ensures a flexible and efficient evaluation of the model, accommodating various metrics and uncertainty assessments as needed.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def test_model_with_metrics(loss_fn: nn.Module, model: nn.Module, test_loader: DataLoader, load_path: str = 'vit_mnist.pth',\n",
        "               calculate_uncert: bool = False, calculate_nll_loss: bool = False, calculate_ece_error: bool = False,\n",
        "               calculate_auprc: bool = False, calculate_auc_roc: bool = False, calculate_fpr_95: bool = False, \n",
        "               count_params: bool = False, plot_uncert: bool = False, predict_uncert: bool = False, \n",
        "               model_class: type = None, models: list = None, num_samples: int = 10, num_classes: int = 10) -> None:\n",
        "    \"\"\"\n",
        "    Evaluates the model on the test dataset with various metrics.\n",
        "\n",
        "    Parameters:\n",
        "        loss_fn (nn.Module): The loss function used while training.\n",
        "        model (nn.Module): The neural network model to be evaluated.\n",
        "        test_loader (DataLoader): DataLoader for the test dataset.\n",
        "        load_path (str): Path to the file from which the model state is loaded.\n",
        "        calculate_uncert (bool): Whether to calculate uncertainty.\n",
        "        calculate_nll_loss (bool): Whether to calculate Negative Log-Likelihood.\n",
        "        calculate_ece_error (bool): Whether to calculate Expected Calibration Error.\n",
        "        calculate_auprc (bool): Whether to calculate Area Under the Precision-Recall Curve.\n",
        "        calculate_auc_roc (bool): Whether to calculate Area Under the ROC Curve.\n",
        "        calculate_fpr_95 (bool): Whether to calculate False Positive Rate at 95% True Positive Rate.\n",
        "        count_params (bool): Whether to count the number of parameters.\n",
        "        plot_uncert (bool): Whether to plot uncertainty.\n",
        "        predict_uncert (bool): Whether to predict with uncertainty using ensemble.\n",
        "        model_class (type): The model class for ensemble prediction.\n",
        "        models (list): List of state dictionaries for ensemble prediction.\n",
        "        num_samples (int): Number of Monte Carlo samples for uncertainty estimation.\n",
        "        num_classes (int): Number of classes in the dataset.\n",
        "    \"\"\"\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.load_state_dict(torch.load(load_path))\n",
        "    model.to(device)\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "    criterion = loss_fn\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    test_loss = 0.0\n",
        "    all_preds = []\n",
        "    all_targets = []\n",
        "    all_probs = []\n",
        "    uncertainties = {i: [] for i in range(num_classes)} if calculate_uncert else None\n",
        "\n",
        "    # Single loop to compute metrics\n",
        "    with torch.no_grad():\n",
        "        for data in test_loader:\n",
        "            images, labels = data\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(images)\n",
        "            test_loss += criterion(outputs, labels).item()\n",
        "            \n",
        "            # Calculate accuracy\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "            \n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "            all_targets.extend(labels.cpu().numpy())\n",
        "            \n",
        "            # Calculate uncertainty if required\n",
        "            if calculate_uncert:\n",
        "                mc_outputs = torch.stack([model(images) for _ in range(num_samples)])\n",
        "                probabilities = torch.softmax(mc_outputs, dim=-1)\n",
        "                variance = probabilities.var(dim=0)\n",
        "                for i in range(num_classes):\n",
        "                    class_mask = (labels == i)\n",
        "                    if class_mask.any():\n",
        "                        class_variance = variance[class_mask, i].mean().item()\n",
        "                        uncertainties[i].append(class_variance)\n",
        "\n",
        "            # Store probabilities for further metrics\n",
        "            if calculate_ece_error or calculate_auprc or calculate_auc_roc or calculate_fpr_95:\n",
        "                probs = torch.softmax(outputs, dim=1)\n",
        "                all_probs.append(probs.cpu().numpy())\n",
        "\n",
        "    # Calculate and print metrics\n",
        "    accuracy = 100 * correct / total \n",
        "    avg_uncertainties = {i: np.mean(uncertainties[i]) if uncertainties[i] else 0 for i in range(num_classes)} if calculate_uncert else None\n",
        "    test_loss /= len(test_loader)\n",
        "    f1 = f1_score(all_targets, all_preds, average='weighted')\n",
        "    \n",
        "    all_probs = np.concatenate(all_probs, axis=0) if all_probs else None\n",
        "    all_targets = np.array(all_targets)\n",
        "    ece_score = ECE(bins=10).measure(all_probs, all_targets) if calculate_ece_error else None\n",
        "    \n",
        "    if calculate_auprc:\n",
        "        auprs = []\n",
        "        all_labels_one_hot = np.eye(num_classes)[all_targets]\n",
        "        for i in range(num_classes):\n",
        "            precision, recall, _ = precision_recall_curve(all_labels_one_hot[:, i], all_probs[:, i])\n",
        "            aupr = auc(recall, precision)\n",
        "            auprs.append(aupr)\n",
        "        mean_aupr = np.mean(auprs)\n",
        "    else:\n",
        "        mean_aupr = None\n",
        "    \n",
        "    if calculate_auc_roc:\n",
        "        aucs = []\n",
        "        all_labels_one_hot = np.eye(num_classes)[all_targets]\n",
        "        for i in range(num_classes):\n",
        "            fpr, tpr, _ = roc_curve(all_labels_one_hot[:, i], all_probs[:, i])\n",
        "            roc_auc = auc(fpr, tpr)\n",
        "            aucs.append(roc_auc)\n",
        "        mean_auc = np.mean(aucs)\n",
        "    else:\n",
        "        mean_auc = None\n",
        "    \n",
        "    if calculate_fpr_95:\n",
        "        fpr_95_recall = []\n",
        "        all_labels_one_hot = np.eye(num_classes)[all_targets]\n",
        "        for i in range(num_classes):\n",
        "            fpr, tpr, _ = roc_curve(all_labels_one_hot[:, i], all_probs[:, i])\n",
        "            idx = np.where(tpr >= 0.95)[0][0]\n",
        "            fpr_at_95_recall = fpr[idx]\n",
        "            fpr_95_recall.append(fpr_at_95_recall)\n",
        "        mean_fpr_95_recall = np.mean(fpr_95_recall)\n",
        "    else:\n",
        "        mean_fpr_95_recall = None\n",
        "\n",
        "    param_count = sum(p.numel() for p in model.parameters() if p.requires_grad) if count_params else None\n",
        "    \n",
        "    if predict_uncert:\n",
        "        if model_class is None or models is None:\n",
        "            raise ValueError(\"model_class and models must be provided for uncertainty prediction.\")\n",
        "        \n",
        "        ensemble_outputs = []\n",
        "        all_labels = []\n",
        "\n",
        "        for model_state_dict in models:\n",
        "            net = model_class()\n",
        "            net.load_state_dict(model_state_dict)\n",
        "            net.to(device)\n",
        "            net.eval()\n",
        "            \n",
        "            with torch.no_grad():\n",
        "                all_outputs = []\n",
        "                for _ in range(num_samples):\n",
        "                    batch_outputs = []\n",
        "                    batch_labels = []\n",
        "                    for data in test_loader:\n",
        "                        inputs, labels = data\n",
        "                        inputs, labels = inputs.to(device), labels.to(device)\n",
        "                        noise = torch.randn(inputs.shape[0], net.fc3.out_features).to(device)  # Sample from Gaussian\n",
        "                        outputs = net(inputs)\n",
        "                        outputs = softmax(outputs, dim=1)  # Apply softmax to get probabilities\n",
        "                        batch_outputs.append(outputs + noise)\n",
        "                        batch_labels.append(labels)\n",
        "                    all_outputs.append(torch.cat(batch_outputs))\n",
        "                    if len(all_labels) == 0:\n",
        "                        all_labels = torch.cat(batch_labels).cpu().numpy()\n",
        "                all_outputs = torch.stack(all_outputs).mean(0)\n",
        "                ensemble_outputs.append(all_outputs)\n",
        "\n",
        "        ensemble_outputs = torch.stack(ensemble_outputs).mean(0)\n",
        "        _, predicted = torch.max(ensemble_outputs, 1)\n",
        "        \n",
        "        correct = (predicted.cpu().numpy() == all_labels).sum()\n",
        "        total = len(all_labels)\n",
        "        accuracy = 100 * correct / total\n",
        "        \n",
        "        class_variances = {}\n",
        "        for class_id in range(num_classes):\n",
        "            class_mask = (all_labels == class_id)\n",
        "            if class_mask.any():\n",
        "                class_predictions = ensemble_outputs[class_mask, class_id].cpu().numpy()\n",
        "                class_variance = np.var(class_predictions)\n",
        "                class_variances[class_id] = class_variance\n",
        "                print(f'Class {class_id} variance: {class_variance:.6f}')\n",
        "        \n",
        "        if plot_uncert:\n",
        "            fig, ax = plt.subplots(figsize=(10, 6))\n",
        "            for class_id in range(num_classes):\n",
        "                if class_variances[class_id]:\n",
        "                    class_predictions = np.array(class_variances[class_id])\n",
        "                    class_variance = class_predictions.var(axis=0)\n",
        "                    class_mean = class_predictions.mean(axis=0)\n",
        "                    ax.errorbar(class_id, class_mean[class_id], yerr=class_variance[class_id], fmt='o', label=f'Class {class_id}')\n",
        "            ax.set_xlabel('Classes')\n",
        "            ax.set_ylabel('Predicted Probability')\n",
        "            ax.set_title('Uncertainty in Predictions')\n",
        "            ax.legend(loc='upper right')\n",
        "            plt.show()\n",
        "\n",
        "    # Print results\n",
        "    print(f'Test set Metrics:\\n Average loss: {test_loss:.4f} \\n F1 Score: {f1:.4f}')\n",
        "    print(f'Accuracy: {accuracy:.2f}%')\n",
        "    if calculate_uncert:\n",
        "        print(f'Uncertainties: {avg_uncertainties}')\n",
        "    if calculate_ece_error:\n",
        "        print(f'ECE: {ece_score:.4f}')\n",
        "    if calculate_auprc:\n",
        "        print(f'Mean AUPR: {mean_aupr:.4f}')\n",
        "    if calculate_auc_roc:\n",
        "        print(f'Mean AUC: {mean_auc:.4f}')\n",
        "    if calculate_fpr_95:\n",
        "        print(f'Mean FPR at 95% Recall: {mean_fpr_95_recall:.4f}')\n",
        "    if count_params:\n",
        "        print(f'Number of Parameters: {param_count}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Implemantion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading http://ufldl.stanford.edu/housenumbers/train_32x32.mat to ./data/train_32x32.mat\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 182040794/182040794 [00:31<00:00, 5785203.13it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading http://ufldl.stanford.edu/housenumbers/test_32x32.mat to ./data/test_32x32.mat\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 64275384/64275384 [00:10<00:00, 5952906.18it/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SVHN Training set size: 58605\n",
            "SVHN Validation set size: 14652\n",
            "SVHN Test set size: 26032\n",
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 170498071/170498071 [00:08<00:00, 19073954.52it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n",
            "CIFAR10 Training set size: 40000\n",
            "CIFAR10 Validation set size: 10000\n",
            "CIFAR10 Test set size: 10000\n",
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./data/cifar-100-python.tar.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 169001437/169001437 [00:08<00:00, 19144015.74it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/cifar-100-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n",
            "CIFAR100 Training set size: 40000\n",
            "CIFAR100 Validation set size: 10000\n",
            "CIFAR100 Test set size: 10000\n"
          ]
        }
      ],
      "source": [
        "# load all data\n",
        "trainloader_svhn, validloader_svhn, testloader_svhn = prepare_svhn_data()\n",
        "trainloader10, validloader10, testloader10 = prepare_cifar10_data()\n",
        "trainloader100, validloader100, testloader100 = prepare_cifar100_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ResNet50 on CIFAR10 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0tLyO2fhd3VK",
        "outputId": "605f643e-7c76-4c43-8d80-b34b8c4eae0b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 64, 32, 32]           1,728\n",
            "       BatchNorm2d-2           [-1, 64, 32, 32]             128\n",
            "            Conv2d-3           [-1, 64, 32, 32]           4,096\n",
            "       BatchNorm2d-4           [-1, 64, 32, 32]             128\n",
            "            Conv2d-5           [-1, 64, 32, 32]          36,864\n",
            "       BatchNorm2d-6           [-1, 64, 32, 32]             128\n",
            "            Conv2d-7          [-1, 256, 32, 32]          16,384\n",
            "       BatchNorm2d-8          [-1, 256, 32, 32]             512\n",
            "            Conv2d-9          [-1, 256, 32, 32]          16,384\n",
            "      BatchNorm2d-10          [-1, 256, 32, 32]             512\n",
            "       Bottleneck-11          [-1, 256, 32, 32]               0\n",
            "           Conv2d-12           [-1, 64, 32, 32]          16,384\n",
            "      BatchNorm2d-13           [-1, 64, 32, 32]             128\n",
            "           Conv2d-14           [-1, 64, 32, 32]          36,864\n",
            "      BatchNorm2d-15           [-1, 64, 32, 32]             128\n",
            "           Conv2d-16          [-1, 256, 32, 32]          16,384\n",
            "      BatchNorm2d-17          [-1, 256, 32, 32]             512\n",
            "       Bottleneck-18          [-1, 256, 32, 32]               0\n",
            "           Conv2d-19           [-1, 64, 32, 32]          16,384\n",
            "      BatchNorm2d-20           [-1, 64, 32, 32]             128\n",
            "           Conv2d-21           [-1, 64, 32, 32]          36,864\n",
            "      BatchNorm2d-22           [-1, 64, 32, 32]             128\n",
            "           Conv2d-23          [-1, 256, 32, 32]          16,384\n",
            "      BatchNorm2d-24          [-1, 256, 32, 32]             512\n",
            "       Bottleneck-25          [-1, 256, 32, 32]               0\n",
            "           Conv2d-26          [-1, 128, 32, 32]          32,768\n",
            "      BatchNorm2d-27          [-1, 128, 32, 32]             256\n",
            "           Conv2d-28          [-1, 128, 16, 16]         147,456\n",
            "      BatchNorm2d-29          [-1, 128, 16, 16]             256\n",
            "           Conv2d-30          [-1, 512, 16, 16]          65,536\n",
            "      BatchNorm2d-31          [-1, 512, 16, 16]           1,024\n",
            "           Conv2d-32          [-1, 512, 16, 16]         131,072\n",
            "      BatchNorm2d-33          [-1, 512, 16, 16]           1,024\n",
            "       Bottleneck-34          [-1, 512, 16, 16]               0\n",
            "           Conv2d-35          [-1, 128, 16, 16]          65,536\n",
            "      BatchNorm2d-36          [-1, 128, 16, 16]             256\n",
            "           Conv2d-37          [-1, 128, 16, 16]         147,456\n",
            "      BatchNorm2d-38          [-1, 128, 16, 16]             256\n",
            "           Conv2d-39          [-1, 512, 16, 16]          65,536\n",
            "      BatchNorm2d-40          [-1, 512, 16, 16]           1,024\n",
            "       Bottleneck-41          [-1, 512, 16, 16]               0\n",
            "           Conv2d-42          [-1, 128, 16, 16]          65,536\n",
            "      BatchNorm2d-43          [-1, 128, 16, 16]             256\n",
            "           Conv2d-44          [-1, 128, 16, 16]         147,456\n",
            "      BatchNorm2d-45          [-1, 128, 16, 16]             256\n",
            "           Conv2d-46          [-1, 512, 16, 16]          65,536\n",
            "      BatchNorm2d-47          [-1, 512, 16, 16]           1,024\n",
            "       Bottleneck-48          [-1, 512, 16, 16]               0\n",
            "           Conv2d-49          [-1, 128, 16, 16]          65,536\n",
            "      BatchNorm2d-50          [-1, 128, 16, 16]             256\n",
            "           Conv2d-51          [-1, 128, 16, 16]         147,456\n",
            "      BatchNorm2d-52          [-1, 128, 16, 16]             256\n",
            "           Conv2d-53          [-1, 512, 16, 16]          65,536\n",
            "      BatchNorm2d-54          [-1, 512, 16, 16]           1,024\n",
            "       Bottleneck-55          [-1, 512, 16, 16]               0\n",
            "           Conv2d-56          [-1, 256, 16, 16]         131,072\n",
            "      BatchNorm2d-57          [-1, 256, 16, 16]             512\n",
            "           Conv2d-58            [-1, 256, 8, 8]         589,824\n",
            "      BatchNorm2d-59            [-1, 256, 8, 8]             512\n",
            "           Conv2d-60           [-1, 1024, 8, 8]         262,144\n",
            "      BatchNorm2d-61           [-1, 1024, 8, 8]           2,048\n",
            "           Conv2d-62           [-1, 1024, 8, 8]         524,288\n",
            "      BatchNorm2d-63           [-1, 1024, 8, 8]           2,048\n",
            "       Bottleneck-64           [-1, 1024, 8, 8]               0\n",
            "           Conv2d-65            [-1, 256, 8, 8]         262,144\n",
            "      BatchNorm2d-66            [-1, 256, 8, 8]             512\n",
            "           Conv2d-67            [-1, 256, 8, 8]         589,824\n",
            "      BatchNorm2d-68            [-1, 256, 8, 8]             512\n",
            "           Conv2d-69           [-1, 1024, 8, 8]         262,144\n",
            "      BatchNorm2d-70           [-1, 1024, 8, 8]           2,048\n",
            "       Bottleneck-71           [-1, 1024, 8, 8]               0\n",
            "           Conv2d-72            [-1, 256, 8, 8]         262,144\n",
            "      BatchNorm2d-73            [-1, 256, 8, 8]             512\n",
            "           Conv2d-74            [-1, 256, 8, 8]         589,824\n",
            "      BatchNorm2d-75            [-1, 256, 8, 8]             512\n",
            "           Conv2d-76           [-1, 1024, 8, 8]         262,144\n",
            "      BatchNorm2d-77           [-1, 1024, 8, 8]           2,048\n",
            "       Bottleneck-78           [-1, 1024, 8, 8]               0\n",
            "           Conv2d-79            [-1, 256, 8, 8]         262,144\n",
            "      BatchNorm2d-80            [-1, 256, 8, 8]             512\n",
            "           Conv2d-81            [-1, 256, 8, 8]         589,824\n",
            "      BatchNorm2d-82            [-1, 256, 8, 8]             512\n",
            "           Conv2d-83           [-1, 1024, 8, 8]         262,144\n",
            "      BatchNorm2d-84           [-1, 1024, 8, 8]           2,048\n",
            "       Bottleneck-85           [-1, 1024, 8, 8]               0\n",
            "           Conv2d-86            [-1, 256, 8, 8]         262,144\n",
            "      BatchNorm2d-87            [-1, 256, 8, 8]             512\n",
            "           Conv2d-88            [-1, 256, 8, 8]         589,824\n",
            "      BatchNorm2d-89            [-1, 256, 8, 8]             512\n",
            "           Conv2d-90           [-1, 1024, 8, 8]         262,144\n",
            "      BatchNorm2d-91           [-1, 1024, 8, 8]           2,048\n",
            "       Bottleneck-92           [-1, 1024, 8, 8]               0\n",
            "           Conv2d-93            [-1, 256, 8, 8]         262,144\n",
            "      BatchNorm2d-94            [-1, 256, 8, 8]             512\n",
            "           Conv2d-95            [-1, 256, 8, 8]         589,824\n",
            "      BatchNorm2d-96            [-1, 256, 8, 8]             512\n",
            "           Conv2d-97           [-1, 1024, 8, 8]         262,144\n",
            "      BatchNorm2d-98           [-1, 1024, 8, 8]           2,048\n",
            "       Bottleneck-99           [-1, 1024, 8, 8]               0\n",
            "          Conv2d-100            [-1, 512, 8, 8]         524,288\n",
            "     BatchNorm2d-101            [-1, 512, 8, 8]           1,024\n",
            "          Conv2d-102            [-1, 512, 4, 4]       2,359,296\n",
            "     BatchNorm2d-103            [-1, 512, 4, 4]           1,024\n",
            "          Conv2d-104           [-1, 2048, 4, 4]       1,048,576\n",
            "     BatchNorm2d-105           [-1, 2048, 4, 4]           4,096\n",
            "          Conv2d-106           [-1, 2048, 4, 4]       2,097,152\n",
            "     BatchNorm2d-107           [-1, 2048, 4, 4]           4,096\n",
            "      Bottleneck-108           [-1, 2048, 4, 4]               0\n",
            "          Conv2d-109            [-1, 512, 4, 4]       1,048,576\n",
            "     BatchNorm2d-110            [-1, 512, 4, 4]           1,024\n",
            "          Conv2d-111            [-1, 512, 4, 4]       2,359,296\n",
            "     BatchNorm2d-112            [-1, 512, 4, 4]           1,024\n",
            "          Conv2d-113           [-1, 2048, 4, 4]       1,048,576\n",
            "     BatchNorm2d-114           [-1, 2048, 4, 4]           4,096\n",
            "      Bottleneck-115           [-1, 2048, 4, 4]               0\n",
            "          Conv2d-116            [-1, 512, 4, 4]       1,048,576\n",
            "     BatchNorm2d-117            [-1, 512, 4, 4]           1,024\n",
            "          Conv2d-118            [-1, 512, 4, 4]       2,359,296\n",
            "     BatchNorm2d-119            [-1, 512, 4, 4]           1,024\n",
            "          Conv2d-120           [-1, 2048, 4, 4]       1,048,576\n",
            "     BatchNorm2d-121           [-1, 2048, 4, 4]           4,096\n",
            "      Bottleneck-122           [-1, 2048, 4, 4]               0\n",
            "          Linear-123                   [-1, 10]          20,490\n",
            "================================================================\n",
            "Total params: 23,520,842\n",
            "Trainable params: 23,520,842\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 66.13\n",
            "Params size (MB): 89.72\n",
            "Estimated Total Size (MB): 155.86\n",
            "----------------------------------------------------------------\n",
            "Epoch 1/200, Train Loss: 2.6296, Val Loss: 2.0132\n",
            "Epoch 2/200, Train Loss: 1.9287, Val Loss: 1.8173\n",
            "Epoch 3/200, Train Loss: 1.7358, Val Loss: 1.6847\n",
            "Epoch 4/200, Train Loss: 1.5692, Val Loss: 1.7559\n",
            "Epoch 5/200, Train Loss: 1.5068, Val Loss: 1.4227\n",
            "Epoch 6/200, Train Loss: 1.2936, Val Loss: 1.1877\n",
            "Epoch 7/200, Train Loss: 1.1507, Val Loss: 1.0632\n",
            "Epoch 8/200, Train Loss: 1.0407, Val Loss: 1.0694\n",
            "Epoch 9/200, Train Loss: 0.9469, Val Loss: 1.0444\n",
            "Epoch 10/200, Train Loss: 0.8924, Val Loss: 0.8924\n",
            "Epoch 11/200, Train Loss: 0.8296, Val Loss: 1.0510\n",
            "Epoch 12/200, Train Loss: 0.7825, Val Loss: 1.1859\n",
            "Epoch 13/200, Train Loss: 0.7345, Val Loss: 0.9386\n",
            "Epoch 14/200, Train Loss: 0.7083, Val Loss: 0.8848\n",
            "Epoch 15/200, Train Loss: 0.6726, Val Loss: 0.8115\n",
            "Epoch 16/200, Train Loss: 0.6311, Val Loss: 0.7560\n",
            "Epoch 17/200, Train Loss: 0.5978, Val Loss: 0.8704\n",
            "Epoch 18/200, Train Loss: 0.5707, Val Loss: 0.7775\n",
            "Epoch 19/200, Train Loss: 0.5398, Val Loss: 0.7557\n",
            "Epoch 20/200, Train Loss: 0.5287, Val Loss: 0.8364\n",
            "Epoch 21/200, Train Loss: 0.4995, Val Loss: 0.6344\n",
            "Epoch 22/200, Train Loss: 0.4877, Val Loss: 0.6256\n",
            "Epoch 23/200, Train Loss: 0.4788, Val Loss: 0.6024\n",
            "Epoch 24/200, Train Loss: 0.4591, Val Loss: 0.7585\n",
            "Epoch 25/200, Train Loss: 0.4525, Val Loss: 0.7379\n",
            "Epoch 26/200, Train Loss: 0.4387, Val Loss: 0.8226\n",
            "Epoch 27/200, Train Loss: 0.4282, Val Loss: 0.6371\n",
            "Epoch 28/200, Train Loss: 0.4150, Val Loss: 0.6541\n",
            "Epoch 29/200, Train Loss: 0.4124, Val Loss: 1.0844\n",
            "Epoch 30/200, Train Loss: 0.4008, Val Loss: 0.5796\n",
            "Epoch 31/200, Train Loss: 0.4000, Val Loss: 0.6543\n",
            "Epoch 32/200, Train Loss: 0.3838, Val Loss: 0.7684\n",
            "Epoch 33/200, Train Loss: 0.3844, Val Loss: 0.7266\n",
            "Epoch 34/200, Train Loss: 0.3751, Val Loss: 0.5802\n",
            "Epoch 35/200, Train Loss: 0.3665, Val Loss: 0.6341\n",
            "Epoch 36/200, Train Loss: 0.3686, Val Loss: 0.7802\n",
            "Epoch 37/200, Train Loss: 0.3732, Val Loss: 0.5449\n",
            "Epoch 38/200, Train Loss: 0.3520, Val Loss: 0.6906\n"
          ]
        }
      ],
      "source": [
        "resnet50_cifr10 = ResNet(Bottleneck, [3, 4, 6, 3], num_classes=10)\n",
        "resnet50_cifr10.to(device)\n",
        "summary(resnet50_cifr10, (3, 32, 32))\n",
        "\n",
        "# Training the model\n",
        "train_losses, val_losses = train_model(\n",
        "    model=resnet50_cifr10, \n",
        "    train_loader=trainloader10, \n",
        "    val_loader=validloader10,\n",
        "    epochs=200, \n",
        "    learning_rate=0.1, \n",
        "    gamma_lr=0.2,\n",
        "    milestones=[60, 120, 160], \n",
        "    save_path='./resnet50_cifr10_dropout.pth', \n",
        "    Weight_decay=5e-4,\n",
        "    Momentum=0.9, \n",
        "    Optimizer_type='SGD',  \n",
        "    Loss_fn='CrossEntropyLoss',\n",
        "    Num_classes=10,\n",
        "    BNL_enable=False,\n",
        "    BNL_load_path='./resnet50_cifr10_dropout.pth'\n",
        ")\n",
        "\n",
        "# Testing the model with metrics\n",
        "test_model_with_metrics(\n",
        "    loss_fn=nn.CrossEntropyLoss(), \n",
        "    model=resnet50_cifr10, \n",
        "    test_loader=testloader10, \n",
        "    load_path=\"./resnet50_cifr10_dropout.pth\",\n",
        "    calculate_uncert=True, \n",
        "    calculate_nll_loss=True, \n",
        "    calculate_ece_error=True,\n",
        "    calculate_auprc=True, \n",
        "    calculate_auc_roc=True, \n",
        "    calculate_fpr_95=True, \n",
        "    count_params=True,\n",
        "    plot_uncert=False, \n",
        "    predict_uncert=False, \n",
        "    model_class=resnet50_cifr10.__class__, \n",
        "    models=[torch.load('./resnet50_cifr10_dropout.pth')],\n",
        "    num_samples=10, \n",
        "    num_classes=10\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ResNet50 on CIFAR100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "resnet50_cifr100 = ResNet(Bottleneck, [3, 4, 6, 3], num_classes=100)\n",
        "resnet50_cifr100.to(device)\n",
        "summary(resnet50_cifr100, (3, 32, 32))\n",
        "\n",
        "# Training the model\n",
        "train_losses, val_losses = train_model(\n",
        "    model=resnet50_cifr10, \n",
        "    train_loader=trainloader100, \n",
        "    val_loader=validloader100,\n",
        "    epochs=200, \n",
        "    learning_rate=0.1, \n",
        "    gamma_lr=0.2,\n",
        "    milestones=[60, 120, 160], \n",
        "    save_path='./resnet50_cifr100_dropout.pth', \n",
        "    Weight_decay=5e-4,\n",
        "    Momentum=0.9, \n",
        "    Optimizer_type='SGD',  \n",
        "    Loss_fn='CrossEntropyLoss',\n",
        "    Num_classes=100,\n",
        "    BNL_enable=False,\n",
        "    BNL_load_path='./resnet50_cifr100_dropout.pth'\n",
        ")\n",
        "\n",
        "# Testing the model with metrics\n",
        "test_model_with_metrics(\n",
        "    loss_fn=nn.CrossEntropyLoss(), \n",
        "    model=resnet50_cifr10, \n",
        "    test_loader=testloader10, \n",
        "    load_path=\"./resnet50_cifr100_dropout.pth\",\n",
        "    calculate_uncert=True, \n",
        "    calculate_nll_loss=True, \n",
        "    calculate_ece_error=True,\n",
        "    calculate_auprc=True, \n",
        "    calculate_auc_roc=True, \n",
        "    calculate_fpr_95=True, \n",
        "    count_params=True,\n",
        "    plot_uncert=False, \n",
        "    predict_uncert=False, \n",
        "    model_class=resnet50_cifr10.__class__, \n",
        "    models=[torch.load('./resnet50_cifr100_dropout.pth')],\n",
        "    num_samples=100, \n",
        "    num_classes=100\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ResNet50 on CIFAR10 with ABNLL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "resnet50_cifr10_bnl = ResNet(Bottleneck, [3, 4, 6, 3], num_classes=10,norm_layer=ABNN.BNL)\n",
        "resnet50_cifr10_bnl.to(device)\n",
        "summary(resnet50_cifr10_bnl, (3, 32, 32))\n",
        "\n",
        "# Training the model\n",
        "train_losses, val_losses = train_model(\n",
        "    model=resnet50_cifr10_bnl, \n",
        "    train_loader=trainloader10, \n",
        "    val_loader=validloader10,\n",
        "    epochs=200, \n",
        "    learning_rate=0.1, \n",
        "    gamma_lr=0.2,\n",
        "    milestones=[60, 120, 160], \n",
        "    save_path='./resnet50_cifr10_bnl_dropout.pth', \n",
        "    Weight_decay=5e-4,\n",
        "    Momentum=0.9, \n",
        "    Optimizer_type='SGD',  \n",
        "    Loss_fn=ABNN.CustomMAPLoss,\n",
        "    Num_classes=10,\n",
        "    BNL_enable=True,\n",
        "    BNL_load_path='./resnet50_cifr10_dropout.pth'\n",
        ")\n",
        "\n",
        "# Testing the model with metrics\n",
        "test_model_with_metrics(\n",
        "    loss_fn=nn.CrossEntropyLoss(), \n",
        "    model=resnet50_cifr10, \n",
        "    test_loader=testloader10, \n",
        "    load_path=\"./resnet50_cifr10_bnl_dropout.pth\",\n",
        "    calculate_uncert=True, \n",
        "    calculate_nll_loss=True, \n",
        "    calculate_ece_error=True,\n",
        "    calculate_auprc=True, \n",
        "    calculate_auc_roc=True, \n",
        "    calculate_fpr_95=True, \n",
        "    count_params=True,\n",
        "    plot_uncert=False, \n",
        "    predict_uncert=False, \n",
        "    model_class=resnet50_cifr10.__class__, \n",
        "    models=[torch.load('./resnet50_cifr10_bnl_dropout.pth')],\n",
        "    num_samples=10, \n",
        "    num_classes=10\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ResNet50 on CIFAR100 with ABNLL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "resnet50_cifr10_bnl = ResNet(Bottleneck, [3, 4, 6, 3], num_classes=10,norm_layer=ABNN.BNL)\n",
        "resnet50_cifr10_bnl.to(device)\n",
        "summary(resnet50_cifr10_bnl, (3, 32, 32))\n",
        "\n",
        "# Training the model\n",
        "train_losses, val_losses = train_model(\n",
        "    model=resnet50_cifr10_bnl, \n",
        "    train_loader=trainloader10, \n",
        "    val_loader=validloader10,\n",
        "    epochs=200, \n",
        "    learning_rate=0.1, \n",
        "    gamma_lr=0.2,\n",
        "    milestones=[60, 120, 160], \n",
        "    save_path='./resnet50_cifr100_bnl_dropout.pth', \n",
        "    Weight_decay=5e-4,\n",
        "    Momentum=0.9, \n",
        "    Optimizer_type='SGD',  \n",
        "    Loss_fn=ABNN.CustomMAPLoss,\n",
        "    Num_classes=10,\n",
        "    BNL_enable=True,\n",
        "    BNL_load_path='./resnet50_cifr100_dropout.pth'\n",
        ")\n",
        "\n",
        "# Testing the model with metrics\n",
        "test_model_with_metrics(\n",
        "    loss_fn=nn.CrossEntropyLoss(), \n",
        "    model=resnet50_cifr10, \n",
        "    test_loader=testloader10, \n",
        "    load_path=\"./resnet50_cifr100_bnl_dropout.pth\",\n",
        "    calculate_uncert=True, \n",
        "    calculate_nll_loss=True, \n",
        "    calculate_ece_error=True,\n",
        "    calculate_auprc=True, \n",
        "    calculate_auc_roc=True, \n",
        "    calculate_fpr_95=True, \n",
        "    count_params=True,\n",
        "    plot_uncert=False, \n",
        "    predict_uncert=False, \n",
        "    model_class=resnet50_cifr10.__class__, \n",
        "    models=[torch.load('./resnet50_cifr100_bnl_dropout.pth')],\n",
        "    num_samples=10, \n",
        "    num_classes=10\n",
        ")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "jBXHs2m0APxl",
        "NW9xd8Xc3UqB",
        "AoXdUp79d8CL",
        "XFwNb7Gf3UqR",
        "AL7gMgK93UqT",
        "cS3UXMDZwmM4",
        "OhKKLx5JAnzq",
        "6S-mVSFzeH0b",
        "a1WXC6ajA2jg",
        "5CcSbEhOiTst",
        "IpQxUPVV6VPH",
        "YAas1qBvIovm",
        "IPppy_FS52vd",
        "HYV-mZuBIqJa",
        "f8W_F8G-IzbY",
        "to_mOzrTI2VV",
        "FpQrqv19I5gC",
        "DsCg3LDgI9zK",
        "BV9SutwYJAKV",
        "QGjM8OySfpR1"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
