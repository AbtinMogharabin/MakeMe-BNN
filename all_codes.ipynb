{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBXHs2m0APxl"
      },
      "source": [
        "# Implemenation of Make Me a BNN\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NW9xd8Xc3UqB"
      },
      "source": [
        "## Import libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Install Missing Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "Wvc77o82xGwt"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: netcal in /home/abduallah_damash/project/ceng502/venv/lib/python3.10/site-packages (1.3.5)\n",
            "Requirement already satisfied: numpy>=1.18 in /home/abduallah_damash/project/ceng502/venv/lib/python3.10/site-packages (from netcal) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.4 in /home/abduallah_damash/project/ceng502/venv/lib/python3.10/site-packages (from netcal) (1.13.1)\n",
            "Requirement already satisfied: matplotlib>=3.3 in /home/abduallah_damash/project/ceng502/venv/lib/python3.10/site-packages (from netcal) (3.9.0)\n",
            "Requirement already satisfied: scikit-learn>=0.24 in /home/abduallah_damash/project/ceng502/venv/lib/python3.10/site-packages (from netcal) (1.5.0)\n",
            "Requirement already satisfied: torch>=1.9 in /home/abduallah_damash/project/ceng502/venv/lib/python3.10/site-packages (from netcal) (2.3.0)\n",
            "Requirement already satisfied: torchvision>=0.10.0 in /home/abduallah_damash/project/ceng502/venv/lib/python3.10/site-packages (from netcal) (0.18.0)\n",
            "Requirement already satisfied: tqdm>=4.40 in /home/abduallah_damash/project/ceng502/venv/lib/python3.10/site-packages (from netcal) (4.66.4)\n",
            "Requirement already satisfied: pyro-ppl>=1.8 in /home/abduallah_damash/project/ceng502/venv/lib/python3.10/site-packages (from netcal) (1.9.0)\n",
            "Requirement already satisfied: tikzplotlib==0.9.8 in /home/abduallah_damash/project/ceng502/venv/lib/python3.10/site-packages (from netcal) (0.9.8)\n",
            "Requirement already satisfied: tensorboard>=2.2 in /home/abduallah_damash/project/ceng502/venv/lib/python3.10/site-packages (from netcal) (2.16.2)\n",
            "Requirement already satisfied: gpytorch>=1.5.1 in /home/abduallah_damash/project/ceng502/venv/lib/python3.10/site-packages (from netcal) (1.11)\n",
            "Requirement already satisfied: Pillow in /home/abduallah_damash/project/ceng502/venv/lib/python3.10/site-packages (from tikzplotlib==0.9.8->netcal) (10.3.0)\n",
            "Requirement already satisfied: linear-operator>=0.5.0 in /home/abduallah_damash/project/ceng502/venv/lib/python3.10/site-packages (from gpytorch>=1.5.1->netcal) (0.5.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /home/abduallah_damash/project/ceng502/venv/lib/python3.10/site-packages (from matplotlib>=3.3->netcal) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /home/abduallah_damash/project/ceng502/venv/lib/python3.10/site-packages (from matplotlib>=3.3->netcal) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /home/abduallah_damash/project/ceng502/venv/lib/python3.10/site-packages (from matplotlib>=3.3->netcal) (4.52.4)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /home/abduallah_damash/project/ceng502/venv/lib/python3.10/site-packages (from matplotlib>=3.3->netcal) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/abduallah_damash/project/ceng502/venv/lib/python3.10/site-packages (from matplotlib>=3.3->netcal) (24.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /home/abduallah_damash/project/ceng502/venv/lib/python3.10/site-packages (from matplotlib>=3.3->netcal) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /home/abduallah_damash/project/ceng502/venv/lib/python3.10/site-packages (from matplotlib>=3.3->netcal) (2.9.0.post0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /home/abduallah_damash/project/ceng502/venv/lib/python3.10/site-packages (from pyro-ppl>=1.8->netcal) (3.3.0)\n",
            "Requirement already satisfied: pyro-api>=0.1.1 in /home/abduallah_damash/project/ceng502/venv/lib/python3.10/site-packages (from pyro-ppl>=1.8->netcal) (0.1.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /home/abduallah_damash/project/ceng502/venv/lib/python3.10/site-packages (from scikit-learn>=0.24->netcal) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/abduallah_damash/project/ceng502/venv/lib/python3.10/site-packages (from scikit-learn>=0.24->netcal) (3.5.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /home/abduallah_damash/project/ceng502/venv/lib/python3.10/site-packages (from tensorboard>=2.2->netcal) (2.1.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /home/abduallah_damash/project/ceng502/venv/lib/python3.10/site-packages (from tensorboard>=2.2->netcal) (1.64.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /home/abduallah_damash/project/ceng502/venv/lib/python3.10/site-packages (from tensorboard>=2.2->netcal) (3.6)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /home/abduallah_damash/project/ceng502/venv/lib/python3.10/site-packages (from tensorboard>=2.2->netcal) (5.27.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /home/abduallah_damash/project/ceng502/venv/lib/python3.10/site-packages (from tensorboard>=2.2->netcal) (68.2.0)\n",
            "Requirement already satisfied: six>1.9 in /home/abduallah_damash/project/ceng502/venv/lib/python3.10/site-packages (from tensorboard>=2.2->netcal) (1.16.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/abduallah_damash/project/ceng502/venv/lib/python3.10/site-packages (from tensorboard>=2.2->netcal) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /home/abduallah_damash/project/ceng502/venv/lib/python3.10/site-packages (from tensorboard>=2.2->netcal) (3.0.3)\n",
            "Requirement already satisfied: filelock in /home/abduallah_damash/project/ceng502/venv/lib/python3.10/site-packages (from torch>=1.9->netcal) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /home/abduallah_damash/project/ceng502/venv/lib/python3.10/site-packages (from torch>=1.9->netcal) (4.12.0)\n",
            "Requirement already satisfied: sympy in /home/abduallah_damash/project/ceng502/venv/lib/python3.10/site-packages (from torch>=1.9->netcal) (1.12)\n",
            "Requirement already satisfied: networkx in /home/abduallah_damash/project/ceng502/venv/lib/python3.10/site-packages (from torch>=1.9->netcal) (3.3)\n",
            "Requirement already satisfied: jinja2 in /home/abduallah_damash/project/ceng502/venv/lib/python3.10/site-packages (from torch>=1.9->netcal) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /home/abduallah_damash/project/ceng502/venv/lib/python3.10/site-packages (from torch>=1.9->netcal) (2024.5.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/abduallah_damash/project/ceng502/venv/lib/python3.10/site-packages (from torch>=1.9->netcal) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/abduallah_damash/project/ceng502/venv/lib/python3.10/site-packages (from torch>=1.9->netcal) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/abduallah_damash/project/ceng502/venv/lib/python3.10/site-packages (from torch>=1.9->netcal) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/abduallah_damash/project/ceng502/venv/lib/python3.10/site-packages (from torch>=1.9->netcal) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/abduallah_damash/project/ceng502/venv/lib/python3.10/site-packages (from torch>=1.9->netcal) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/abduallah_damash/project/ceng502/venv/lib/python3.10/site-packages (from torch>=1.9->netcal) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/abduallah_damash/project/ceng502/venv/lib/python3.10/site-packages (from torch>=1.9->netcal) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/abduallah_damash/project/ceng502/venv/lib/python3.10/site-packages (from torch>=1.9->netcal) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/abduallah_damash/project/ceng502/venv/lib/python3.10/site-packages (from torch>=1.9->netcal) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/abduallah_damash/project/ceng502/venv/lib/python3.10/site-packages (from torch>=1.9->netcal) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/abduallah_damash/project/ceng502/venv/lib/python3.10/site-packages (from torch>=1.9->netcal) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /home/abduallah_damash/project/ceng502/venv/lib/python3.10/site-packages (from torch>=1.9->netcal) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/abduallah_damash/project/ceng502/venv/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.9->netcal) (12.5.40)\n",
            "Requirement already satisfied: jaxtyping>=0.2.9 in /home/abduallah_damash/project/ceng502/venv/lib/python3.10/site-packages (from linear-operator>=0.5.0->gpytorch>=1.5.1->netcal) (0.2.29)\n",
            "Requirement already satisfied: typeguard~=2.13.3 in /home/abduallah_damash/project/ceng502/venv/lib/python3.10/site-packages (from linear-operator>=0.5.0->gpytorch>=1.5.1->netcal) (2.13.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/abduallah_damash/project/ceng502/venv/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard>=2.2->netcal) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /home/abduallah_damash/project/ceng502/venv/lib/python3.10/site-packages (from sympy->torch>=1.9->netcal) (1.3.0)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "Requirement already satisfied: torchsummary in /home/abduallah_damash/project/ceng502/venv/lib/python3.10/site-packages (1.5.1)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install netcal\n",
        "!pip install torchsummary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Import Used Libs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "REotBNCV3UqO"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import numpy as np\n",
        "from torchsummary import summary\n",
        "from torch.nn.functional import softmax\n",
        "import torch.nn.functional as F\n",
        "import netcal.metrics as metrics\n",
        "from netcal.metrics import ECE\n",
        "from sklearn.metrics import precision_recall_curve, auc, roc_auc_score, roc_curve\n",
        "from torch.utils.data import random_split\n",
        "import multiprocessing\n",
        "from sklearn.metrics import f1_score\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Import Custom Libs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "current dir is /home/abduallah_damash/project/ceng502/MakeMe-BNN\n",
            "ABNN dir is /home/abduallah_damash/project/ceng502/MakeMe-BNN/MakeMe-BNN/ABNN\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "# Get the current working directory of the notebook\n",
        "current_dir = os.path.dirname(os.path.abspath('__file__'))\n",
        "print(f\"current dir is {current_dir}\")\n",
        "# Add the ABNN and 'Simple CNN Demo' directories to the Python path\n",
        "abnn_dir = os.path.abspath(os.path.join(current_dir, './MakeMe-BNN/ABNN'))\n",
        "print(f\"ABNN dir is {abnn_dir}\")\n",
        "if abnn_dir not in sys.path:\n",
        "    sys.path.insert(0, abnn_dir)\n",
        "\n",
        "# Now import the necessary modules\n",
        "from ABNN import ABNN\n",
        "from ABNN.ABNN import BNL, CustomMAPLoss\n",
        "# from ABNN import ABNN, test_and_eval, train\n",
        "# from ABNN.datasets import dtd,imagenet,cifar10,cifar100,streethazards,svhn,bddanomaly,muad\n",
        "# from ABNN.deep_learning_models import resnet,resnet_diff_arc,wide_resnet18_10,vit,deeplabv3plus_resnet50"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Set the device Usage:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "multiprocessing.set_start_method('forkserver', force=True)\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "torch.backends.cudnn.benchmark = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AoXdUp79d8CL"
      },
      "source": [
        "##  Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFwNb7Gf3UqR"
      },
      "source": [
        "### Load and Augment CIFAR-10 and CIFAR100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uyQVyB3oCizi"
      },
      "source": [
        "Similar to the paper, only used:\n",
        "- Horizontal Flip for data augmentation.\n",
        "- Batch size as 128."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Load the CIFAR10 dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mSmx1nli3UqS",
        "outputId": "e4050327-9011-4833-8df4-9af7382bf900"
      },
      "outputs": [],
      "source": [
        "def prepare_cifar10_data(batch_size=128, train_split_ratio=0.8, num_workers=2):\n",
        "    \"\"\"\n",
        "    Prepares the CIFAR-10 dataset for training, validation, and testing.\n",
        "\n",
        "    Parameters:\n",
        "    - batch_size (int): The number of samples per batch to load. Default is 128.\n",
        "    - train_split_ratio (float): The proportion of the dataset to include in the train split. Default is 0.8 (80%).\n",
        "    - num_workers (int): How many subprocesses to use for data loading. Default is 2.\n",
        "\n",
        "    Returns:\n",
        "    - trainloader (DataLoader): DataLoader for the training set.\n",
        "    - validloader (DataLoader): DataLoader for the validation set.\n",
        "    - testloader (DataLoader): DataLoader for the test set.\n",
        "    \"\"\"\n",
        "    # Define the transform\n",
        "    transform = transforms.Compose([\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "    ])\n",
        "\n",
        "    # Load the CIFAR10 dataset\n",
        "    full_trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "\n",
        "    # Split the dataset into training and validation sets\n",
        "    train_size = int(train_split_ratio * len(full_trainset))  # 80% for training\n",
        "    valid_size = len(full_trainset) - train_size  # 20% for validation\n",
        "    train_subset, valid_subset = random_split(full_trainset, [train_size, valid_size])\n",
        "\n",
        "    # Create DataLoaders\n",
        "    trainloader = DataLoader(train_subset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
        "    validloader = DataLoader(valid_subset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
        "\n",
        "    # Load the test set\n",
        "    testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "    testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
        "\n",
        "    # Print the sizes of the datasets\n",
        "    print(f'CIFAR10 Training set size: {len(train_subset)}')\n",
        "    print(f'CIFAR10 Validation set size: {len(valid_subset)}')\n",
        "    print(f'CIFAR10 Test set size: {len(testset)}')\n",
        "\n",
        "    return trainloader, validloader, testloader\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     trainloader10, validloader10, testloader10 = prepare_cifar10_data()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Load the CIFAR100 dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {},
      "outputs": [],
      "source": [
        "def prepare_cifar100_data(batch_size=128, train_split_ratio=0.8, num_workers=2):\n",
        "    \"\"\"\n",
        "    Prepares the CIFAR-100 dataset for training, validation, and testing.\n",
        "\n",
        "    Parameters:\n",
        "    - batch_size (int): The number of samples per batch to load. Default is 128.\n",
        "    - train_split_ratio (float): The proportion of the dataset to include in the train split. Default is 0.8 (80%).\n",
        "    - num_workers (int): How many subprocesses to use for data loading. Default is 2.\n",
        "\n",
        "    Returns:\n",
        "    - trainloader (DataLoader): DataLoader for the training set.\n",
        "    - validloader (DataLoader): DataLoader for the validation set.\n",
        "    - testloader (DataLoader): DataLoader for the test set.\n",
        "    \"\"\"\n",
        "    # Define the transform\n",
        "    transform = transforms.Compose([\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "    ])\n",
        "\n",
        "    # Load the CIFAR100 dataset\n",
        "    full_trainset = datasets.CIFAR100(root='./data', train=True, download=True, transform=transform)\n",
        "\n",
        "    # Split the dataset into training and validation sets\n",
        "    train_size = int(train_split_ratio * len(full_trainset))  # 80% for training\n",
        "    valid_size = len(full_trainset) - train_size  # 20% for validation\n",
        "    train_subset, valid_subset = random_split(full_trainset, [train_size, valid_size])\n",
        "\n",
        "    # Create DataLoaders\n",
        "    trainloader = DataLoader(train_subset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
        "    validloader = DataLoader(valid_subset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
        "\n",
        "    # Load the test set\n",
        "    testset = datasets.CIFAR100(root='./data', train=False, download=True, transform=transform)\n",
        "    testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
        "\n",
        "    # Print the sizes of the datasets\n",
        "    print(f'CIFAR100 Training set size: {len(train_subset)}')\n",
        "    print(f'CIFAR100 Validation set size: {len(valid_subset)}')\n",
        "    print(f'CIFAR100 Test set size: {len(testset)}')\n",
        "\n",
        "    return trainloader, validloader, testloader\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     trainloader100, validloader100, testloader100 = prepare_cifar100_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {},
      "outputs": [],
      "source": [
        "def prepare_svhn_data(batch_size=128, train_split_ratio=0.8, num_workers=2):\n",
        "    \"\"\"\n",
        "    Prepares the SVHN dataset for training, validation, and testing.\n",
        "\n",
        "    Parameters:\n",
        "    - batch_size (int): The number of samples per batch to load. Default is 128.\n",
        "    - train_split_ratio (float): The proportion of the dataset to include in the train split. Default is 0.8 (80%).\n",
        "    - num_workers (int): How many subprocesses to use for data loading. Default is 2.\n",
        "\n",
        "    Returns:\n",
        "    - trainloader (DataLoader): DataLoader for the training set.\n",
        "    - validloader (DataLoader): DataLoader for the validation set.\n",
        "    - testloader (DataLoader): DataLoader for the test set.\n",
        "    \"\"\"\n",
        "    # Define the transform\n",
        "    transform = transforms.Compose([\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "    ])\n",
        "\n",
        "    # Load the SVHN dataset\n",
        "    full_trainset = datasets.SVHN(root='./data', split='train', download=True, transform=transform)\n",
        "\n",
        "    # Split the dataset into training and validation sets\n",
        "    train_size = int(train_split_ratio * len(full_trainset))  # 80% for training\n",
        "    valid_size = len(full_trainset) - train_size  # 20% for validation\n",
        "    train_subset, valid_subset = random_split(full_trainset, [train_size, valid_size])\n",
        "\n",
        "    # Create DataLoaders\n",
        "    trainloader = DataLoader(train_subset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
        "    validloader = DataLoader(valid_subset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
        "\n",
        "    # Load the test set\n",
        "    testset = datasets.SVHN(root='./data', split='test', download=True, transform=transform)\n",
        "    testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
        "\n",
        "    # Print the sizes of the datasets\n",
        "    print(f'SVHN Training set size: {len(train_subset)}')\n",
        "    print(f'SVHN Validation set size: {len(valid_subset)}')\n",
        "    print(f'SVHN Test set size: {len(testset)}')\n",
        "\n",
        "    return trainloader, validloader, testloader\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     trainloader_svhn, validloader_svhn, testloader_svhn = prepare_svhn_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AL7gMgK93UqT"
      },
      "source": [
        "## Define the DL model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define ResNen Model\n",
        "\n",
        "- It can be used as ResNet18, ResNet34, ResNet50, ResNet101, ResNet152\n",
        "- Added a dropout layer with P=0.3 to prevent overfitting. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "a4UNu1Ei4qr4"
      },
      "outputs": [],
      "source": [
        "\"\"\"resnet in pytorch\n",
        "[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun.\n",
        "\n",
        "    Deep Residual Learning for Image Recognition\n",
        "    https://arxiv.org/abs/1512.03385v1\n",
        "\"\"\"\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1,norm_layer=None):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d        \n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = norm_layer(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn2 = norm_layer(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                norm_layer(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1,norm_layer=None):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d        \n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = norm_layer(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=stride, padding=1, bias=False)\n",
        "        self.bn2 = norm_layer(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, self.expansion *\n",
        "                               planes, kernel_size=1, bias=False)\n",
        "        self.bn3 = norm_layer(self.expansion*planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                norm_layer(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = F.relu(self.bn2(self.conv2(out)))\n",
        "        out = self.bn3(self.conv3(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10,norm_layer=None):\n",
        "        super(ResNet, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        self._norm_layer = norm_layer\n",
        "        self.in_planes = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn1 = norm_layer(64)\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride,norm_layer=self._norm_layer))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = F.avg_pool2d(out, 4)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "    \n",
        "def resnet18():\n",
        "    \"\"\" return a ResNet 18 object\n",
        "    \"\"\"\n",
        "    return ResNet(BasicBlock, [2, 2, 2, 2])\n",
        "\n",
        "def resnet34():\n",
        "    \"\"\" return a ResNet 34 object\n",
        "    \"\"\"\n",
        "    return ResNet(BasicBlock, [3, 4, 6, 3])\n",
        "\n",
        "def resnet50():\n",
        "    \"\"\" return a ResNet 50 object\n",
        "    \"\"\"\n",
        "    return ResNet(Bottleneck, [3, 4, 6, 3])\n",
        "\n",
        "def resnet101():\n",
        "    \"\"\" return a ResNet 101 object\n",
        "    \"\"\"\n",
        "    return ResNet(Bottleneck, [3, 4, 23, 3])\n",
        "\n",
        "def resnet152():\n",
        "    \"\"\" return a ResNet 152 object\n",
        "    \"\"\"\n",
        "    return ResNet(Bottleneck, [3, 8, 36, 3])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define Training and Tesing Loops"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training Loop:\n",
        "\n",
        "The `train_model` function is a versatile training loop designed for PyTorch models, providing flexibility in selecting optimizers, loss functions, and various hyperparameters. Here's a brief description of its key features and how it utilizes different parameters:\n",
        "\n",
        "1. **Optimizer Selection**:\n",
        "   - The function allows the choice between 'SGD' (Stochastic Gradient Descent) and 'Adam' optimizers via the `Optimizer_type` parameter. This is achieved by checking the value of `Optimizer_type` and initializing the respective optimizer with the specified `learning_rate`, `Weight_decay`, and `Momentum` (for SGD).\n",
        "\n",
        "2. **Loss Function Selection**:\n",
        "   - The function supports multiple loss functions, including 'CrossEntropyLoss', 'MSELoss', and a custom loss function 'CustomMAPLoss'. The appropriate loss function is selected based on the `Loss_fn` parameter. For 'CustomMAPLoss', the `Num_classes` and `Weight_decay` parameters are used for initialization.\n",
        "\n",
        "3. **Learning Rate Scheduler**:\n",
        "   - A MultiStepLR scheduler is used to adjust the learning rate at specified milestones. The `milestones` parameter defines the epochs at which the learning rate is reduced by a factor specified by `gamma_lr`.\n",
        "\n",
        "4. **Training and Validation Loop**:\n",
        "   - The function contains a standard training loop where it iterates over the training dataset, computes the loss, performs backpropagation, and updates the model parameters.\n",
        "   - After each epoch, the model is evaluated on the validation dataset, and the average validation loss is computed and stored.\n",
        "\n",
        "5. **Hyperparameters**:\n",
        "   - Various hyperparameters such as `epochs`, `learning_rate`, `Weight_decay`, `Momentum`, and `Num_classes` can be adjusted to fine-tune the training process according to specific needs.\n",
        "\n",
        "6. **Model Saving**:\n",
        "   - The trained model's state dictionary is saved to a specified path (`save_path`) after the training is complete.\n",
        "\n",
        "7. **Loss Visualization**:\n",
        "   - The function plots the training and validation losses over epochs for easy visualization of the model's performance.\n",
        "\n",
        "\n",
        "This design provides flexibility and ease of experimentation with different training configurations, making it suitable for various deep learning tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CustomMAPLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Custom Maximum A Posteriori (MAP) Loss.\n",
        "\n",
        "    This loss function combines the standard MAP loss with an additional epsilon term \n",
        "    to manage Bayesian aspects of the model. It incorporates class-dependent random \n",
        "    weights to account for uncertainty.\n",
        "\n",
        "    Args:\n",
        "        num_classes (int): Number of classes in the dataset.\n",
        "        weight_decay (float): Weight decay factor for the prior term.\n",
        "        model (nn.Module): Weight decay factor for the prior term.\n",
        "\n",
        "    Methods:\n",
        "        forward(outputs, labels): Computes the total loss for the given outputs \n",
        "                                         and labels using the MAP and epsilon terms.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_classes, weight_decay,Model):\n",
        "        super(CustomMAPLoss, self).__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.weight_decay = weight_decay\n",
        "        self.model = Model\n",
        "        self.criterion = nn.CrossEntropyLoss(reduction='none')\n",
        "        self.eta = np.random.uniform(0, 1, num_classes)  # Random weights for each class\n",
        "\n",
        "    def forward(self, outputs, labels):\n",
        "        # Compute Cross Entropy Loss\n",
        "        ce_loss = self.criterion(outputs, labels)\n",
        "\n",
        "        # Compute class-dependent random weights\n",
        "        class_weights = torch.tensor([self.eta[label] for label in labels], device=outputs.device)\n",
        "        weighted_loss = ce_loss * class_weights\n",
        "\n",
        "        # Compute the log-likelihood loss\n",
        "        log_likelihood_loss = ce_loss.mean()\n",
        "\n",
        "        # Compute the prior term (weight decay)\n",
        "        prior_loss = 0\n",
        "        for param in self.model.parameters():\n",
        "            prior_loss += torch.sum(param ** 2)\n",
        "        prior_loss *= self.weight_decay / 2\n",
        "\n",
        "        # Combine the MAP loss and the epsilon term\n",
        "        map_loss = log_likelihood_loss + prior_loss\n",
        "        epsilon_term = weighted_loss.mean()\n",
        "        total_loss = map_loss + epsilon_term\n",
        "\n",
        "        return total_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_model(model: nn.Module, train_loader: DataLoader, val_loader: DataLoader,\n",
        "                epochs: int = 10, learning_rate: float = 0.005, gamma_lr: float = 0.1, \n",
        "                milestones: list = [5, 15], save_path: str = 'model.pth', Weight_decay: float = 5e-4,\n",
        "                Momentum: float = 0.9, Optimizer_type: str = 'SGD',  Loss_fn: str = 'CrossEntropyLoss',\n",
        "                Num_classes: int = 10, BNL_enable: bool = False, BNL_load_path: str = \"model.pth\") -> (list, list):\n",
        "    \"\"\"\n",
        "    Trains the model and evaluates it on the validation set after each epoch.\n",
        "\n",
        "    Parameters:\n",
        "        model (nn.Module): The neural network model to train.\n",
        "        train_loader (DataLoader): DataLoader for the training dataset.\n",
        "        val_loader (DataLoader): DataLoader for the validation dataset.\n",
        "        epochs (int): The number of epochs to train the model.\n",
        "        learning_rate (float): The learning rate for the optimizer.\n",
        "        gamma_lr (float): Factor by which the learning rate will be multiplied at each milestone.\n",
        "        milestones (list): List of epoch indices at which to adjust the learning rate.\n",
        "        save_path (str): Path to save the trained model state.\n",
        "        Weight_decay (float): The weight decay for the optimizer.\n",
        "        Momentum (float): The Momentum for the optimizer.\n",
        "        Optimizer_type (str): The optimizer type.\n",
        "        Loss_fn (str): The loss function type.\n",
        "        Num_classes (int): Number of classes in the dataset.\n",
        "        BNL_enable (bool): this to enable the training loop to load the trined wights of deep learning model.\n",
        "        BNL_load_path (str): the loading path of the trined wights of deep learning model.\n",
        "\n",
        "\n",
        "    Returns:\n",
        "        train_losses (tuple): A tuple containing lists of training losses per epoch.\n",
        "        val_losses (tuple): A tuple containing lists of validation losses per epoch.\n",
        "    \"\"\"    \n",
        "    if BNL_enable:\n",
        "        model.load_state_dict(torch.load(BNL_load_path),strict=False)\n",
        "        print(\"BNL model loaded from {}\".format(BNL_load_path))\n",
        "        print('Model weights loaded.')\n",
        "\n",
        "    if Optimizer_type == 'SGD':\n",
        "        optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=Momentum, weight_decay=Weight_decay)\n",
        "    elif Optimizer_type == 'Adam':\n",
        "        optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=Weight_decay)\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported optimizer type. Choose either 'SGD' or 'Adam'.\")\n",
        "\n",
        "    if Loss_fn == 'CrossEntropyLoss':\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "    elif Loss_fn == 'MSELoss':\n",
        "        criterion = nn.MSELoss()\n",
        "    elif Loss_fn == 'CustomMAPLoss':\n",
        "        criterion = CustomMAPLoss(num_classes=Num_classes, weight_decay=Weight_decay,Model=model)    \n",
        "    else:\n",
        "        raise ValueError(\"Unsupported loss function. Implement additional loss functions as needed. Choose either 'CrossEntropyLoss' or 'MSELoss' or 'CustomMAPLoss'\")\n",
        "    \n",
        "\n",
        "    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=milestones, gamma=gamma_lr)\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()  # Set model to training mode\n",
        "        train_loss = 0.0\n",
        "\n",
        "        # Training loop\n",
        "        for data, target in train_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            optimizer.zero_grad()  # Zero the parameter gradients\n",
        "            output = model(data)  # Forward pass\n",
        "            loss = criterion(output, target)  # Loss calculation\n",
        "            loss.backward()  # Backward pass (backpropagation)\n",
        "            optimizer.step()  # Optimize model parameters\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        # Store average training loss\n",
        "        train_losses.append(train_loss / len(train_loader))\n",
        "\n",
        "        # Validation loop\n",
        "        val_loss = 0.0\n",
        "        model.eval()  # Set model to evaluation mode\n",
        "        with torch.no_grad():\n",
        "            for data, target in val_loader:\n",
        "                data, target = data.to(device), target.to(device)\n",
        "                output = model(data)\n",
        "                val_loss += criterion(output, target).item()\n",
        "\n",
        "        # Store average validation loss\n",
        "        val_losses.append(val_loss / len(val_loader))\n",
        "\n",
        "        # Print epoch summary\n",
        "        print(f'Epoch {epoch + 1}/{epochs}, Train Loss: {train_losses[-1]:.4f}, Val Loss: {val_losses[-1]:.4f}')\n",
        "\n",
        "        scheduler.step()  # Adjust learning rate\n",
        "\n",
        "    # Save the trained model state\n",
        "    torch.save(model.state_dict(), save_path)\n",
        "\n",
        "    # Plot training and validation losses\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    plt.plot(train_losses, label='Training Loss')\n",
        "    plt.plot(val_losses, label='Validation Loss')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    return train_losses, val_losses\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Testing Loop:\n",
        "#### Description:\n",
        "\n",
        "The `test_model_with_metrics` function evaluates a neural network model on a test dataset, providing flexibility to calculate various performance metrics and uncertainty measures based on the provided flags. This function is designed to offer a comprehensive evaluation with a single loop over the test data, ensuring efficient computation.\n",
        "\n",
        "#### Functionality:\n",
        "1. Model Loading: Loads the model state from the specified path and sets the model to evaluation mode.\n",
        "2. Single Test Loop: Iterates over the test dataset once to compute the required metrics.\n",
        "4. Uncertainty Calculation: Computes the average uncertainty (variance) for each class if `calculate_uncert` is enabled.\n",
        "5. Negative Log-Likelihood: Computes and prints the NLL if `calculate_nll_loss` is enabled.\n",
        "6. Expected Calibration Error: Computes and prints the ECE if `calculate_ece_error` is enabled.\n",
        "7. Precision-Recall AUC: Computes and prints the mean AUPR if `calculate_auprc` is enabled.\n",
        "8. ROC AUC: Computes and prints the mean AUC if `calculate_auc_roc` is enabled.\n",
        "9. FPR at 95% TPR: Computes and prints the mean FPR at 95% TPR if `calculate_fpr_95` is enabled.\n",
        "10. Parameter Counting: Counts and prints the number of trainable parameters if `count_params` is enabled.\n",
        "11. Uncertainty Plotting: Plots the uncertainty for different classes if `plot_uncert` is enabled.\n",
        "12. Ensemble Prediction: Uses an ensemble of models for prediction if `predict_uncert` is enabled, calculating accuracy and variance.\n",
        "\n",
        "This function ensures a flexible and efficient evaluation of the model, accommodating various metrics and uncertainty assessments as needed.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def test_model_with_metrics(loss_fn: nn.Module, model: nn.Module, test_loader: DataLoader, load_path: str = 'vit_mnist.pth',\n",
        "               calculate_uncert: bool = False, calculate_nll_loss: bool = False, calculate_ece_error: bool = False,\n",
        "               calculate_auprc: bool = False, calculate_auc_roc: bool = False, calculate_fpr_95: bool = False, \n",
        "               count_params: bool = False, plot_uncert: bool = False, predict_uncert: bool = False, \n",
        "               model_class: type = None, models: list = None, num_samples: int = 10, num_classes: int = 10,\n",
        "               Weight_decay: float = 5e-4) -> None:\n",
        "    \"\"\"\n",
        "    Evaluates the model on the test dataset with various metrics.\n",
        "\n",
        "    Parameters:\n",
        "        loss_fn (nn.Module): The loss function used while training.\n",
        "        model (nn.Module): The neural network model to be evaluated.\n",
        "        test_loader (DataLoader): DataLoader for the test dataset.\n",
        "        load_path (str): Path to the file from which the model state is loaded.\n",
        "        calculate_uncert (bool): Whether to calculate uncertainty.\n",
        "        calculate_nll_loss (bool): Whether to calculate Negative Log-Likelihood.\n",
        "        calculate_ece_error (bool): Whether to calculate Expected Calibration Error.\n",
        "        calculate_auprc (bool): Whether to calculate Area Under the Precision-Recall Curve.\n",
        "        calculate_auc_roc (bool): Whether to calculate Area Under the ROC Curve.\n",
        "        calculate_fpr_95 (bool): Whether to calculate False Positive Rate at 95% True Positive Rate.\n",
        "        count_params (bool): Whether to count the number of parameters.\n",
        "        plot_uncert (bool): Whether to plot uncertainty.\n",
        "        predict_uncert (bool): Whether to predict with uncertainty using ensemble.\n",
        "        model_class (type): The model class for ensemble prediction.\n",
        "        models (list): List of state dictionaries for ensemble prediction.\n",
        "        num_samples (int): Number of Monte Carlo samples for uncertainty estimation.\n",
        "        num_classes (int): Number of classes in the dataset.\n",
        "        Weight_decay (float): Weight_decay of customMapLoss function.\n",
        "    \"\"\"\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.load_state_dict(torch.load(load_path), strict=False)\n",
        "    model.to(device)\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "    \n",
        "    \n",
        "    if loss_fn == \"CustomMAPLoss\":\n",
        "        criterion = CustomMAPLoss(num_classes=num_classes, weight_decay=Weight_decay,Model=model)\n",
        "    else:\n",
        "        criterion = loss_fn\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    test_loss = 0.0\n",
        "    all_preds = []\n",
        "    all_targets = []\n",
        "    all_probs = []\n",
        "    uncertainties = {i: [] for i in range(num_classes)} if calculate_uncert else None\n",
        "\n",
        "    # Single loop to compute metrics\n",
        "    with torch.no_grad():\n",
        "        for data in test_loader:\n",
        "            images, labels = data\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(images)\n",
        "            test_loss += criterion(outputs, labels).item()\n",
        "            \n",
        "            # Calculate accuracy\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "            \n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "            all_targets.extend(labels.cpu().numpy())\n",
        "            \n",
        "            # Calculate uncertainty if required\n",
        "            if calculate_uncert:\n",
        "                mc_outputs = torch.stack([model(images) for _ in range(num_samples)])\n",
        "                probabilities = torch.softmax(mc_outputs, dim=-1)\n",
        "                variance = probabilities.var(dim=0)\n",
        "                for i in range(num_classes):\n",
        "                    class_mask = (labels == i)\n",
        "                    if class_mask.any():\n",
        "                        class_variance = variance[class_mask, i].mean().item()\n",
        "                        uncertainties[i].append(class_variance)\n",
        "\n",
        "            # Store probabilities for further metrics\n",
        "            if calculate_ece_error or calculate_auprc or calculate_auc_roc or calculate_fpr_95:\n",
        "                probs = torch.softmax(outputs, dim=1)\n",
        "                all_probs.append(probs.cpu().numpy())\n",
        "\n",
        "    # Calculate and print metrics\n",
        "    accuracy = 100 * correct / total \n",
        "    avg_uncertainties = {i: np.mean(uncertainties[i]) if uncertainties[i] else 0 for i in range(num_classes)} if calculate_uncert else None\n",
        "    test_loss /= len(test_loader)\n",
        "    f1 = f1_score(all_targets, all_preds, average='weighted')\n",
        "    \n",
        "    all_probs = np.concatenate(all_probs, axis=0) if all_probs else None\n",
        "    all_targets = np.array(all_targets)\n",
        "    ece_score = ECE(bins=10).measure(all_probs, all_targets) if calculate_ece_error else None\n",
        "    \n",
        "    if calculate_auprc:\n",
        "        auprs = []\n",
        "        all_labels_one_hot = np.eye(num_classes)[all_targets]\n",
        "        for i in range(num_classes):\n",
        "            precision, recall, _ = precision_recall_curve(all_labels_one_hot[:, i], all_probs[:, i])\n",
        "            aupr = auc(recall, precision)\n",
        "            auprs.append(aupr)\n",
        "        mean_aupr = np.mean(auprs)\n",
        "    else:\n",
        "        mean_aupr = None\n",
        "    \n",
        "    if calculate_auc_roc:\n",
        "        aucs = []\n",
        "        all_labels_one_hot = np.eye(num_classes)[all_targets]\n",
        "        for i in range(num_classes):\n",
        "            fpr, tpr, _ = roc_curve(all_labels_one_hot[:, i], all_probs[:, i])\n",
        "            roc_auc = auc(fpr, tpr)\n",
        "            aucs.append(roc_auc)\n",
        "        mean_auc = np.mean(aucs)\n",
        "    else:\n",
        "        mean_auc = None\n",
        "    \n",
        "    if calculate_fpr_95:\n",
        "        fpr_95_recall = []\n",
        "        all_labels_one_hot = np.eye(num_classes)[all_targets]\n",
        "        for i in range(num_classes):\n",
        "            fpr, tpr, _ = roc_curve(all_labels_one_hot[:, i], all_probs[:, i])\n",
        "            idx = np.where(tpr >= 0.95)[0][0]\n",
        "            fpr_at_95_recall = fpr[idx]\n",
        "            fpr_95_recall.append(fpr_at_95_recall)\n",
        "        mean_fpr_95_recall = np.mean(fpr_95_recall)\n",
        "    else:\n",
        "        mean_fpr_95_recall = None\n",
        "\n",
        "    param_count = sum(p.numel() for p in model.parameters() if p.requires_grad) if count_params else None\n",
        "    \n",
        "    if predict_uncert:\n",
        "        if model_class is None or models is None:\n",
        "            raise ValueError(\"model_class and models must be provided for uncertainty prediction.\")\n",
        "        \n",
        "        ensemble_outputs = []\n",
        "        all_labels = []\n",
        "\n",
        "        for model_state_dict in models:\n",
        "            net = model_class()\n",
        "            net.load_state_dict(model_state_dict)\n",
        "            net.to(device)\n",
        "            net.eval()\n",
        "            \n",
        "            with torch.no_grad():\n",
        "                all_outputs = []\n",
        "                for _ in range(num_samples):\n",
        "                    batch_outputs = []\n",
        "                    batch_labels = []\n",
        "                    for data in test_loader:\n",
        "                        inputs, labels = data\n",
        "                        inputs, labels = inputs.to(device), labels.to(device)\n",
        "                        noise = torch.randn(inputs.shape[0], net.fc3.out_features).to(device)  # Sample from Gaussian\n",
        "                        outputs = net(inputs)\n",
        "                        outputs = softmax(outputs, dim=1)  # Apply softmax to get probabilities\n",
        "                        batch_outputs.append(outputs + noise)\n",
        "                        batch_labels.append(labels)\n",
        "                    all_outputs.append(torch.cat(batch_outputs))\n",
        "                    if len(all_labels) == 0:\n",
        "                        all_labels = torch.cat(batch_labels).cpu().numpy()\n",
        "                all_outputs = torch.stack(all_outputs).mean(0)\n",
        "                ensemble_outputs.append(all_outputs)\n",
        "\n",
        "        ensemble_outputs = torch.stack(ensemble_outputs).mean(0)\n",
        "        _, predicted = torch.max(ensemble_outputs, 1)\n",
        "        \n",
        "        correct = (predicted.cpu().numpy() == all_labels).sum()\n",
        "        total = len(all_labels)\n",
        "        accuracy = 100 * correct / total\n",
        "        \n",
        "        class_variances = {}\n",
        "        for class_id in range(num_classes):\n",
        "            class_mask = (all_labels == class_id)\n",
        "            if class_mask.any():\n",
        "                class_predictions = ensemble_outputs[class_mask, class_id].cpu().numpy()\n",
        "                class_variance = np.var(class_predictions)\n",
        "                class_variances[class_id] = class_variance\n",
        "                print(f'Class {class_id} variance: {class_variance:.6f}')\n",
        "        \n",
        "        if plot_uncert:\n",
        "            fig, ax = plt.subplots(figsize=(10, 6))\n",
        "            for class_id in range(num_classes):\n",
        "                if class_variances[class_id]:\n",
        "                    class_predictions = np.array(class_variances[class_id])\n",
        "                    class_variance = class_predictions.var(axis=0)\n",
        "                    class_mean = class_predictions.mean(axis=0)\n",
        "                    ax.errorbar(class_id, class_mean[class_id], yerr=class_variance[class_id], fmt='o', label=f'Class {class_id}')\n",
        "            ax.set_xlabel('Classes')\n",
        "            ax.set_ylabel('Predicted Probability')\n",
        "            ax.set_title('Uncertainty in Predictions')\n",
        "            ax.legend(loc='upper right')\n",
        "            plt.show()\n",
        "\n",
        "    # Print results\n",
        "    print(f'Test set Metrics:\\n Average loss: {test_loss:.4f} \\n F1 Score: {f1:.4f}')\n",
        "    print(f'Accuracy: {accuracy:.2f}%')\n",
        "    if calculate_uncert:\n",
        "        print(f'Uncertainties: {avg_uncertainties}')\n",
        "    if calculate_ece_error:\n",
        "        print(f'ECE: {ece_score:.4f}')\n",
        "    if calculate_auprc:\n",
        "        print(f'Mean AUPR: {mean_aupr:.4f}')\n",
        "    if calculate_auc_roc:\n",
        "        print(f'Mean AUC: {mean_auc:.4f}')\n",
        "    if calculate_fpr_95:\n",
        "        print(f'Mean FPR at 95% Recall: {mean_fpr_95_recall:.4f}')\n",
        "    if count_params:\n",
        "        print(f'Number of Parameters: {param_count}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Implemantion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using downloaded and verified file: ./data/train_32x32.mat\n",
            "Using downloaded and verified file: ./data/test_32x32.mat\n",
            "SVHN Training set size: 58605\n",
            "SVHN Validation set size: 14652\n",
            "SVHN Test set size: 26032\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "CIFAR10 Training set size: 40000\n",
            "CIFAR10 Validation set size: 10000\n",
            "CIFAR10 Test set size: 10000\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "CIFAR100 Training set size: 40000\n",
            "CIFAR100 Validation set size: 10000\n",
            "CIFAR100 Test set size: 10000\n"
          ]
        }
      ],
      "source": [
        "# load all data\n",
        "trainloader_svhn, validloader_svhn, testloader_svhn = prepare_svhn_data()\n",
        "trainloader10, validloader10, testloader10 = prepare_cifar10_data()\n",
        "trainloader100, validloader100, testloader100 = prepare_cifar100_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ResNet50 on CIFAR10 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0tLyO2fhd3VK",
        "outputId": "605f643e-7c76-4c43-8d80-b34b8c4eae0b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 64, 32, 32]           1,728\n",
            "       BatchNorm2d-2           [-1, 64, 32, 32]             128\n",
            "            Conv2d-3           [-1, 64, 32, 32]           4,096\n",
            "       BatchNorm2d-4           [-1, 64, 32, 32]             128\n",
            "            Conv2d-5           [-1, 64, 32, 32]          36,864\n",
            "       BatchNorm2d-6           [-1, 64, 32, 32]             128\n",
            "            Conv2d-7          [-1, 256, 32, 32]          16,384\n",
            "       BatchNorm2d-8          [-1, 256, 32, 32]             512\n",
            "            Conv2d-9          [-1, 256, 32, 32]          16,384\n",
            "      BatchNorm2d-10          [-1, 256, 32, 32]             512\n",
            "       Bottleneck-11          [-1, 256, 32, 32]               0\n",
            "           Conv2d-12           [-1, 64, 32, 32]          16,384\n",
            "      BatchNorm2d-13           [-1, 64, 32, 32]             128\n",
            "           Conv2d-14           [-1, 64, 32, 32]          36,864\n",
            "      BatchNorm2d-15           [-1, 64, 32, 32]             128\n",
            "           Conv2d-16          [-1, 256, 32, 32]          16,384\n",
            "      BatchNorm2d-17          [-1, 256, 32, 32]             512\n",
            "       Bottleneck-18          [-1, 256, 32, 32]               0\n",
            "           Conv2d-19           [-1, 64, 32, 32]          16,384\n",
            "      BatchNorm2d-20           [-1, 64, 32, 32]             128\n",
            "           Conv2d-21           [-1, 64, 32, 32]          36,864\n",
            "      BatchNorm2d-22           [-1, 64, 32, 32]             128\n",
            "           Conv2d-23          [-1, 256, 32, 32]          16,384\n",
            "      BatchNorm2d-24          [-1, 256, 32, 32]             512\n",
            "       Bottleneck-25          [-1, 256, 32, 32]               0\n",
            "           Conv2d-26          [-1, 128, 32, 32]          32,768\n",
            "      BatchNorm2d-27          [-1, 128, 32, 32]             256\n",
            "           Conv2d-28          [-1, 128, 16, 16]         147,456\n",
            "      BatchNorm2d-29          [-1, 128, 16, 16]             256\n",
            "           Conv2d-30          [-1, 512, 16, 16]          65,536\n",
            "      BatchNorm2d-31          [-1, 512, 16, 16]           1,024\n",
            "           Conv2d-32          [-1, 512, 16, 16]         131,072\n",
            "      BatchNorm2d-33          [-1, 512, 16, 16]           1,024\n",
            "       Bottleneck-34          [-1, 512, 16, 16]               0\n",
            "           Conv2d-35          [-1, 128, 16, 16]          65,536\n",
            "      BatchNorm2d-36          [-1, 128, 16, 16]             256\n",
            "           Conv2d-37          [-1, 128, 16, 16]         147,456\n",
            "      BatchNorm2d-38          [-1, 128, 16, 16]             256\n",
            "           Conv2d-39          [-1, 512, 16, 16]          65,536\n",
            "      BatchNorm2d-40          [-1, 512, 16, 16]           1,024\n",
            "       Bottleneck-41          [-1, 512, 16, 16]               0\n",
            "           Conv2d-42          [-1, 128, 16, 16]          65,536\n",
            "      BatchNorm2d-43          [-1, 128, 16, 16]             256\n",
            "           Conv2d-44          [-1, 128, 16, 16]         147,456\n",
            "      BatchNorm2d-45          [-1, 128, 16, 16]             256\n",
            "           Conv2d-46          [-1, 512, 16, 16]          65,536\n",
            "      BatchNorm2d-47          [-1, 512, 16, 16]           1,024\n",
            "       Bottleneck-48          [-1, 512, 16, 16]               0\n",
            "           Conv2d-49          [-1, 128, 16, 16]          65,536\n",
            "      BatchNorm2d-50          [-1, 128, 16, 16]             256\n",
            "           Conv2d-51          [-1, 128, 16, 16]         147,456\n",
            "      BatchNorm2d-52          [-1, 128, 16, 16]             256\n",
            "           Conv2d-53          [-1, 512, 16, 16]          65,536\n",
            "      BatchNorm2d-54          [-1, 512, 16, 16]           1,024\n",
            "       Bottleneck-55          [-1, 512, 16, 16]               0\n",
            "           Conv2d-56          [-1, 256, 16, 16]         131,072\n",
            "      BatchNorm2d-57          [-1, 256, 16, 16]             512\n",
            "           Conv2d-58            [-1, 256, 8, 8]         589,824\n",
            "      BatchNorm2d-59            [-1, 256, 8, 8]             512\n",
            "           Conv2d-60           [-1, 1024, 8, 8]         262,144\n",
            "      BatchNorm2d-61           [-1, 1024, 8, 8]           2,048\n",
            "           Conv2d-62           [-1, 1024, 8, 8]         524,288\n",
            "      BatchNorm2d-63           [-1, 1024, 8, 8]           2,048\n",
            "       Bottleneck-64           [-1, 1024, 8, 8]               0\n",
            "           Conv2d-65            [-1, 256, 8, 8]         262,144\n",
            "      BatchNorm2d-66            [-1, 256, 8, 8]             512\n",
            "           Conv2d-67            [-1, 256, 8, 8]         589,824\n",
            "      BatchNorm2d-68            [-1, 256, 8, 8]             512\n",
            "           Conv2d-69           [-1, 1024, 8, 8]         262,144\n",
            "      BatchNorm2d-70           [-1, 1024, 8, 8]           2,048\n",
            "       Bottleneck-71           [-1, 1024, 8, 8]               0\n",
            "           Conv2d-72            [-1, 256, 8, 8]         262,144\n",
            "      BatchNorm2d-73            [-1, 256, 8, 8]             512\n",
            "           Conv2d-74            [-1, 256, 8, 8]         589,824\n",
            "      BatchNorm2d-75            [-1, 256, 8, 8]             512\n",
            "           Conv2d-76           [-1, 1024, 8, 8]         262,144\n",
            "      BatchNorm2d-77           [-1, 1024, 8, 8]           2,048\n",
            "       Bottleneck-78           [-1, 1024, 8, 8]               0\n",
            "           Conv2d-79            [-1, 256, 8, 8]         262,144\n",
            "      BatchNorm2d-80            [-1, 256, 8, 8]             512\n",
            "           Conv2d-81            [-1, 256, 8, 8]         589,824\n",
            "      BatchNorm2d-82            [-1, 256, 8, 8]             512\n",
            "           Conv2d-83           [-1, 1024, 8, 8]         262,144\n",
            "      BatchNorm2d-84           [-1, 1024, 8, 8]           2,048\n",
            "       Bottleneck-85           [-1, 1024, 8, 8]               0\n",
            "           Conv2d-86            [-1, 256, 8, 8]         262,144\n",
            "      BatchNorm2d-87            [-1, 256, 8, 8]             512\n",
            "           Conv2d-88            [-1, 256, 8, 8]         589,824\n",
            "      BatchNorm2d-89            [-1, 256, 8, 8]             512\n",
            "           Conv2d-90           [-1, 1024, 8, 8]         262,144\n",
            "      BatchNorm2d-91           [-1, 1024, 8, 8]           2,048\n",
            "       Bottleneck-92           [-1, 1024, 8, 8]               0\n",
            "           Conv2d-93            [-1, 256, 8, 8]         262,144\n",
            "      BatchNorm2d-94            [-1, 256, 8, 8]             512\n",
            "           Conv2d-95            [-1, 256, 8, 8]         589,824\n",
            "      BatchNorm2d-96            [-1, 256, 8, 8]             512\n",
            "           Conv2d-97           [-1, 1024, 8, 8]         262,144\n",
            "      BatchNorm2d-98           [-1, 1024, 8, 8]           2,048\n",
            "       Bottleneck-99           [-1, 1024, 8, 8]               0\n",
            "          Conv2d-100            [-1, 512, 8, 8]         524,288\n",
            "     BatchNorm2d-101            [-1, 512, 8, 8]           1,024\n",
            "          Conv2d-102            [-1, 512, 4, 4]       2,359,296\n",
            "     BatchNorm2d-103            [-1, 512, 4, 4]           1,024\n",
            "          Conv2d-104           [-1, 2048, 4, 4]       1,048,576\n",
            "     BatchNorm2d-105           [-1, 2048, 4, 4]           4,096\n",
            "          Conv2d-106           [-1, 2048, 4, 4]       2,097,152\n",
            "     BatchNorm2d-107           [-1, 2048, 4, 4]           4,096\n",
            "      Bottleneck-108           [-1, 2048, 4, 4]               0\n",
            "          Conv2d-109            [-1, 512, 4, 4]       1,048,576\n",
            "     BatchNorm2d-110            [-1, 512, 4, 4]           1,024\n",
            "          Conv2d-111            [-1, 512, 4, 4]       2,359,296\n",
            "     BatchNorm2d-112            [-1, 512, 4, 4]           1,024\n",
            "          Conv2d-113           [-1, 2048, 4, 4]       1,048,576\n",
            "     BatchNorm2d-114           [-1, 2048, 4, 4]           4,096\n",
            "      Bottleneck-115           [-1, 2048, 4, 4]               0\n",
            "          Conv2d-116            [-1, 512, 4, 4]       1,048,576\n",
            "     BatchNorm2d-117            [-1, 512, 4, 4]           1,024\n",
            "          Conv2d-118            [-1, 512, 4, 4]       2,359,296\n",
            "     BatchNorm2d-119            [-1, 512, 4, 4]           1,024\n",
            "          Conv2d-120           [-1, 2048, 4, 4]       1,048,576\n",
            "     BatchNorm2d-121           [-1, 2048, 4, 4]           4,096\n",
            "      Bottleneck-122           [-1, 2048, 4, 4]               0\n",
            "          Linear-123                   [-1, 10]          20,490\n",
            "================================================================\n",
            "Total params: 23,520,842\n",
            "Trainable params: 23,520,842\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 66.13\n",
            "Params size (MB): 89.72\n",
            "Estimated Total Size (MB): 155.86\n",
            "----------------------------------------------------------------\n",
            "Epoch 1/200, Train Loss: 2.8095, Val Loss: 1.9908\n",
            "Epoch 2/200, Train Loss: 1.8488, Val Loss: 1.8101\n",
            "Epoch 3/200, Train Loss: 1.6492, Val Loss: 1.5561\n",
            "Epoch 4/200, Train Loss: 1.4471, Val Loss: 1.4417\n",
            "Epoch 5/200, Train Loss: 1.2912, Val Loss: 1.2257\n",
            "Epoch 6/200, Train Loss: 1.1679, Val Loss: 1.3052\n",
            "Epoch 7/200, Train Loss: 1.0519, Val Loss: 1.0813\n",
            "Epoch 8/200, Train Loss: 0.9435, Val Loss: 0.9320\n",
            "Epoch 9/200, Train Loss: 0.8501, Val Loss: 0.8735\n",
            "Epoch 10/200, Train Loss: 0.7800, Val Loss: 0.9433\n",
            "Epoch 11/200, Train Loss: 0.7117, Val Loss: 0.8498\n",
            "Epoch 12/200, Train Loss: 0.6585, Val Loss: 0.8291\n",
            "Epoch 13/200, Train Loss: 0.5996, Val Loss: 0.6706\n",
            "Epoch 14/200, Train Loss: 0.5727, Val Loss: 0.8574\n",
            "Epoch 15/200, Train Loss: 0.5453, Val Loss: 0.8128\n",
            "Epoch 16/200, Train Loss: 0.5143, Val Loss: 0.6206\n",
            "Epoch 17/200, Train Loss: 0.5010, Val Loss: 0.8905\n",
            "Epoch 18/200, Train Loss: 0.4873, Val Loss: 0.6141\n",
            "Epoch 19/200, Train Loss: 0.4689, Val Loss: 0.6015\n",
            "Epoch 20/200, Train Loss: 0.4551, Val Loss: 0.7031\n",
            "Epoch 21/200, Train Loss: 0.4408, Val Loss: 0.7626\n",
            "Epoch 22/200, Train Loss: 0.4281, Val Loss: 0.7057\n",
            "Epoch 23/200, Train Loss: 0.4231, Val Loss: 0.7869\n",
            "Epoch 24/200, Train Loss: 0.4116, Val Loss: 0.7814\n",
            "Epoch 25/200, Train Loss: 0.4112, Val Loss: 0.6998\n",
            "Epoch 26/200, Train Loss: 0.3967, Val Loss: 0.8937\n",
            "Epoch 27/200, Train Loss: 0.3893, Val Loss: 0.5897\n",
            "Epoch 28/200, Train Loss: 0.3800, Val Loss: 0.5912\n",
            "Epoch 29/200, Train Loss: 0.3744, Val Loss: 0.7011\n",
            "Epoch 30/200, Train Loss: 0.3677, Val Loss: 0.6953\n",
            "Epoch 31/200, Train Loss: 0.3634, Val Loss: 0.6766\n",
            "Epoch 32/200, Train Loss: 0.3663, Val Loss: 0.5340\n",
            "Epoch 33/200, Train Loss: 0.3498, Val Loss: 0.6167\n",
            "Epoch 34/200, Train Loss: 0.3459, Val Loss: 0.7365\n",
            "Epoch 35/200, Train Loss: 0.3482, Val Loss: 0.6374\n",
            "Epoch 36/200, Train Loss: 0.3415, Val Loss: 0.6327\n",
            "Epoch 37/200, Train Loss: 0.3359, Val Loss: 0.5843\n",
            "Epoch 38/200, Train Loss: 0.3378, Val Loss: 0.5478\n",
            "Epoch 39/200, Train Loss: 0.3204, Val Loss: 0.5879\n",
            "Epoch 40/200, Train Loss: 0.3367, Val Loss: 0.5915\n",
            "Epoch 41/200, Train Loss: 0.3192, Val Loss: 0.6070\n",
            "Epoch 42/200, Train Loss: 0.3254, Val Loss: 0.7011\n",
            "Epoch 43/200, Train Loss: 0.3115, Val Loss: 0.7521\n",
            "Epoch 44/200, Train Loss: 0.3108, Val Loss: 0.5855\n",
            "Epoch 45/200, Train Loss: 0.3112, Val Loss: 0.6354\n",
            "Epoch 46/200, Train Loss: 0.3044, Val Loss: 0.5634\n",
            "Epoch 47/200, Train Loss: 0.3077, Val Loss: 0.8436\n",
            "Epoch 48/200, Train Loss: 0.3060, Val Loss: 0.5948\n",
            "Epoch 49/200, Train Loss: 0.2969, Val Loss: 0.5693\n",
            "Epoch 50/200, Train Loss: 0.2969, Val Loss: 0.9222\n",
            "Epoch 51/200, Train Loss: 0.2978, Val Loss: 0.7719\n",
            "Epoch 52/200, Train Loss: 0.3008, Val Loss: 0.6551\n",
            "Epoch 53/200, Train Loss: 0.2983, Val Loss: 0.6163\n",
            "Epoch 54/200, Train Loss: 0.2898, Val Loss: 0.6323\n",
            "Epoch 55/200, Train Loss: 0.2885, Val Loss: 0.6241\n",
            "Epoch 56/200, Train Loss: 0.3000, Val Loss: 0.6175\n",
            "Epoch 57/200, Train Loss: 0.2872, Val Loss: 0.5390\n",
            "Epoch 58/200, Train Loss: 0.2862, Val Loss: 0.5998\n",
            "Epoch 59/200, Train Loss: 0.2908, Val Loss: 0.4971\n",
            "Epoch 60/200, Train Loss: 0.2823, Val Loss: 0.9019\n",
            "Epoch 61/200, Train Loss: 0.1036, Val Loss: 0.3093\n",
            "Epoch 62/200, Train Loss: 0.0476, Val Loss: 0.3311\n",
            "Epoch 63/200, Train Loss: 0.0300, Val Loss: 0.3436\n",
            "Epoch 64/200, Train Loss: 0.0194, Val Loss: 0.3406\n",
            "Epoch 65/200, Train Loss: 0.0137, Val Loss: 0.3562\n",
            "Epoch 66/200, Train Loss: 0.0124, Val Loss: 0.3606\n",
            "Epoch 67/200, Train Loss: 0.0097, Val Loss: 0.3635\n",
            "Epoch 68/200, Train Loss: 0.0081, Val Loss: 0.3540\n",
            "Epoch 69/200, Train Loss: 0.0068, Val Loss: 0.3720\n",
            "Epoch 70/200, Train Loss: 0.0084, Val Loss: 0.4353\n",
            "Epoch 71/200, Train Loss: 0.0106, Val Loss: 0.3903\n",
            "Epoch 72/200, Train Loss: 0.0126, Val Loss: 0.3726\n",
            "Epoch 73/200, Train Loss: 0.0173, Val Loss: 0.4373\n",
            "Epoch 74/200, Train Loss: 0.0311, Val Loss: 0.4333\n",
            "Epoch 75/200, Train Loss: 0.0377, Val Loss: 0.4695\n",
            "Epoch 76/200, Train Loss: 0.0467, Val Loss: 0.4118\n",
            "Epoch 77/200, Train Loss: 0.0505, Val Loss: 0.4980\n",
            "Epoch 78/200, Train Loss: 0.0506, Val Loss: 0.4781\n",
            "Epoch 79/200, Train Loss: 0.0466, Val Loss: 0.4574\n",
            "Epoch 80/200, Train Loss: 0.0589, Val Loss: 0.4711\n",
            "Epoch 81/200, Train Loss: 0.0580, Val Loss: 0.4298\n",
            "Epoch 82/200, Train Loss: 0.0572, Val Loss: 0.4561\n",
            "Epoch 83/200, Train Loss: 0.0556, Val Loss: 0.4785\n",
            "Epoch 84/200, Train Loss: 0.0551, Val Loss: 0.4938\n",
            "Epoch 85/200, Train Loss: 0.0612, Val Loss: 0.4380\n",
            "Epoch 86/200, Train Loss: 0.0546, Val Loss: 0.4898\n",
            "Epoch 87/200, Train Loss: 0.0591, Val Loss: 0.4823\n",
            "Epoch 88/200, Train Loss: 0.0547, Val Loss: 0.5494\n",
            "Epoch 89/200, Train Loss: 0.0568, Val Loss: 0.4683\n",
            "Epoch 90/200, Train Loss: 0.0494, Val Loss: 0.4971\n",
            "Epoch 91/200, Train Loss: 0.0556, Val Loss: 0.4896\n",
            "Epoch 92/200, Train Loss: 0.0546, Val Loss: 0.4919\n",
            "Epoch 93/200, Train Loss: 0.0543, Val Loss: 0.5817\n",
            "Epoch 94/200, Train Loss: 0.0618, Val Loss: 0.4620\n",
            "Epoch 95/200, Train Loss: 0.0591, Val Loss: 0.5593\n",
            "Epoch 96/200, Train Loss: 0.0555, Val Loss: 0.5014\n",
            "Epoch 97/200, Train Loss: 0.0577, Val Loss: 0.4841\n",
            "Epoch 98/200, Train Loss: 0.0480, Val Loss: 0.4859\n",
            "Epoch 99/200, Train Loss: 0.0527, Val Loss: 0.5026\n",
            "Epoch 100/200, Train Loss: 0.0482, Val Loss: 0.5210\n",
            "Epoch 101/200, Train Loss: 0.0572, Val Loss: 0.5445\n",
            "Epoch 102/200, Train Loss: 0.0578, Val Loss: 0.5615\n",
            "Epoch 103/200, Train Loss: 0.0529, Val Loss: 0.5836\n",
            "Epoch 104/200, Train Loss: 0.0604, Val Loss: 0.5197\n",
            "Epoch 105/200, Train Loss: 0.0490, Val Loss: 0.4680\n",
            "Epoch 106/200, Train Loss: 0.0521, Val Loss: 0.4782\n",
            "Epoch 107/200, Train Loss: 0.0495, Val Loss: 0.4576\n",
            "Epoch 108/200, Train Loss: 0.0570, Val Loss: 0.5394\n",
            "Epoch 109/200, Train Loss: 0.0490, Val Loss: 0.5480\n",
            "Epoch 110/200, Train Loss: 0.0624, Val Loss: 0.5069\n",
            "Epoch 111/200, Train Loss: 0.0461, Val Loss: 0.5187\n",
            "Epoch 112/200, Train Loss: 0.0552, Val Loss: 0.5155\n",
            "Epoch 113/200, Train Loss: 0.0524, Val Loss: 0.4558\n",
            "Epoch 114/200, Train Loss: 0.0478, Val Loss: 0.5537\n",
            "Epoch 115/200, Train Loss: 0.0512, Val Loss: 0.4754\n",
            "Epoch 116/200, Train Loss: 0.0472, Val Loss: 0.4983\n",
            "Epoch 117/200, Train Loss: 0.0584, Val Loss: 0.4856\n",
            "Epoch 118/200, Train Loss: 0.0533, Val Loss: 0.5205\n",
            "Epoch 119/200, Train Loss: 0.0596, Val Loss: 0.5417\n",
            "Epoch 120/200, Train Loss: 0.0467, Val Loss: 0.6277\n",
            "Epoch 121/200, Train Loss: 0.0165, Val Loss: 0.3709\n",
            "Epoch 122/200, Train Loss: 0.0054, Val Loss: 0.3613\n",
            "Epoch 123/200, Train Loss: 0.0038, Val Loss: 0.3514\n",
            "Epoch 124/200, Train Loss: 0.0027, Val Loss: 0.3656\n",
            "Epoch 125/200, Train Loss: 0.0022, Val Loss: 0.3558\n",
            "Epoch 126/200, Train Loss: 0.0018, Val Loss: 0.3470\n",
            "Epoch 127/200, Train Loss: 0.0019, Val Loss: 0.3541\n",
            "Epoch 128/200, Train Loss: 0.0017, Val Loss: 0.3488\n",
            "Epoch 129/200, Train Loss: 0.0015, Val Loss: 0.3551\n",
            "Epoch 130/200, Train Loss: 0.0017, Val Loss: 0.3505\n",
            "Epoch 131/200, Train Loss: 0.0014, Val Loss: 0.3495\n",
            "Epoch 132/200, Train Loss: 0.0012, Val Loss: 0.3459\n",
            "Epoch 133/200, Train Loss: 0.0013, Val Loss: 0.3453\n",
            "Epoch 134/200, Train Loss: 0.0013, Val Loss: 0.3375\n",
            "Epoch 135/200, Train Loss: 0.0012, Val Loss: 0.3410\n",
            "Epoch 136/200, Train Loss: 0.0013, Val Loss: 0.3421\n",
            "Epoch 137/200, Train Loss: 0.0012, Val Loss: 0.3389\n",
            "Epoch 138/200, Train Loss: 0.0012, Val Loss: 0.3356\n",
            "Epoch 139/200, Train Loss: 0.0012, Val Loss: 0.3315\n",
            "Epoch 140/200, Train Loss: 0.0013, Val Loss: 0.3378\n",
            "Epoch 141/200, Train Loss: 0.0012, Val Loss: 0.3368\n",
            "Epoch 142/200, Train Loss: 0.0012, Val Loss: 0.3437\n",
            "Epoch 143/200, Train Loss: 0.0013, Val Loss: 0.3393\n",
            "Epoch 144/200, Train Loss: 0.0012, Val Loss: 0.3410\n",
            "Epoch 145/200, Train Loss: 0.0012, Val Loss: 0.3336\n",
            "Epoch 146/200, Train Loss: 0.0012, Val Loss: 0.3349\n",
            "Epoch 147/200, Train Loss: 0.0012, Val Loss: 0.3377\n",
            "Epoch 148/200, Train Loss: 0.0012, Val Loss: 0.3322\n",
            "Epoch 149/200, Train Loss: 0.0013, Val Loss: 0.3351\n",
            "Epoch 150/200, Train Loss: 0.0012, Val Loss: 0.3368\n",
            "Epoch 151/200, Train Loss: 0.0012, Val Loss: 0.3287\n",
            "Epoch 152/200, Train Loss: 0.0013, Val Loss: 0.3362\n",
            "Epoch 153/200, Train Loss: 0.0012, Val Loss: 0.3271\n",
            "Epoch 154/200, Train Loss: 0.0012, Val Loss: 0.3265\n",
            "Epoch 155/200, Train Loss: 0.0013, Val Loss: 0.3320\n",
            "Epoch 156/200, Train Loss: 0.0013, Val Loss: 0.3280\n",
            "Epoch 157/200, Train Loss: 0.0012, Val Loss: 0.3336\n",
            "Epoch 158/200, Train Loss: 0.0013, Val Loss: 0.3233\n",
            "Epoch 159/200, Train Loss: 0.0013, Val Loss: 0.3258\n",
            "Epoch 160/200, Train Loss: 0.0013, Val Loss: 0.3283\n",
            "Epoch 161/200, Train Loss: 0.0013, Val Loss: 0.3293\n",
            "Epoch 162/200, Train Loss: 0.0013, Val Loss: 0.3296\n",
            "Epoch 163/200, Train Loss: 0.0013, Val Loss: 0.3263\n",
            "Epoch 164/200, Train Loss: 0.0013, Val Loss: 0.3291\n",
            "Epoch 165/200, Train Loss: 0.0013, Val Loss: 0.3281\n",
            "Epoch 166/200, Train Loss: 0.0013, Val Loss: 0.3283\n",
            "Epoch 167/200, Train Loss: 0.0013, Val Loss: 0.3237\n",
            "Epoch 168/200, Train Loss: 0.0013, Val Loss: 0.3280\n",
            "Epoch 169/200, Train Loss: 0.0013, Val Loss: 0.3258\n",
            "Epoch 170/200, Train Loss: 0.0014, Val Loss: 0.3249\n",
            "Epoch 171/200, Train Loss: 0.0013, Val Loss: 0.3291\n",
            "Epoch 172/200, Train Loss: 0.0012, Val Loss: 0.3319\n",
            "Epoch 173/200, Train Loss: 0.0013, Val Loss: 0.3314\n",
            "Epoch 174/200, Train Loss: 0.0012, Val Loss: 0.3272\n",
            "Epoch 175/200, Train Loss: 0.0013, Val Loss: 0.3262\n",
            "Epoch 176/200, Train Loss: 0.0013, Val Loss: 0.3276\n",
            "Epoch 177/200, Train Loss: 0.0013, Val Loss: 0.3414\n",
            "Epoch 178/200, Train Loss: 0.0013, Val Loss: 0.3205\n",
            "Epoch 179/200, Train Loss: 0.0013, Val Loss: 0.3259\n",
            "Epoch 180/200, Train Loss: 0.0013, Val Loss: 0.3289\n",
            "Epoch 181/200, Train Loss: 0.0013, Val Loss: 0.3273\n",
            "Epoch 182/200, Train Loss: 0.0013, Val Loss: 0.3357\n",
            "Epoch 183/200, Train Loss: 0.0013, Val Loss: 0.3285\n",
            "Epoch 184/200, Train Loss: 0.0013, Val Loss: 0.3306\n",
            "Epoch 185/200, Train Loss: 0.0013, Val Loss: 0.3275\n",
            "Epoch 186/200, Train Loss: 0.0013, Val Loss: 0.3271\n",
            "Epoch 187/200, Train Loss: 0.0013, Val Loss: 0.3265\n",
            "Epoch 188/200, Train Loss: 0.0013, Val Loss: 0.3289\n",
            "Epoch 189/200, Train Loss: 0.0013, Val Loss: 0.3325\n",
            "Epoch 190/200, Train Loss: 0.0013, Val Loss: 0.3219\n",
            "Epoch 191/200, Train Loss: 0.0013, Val Loss: 0.3330\n",
            "Epoch 192/200, Train Loss: 0.0013, Val Loss: 0.3180\n",
            "Epoch 193/200, Train Loss: 0.0013, Val Loss: 0.3328\n",
            "Epoch 194/200, Train Loss: 0.0013, Val Loss: 0.3251\n",
            "Epoch 195/200, Train Loss: 0.0013, Val Loss: 0.3276\n",
            "Epoch 196/200, Train Loss: 0.0013, Val Loss: 0.3263\n",
            "Epoch 197/200, Train Loss: 0.0013, Val Loss: 0.3306\n",
            "Epoch 198/200, Train Loss: 0.0013, Val Loss: 0.3358\n",
            "Epoch 199/200, Train Loss: 0.0013, Val Loss: 0.3288\n",
            "Epoch 200/200, Train Loss: 0.0013, Val Loss: 0.3305\n"
          ]
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA04AAAGJCAYAAAC90mOkAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy80BEi2AAAACXBIWXMAAA9hAAAPYQGoP6dpAACKKklEQVR4nOzdd3iUVdrH8e+k9wKEJEDovXcEV0BFaaIolmVRwLoq2PVVLIjsrthW3UVFXRVsWHAVXQtVQAVUuvROaEmo6T3zvH+czCRDKpAwk/D7XNc4M0+bMzMP8bnnPuc+NsuyLERERERERKRMXu5ugIiIiIiIiKdT4CQiIiIiIlIBBU4iIiIiIiIVUOAkIiIiIiJSAQVOIiIiIiIiFVDgJCIiIiIiUgEFTiIiIiIiIhVQ4CQiIiIiIlIBBU4iIiIiIiIVUOAkIuKBxo8fT9OmTc9o3ylTpmCz2aq2QR5m37592Gw2Zs2adc5f22azMWXKFOfzWbNmYbPZ2LdvX4X7Nm3alPHjx1dpe87mXBERkcpT4CQichpsNlulbkuXLnV3U8979957LzabjV27dpW5zRNPPIHNZuOPP/44hy07fYcPH2bKlCmsX7/e3U1xcgSvL730krubIiJyTvi4uwEiIjXJhx9+6PL8gw8+YOHChSWWt2vX7qxe5z//+Q92u/2M9n3yySd57LHHzur1a4MxY8Ywffp0Zs+ezeTJk0vd5pNPPqFTp0507tz5jF/npptu4s9//jP+/v5nfIyKHD58mGeeeYamTZvStWtXl3Vnc66IiEjlKXASETkNN954o8vzX3/9lYULF5ZYfqrMzEyCgoIq/Tq+vr5n1D4AHx8ffHz0571Pnz60bNmSTz75pNTAaeXKlezdu5fnnnvurF7H29sbb2/vszrG2Tibc0VERCpPXfVERKrYwIED6dixI2vWrKF///4EBQXx+OOPA/D1118zfPhwGjRogL+/Py1atOBvf/sbBQUFLsc4ddxK8W5Rb7/9Ni1atMDf359evXqxatUql31LG+Nks9mYOHEic+fOpWPHjvj7+9OhQwfmzZtXov1Lly6lZ8+eBAQE0KJFC956661Kj5v6+eefue6662jcuDH+/v7ExcXxwAMPkJWVVeL9hYSEcOjQIUaOHElISAhRUVE8/PDDJT6L5ORkxo8fT3h4OBEREYwbN47k5OQK2wIm67Rt2zbWrl1bYt3s2bOx2WyMHj2a3NxcJk+eTI8ePQgPDyc4OJiLLrqIJUuWVPgapY1xsiyLv//97zRq1IigoCAuvvhiNm/eXGLfEydO8PDDD9OpUydCQkIICwtj6NChbNiwwbnN0qVL6dWrFwA333yzszuoY3xXaWOcMjIyeOihh4iLi8Pf3582bdrw0ksvYVmWy3anc16cqSNHjnDrrbcSHR1NQEAAXbp04f333y+x3aeffkqPHj0IDQ0lLCyMTp068a9//cu5Pi8vj2eeeYZWrVoREBBA3bp1+dOf/sTChQurrK0iIuXRT5IiItXg+PHjDB06lD//+c/ceOONREdHA+YiOyQkhAcffJCQkBB+/PFHJk+eTGpqKi+++GKFx509ezZpaWn89a9/xWaz8cILL3DNNdewZ8+eCjMPv/zyC19++SV33303oaGh/Pvf/2bUqFHs37+funXrArBu3TqGDBlCbGwszzzzDAUFBUydOpWoqKhKve85c+aQmZnJXXfdRd26dfn999+ZPn06Bw8eZM6cOS7bFhQUMHjwYPr06cNLL73EokWL+Oc//0mLFi246667ABOAXHXVVfzyyy/ceeedtGvXjq+++opx48ZVqj1jxozhmWeeYfbs2XTv3t3ltT///HMuuugiGjduzLFjx3jnnXcYPXo0t99+O2lpabz77rsMHjyY33//vUT3uIpMnjyZv//97wwbNoxhw4axdu1aLr/8cnJzc12227NnD3PnzuW6666jWbNmJCUl8dZbbzFgwAC2bNlCgwYNaNeuHVOnTmXy5MnccccdXHTRRQD069ev1Ne2LIsrr7ySJUuWcOutt9K1a1fmz5/PI488wqFDh3jllVdctq/MeXGmsrKyGDhwILt27WLixIk0a9aMOXPmMH78eJKTk7nvvvsAWLhwIaNHj+bSSy/l+eefB2Dr1q0sX77cuc2UKVOYNm0at912G7179yY1NZXVq1ezdu1aLrvssrNqp4hIpVgiInLGJkyYYJ36p3TAgAEWYL355pslts/MzCyx7K9//asVFBRkZWdnO5eNGzfOatKkifP53r17LcCqW7eudeLECefyr7/+2gKs//3vf85lTz/9dIk2AZafn5+1a9cu57INGzZYgDV9+nTnshEjRlhBQUHWoUOHnMt27txp+fj4lDhmaUp7f9OmTbNsNpsVHx/v8v4Aa+rUqS7bduvWzerRo4fz+dy5cy3AeuGFF5zL8vPzrYsuusgCrJkzZ1bYpl69elmNGjWyCgoKnMvmzZtnAdZbb73lPGZOTo7LfidPnrSio6OtW265xWU5YD399NPO5zNnzrQAa+/evZZlWdaRI0csPz8/a/jw4Zbdbndu9/jjj1uANW7cOOey7Oxsl3ZZlvmu/f39XT6bVatWlfl+Tz1XHJ/Z3//+d5ftrr32Wstms7mcA5U9L0rjOCdffPHFMrd59dVXLcD66KOPnMtyc3Otvn37WiEhIVZqaqplWZZ13333WWFhYVZ+fn6Zx+rSpYs1fPjwctskIlKd1FVPRKQa+Pv7c/PNN5dYHhgY6HyclpbGsWPHuOiii8jMzGTbtm0VHveGG24gMjLS+dyRfdizZ0+F+w4aNIgWLVo4n3fu3JmwsDDnvgUFBSxatIiRI0fSoEED53YtW7Zk6NChFR4fXN9fRkYGx44do1+/fliWxbp160psf+edd7o8v+iii1zey/fff4+Pj48zAwVmTNE999xTqfaAGZd28OBBfvrpJ+ey2bNn4+fnx3XXXec8pp+fHwB2u50TJ06Qn59Pz549S+3mV55FixaRm5vLPffc49K98f777y+xrb+/P15e5n/FBQUFHD9+nJCQENq0aXPar+vw/fff4+3tzb333uuy/KGHHsKyLH744QeX5RWdF2fj+++/JyYmhtGjRzuX+fr6cu+995Kens6yZcsAiIiIICMjo9xudxEREWzevJmdO3eedbtERM6EAicRkWrQsGFD54V4cZs3b+bqq68mPDycsLAwoqKinIUlUlJSKjxu48aNXZ47gqiTJ0+e9r6O/R37HjlyhKysLFq2bFliu9KWlWb//v2MHz+eOnXqOMctDRgwACj5/gICAkp0ASzeHoD4+HhiY2MJCQlx2a5NmzaVag/An//8Z7y9vZk9ezYA2dnZfPXVVwwdOtQlCH3//ffp3Lmzc/xMVFQU3333XaW+l+Li4+MBaNWqlcvyqKgol9cDE6S98sortGrVCn9/f+rVq0dUVBR//PHHab9u8ddv0KABoaGhLssdlR4d7XOo6Lw4G/Hx8bRq1coZHJbVlrvvvpvWrVszdOhQGjVqxC233FJinNXUqVNJTk6mdevWdOrUiUceecTjy8iLSO2iwElEpBoUz7w4JCcnM2DAADZs2MDUqVP53//+x8KFC51jOipTUrqs6m3WKYP+q3rfyigoKOCyyy7ju+++49FHH2Xu3LksXLjQWcTg1Pd3rirR1a9fn8suu4z//ve/5OXl8b///Y+0tDTGjBnj3Oajjz5i/PjxtGjRgnfffZd58+axcOFCLrnkkmot9f3ss8/y4IMP0r9/fz766CPmz5/PwoUL6dChwzkrMV7d50Vl1K9fn/Xr1/PNN984x2cNHTrUZSxb//792b17N++99x4dO3bknXfeoXv37rzzzjvnrJ0icn5TcQgRkXNk6dKlHD9+nC+//JL+/fs7l+/du9eNrSpSv359AgICSp0wtrxJZB02btzIjh07eP/99xk7dqxz+dlUPWvSpAmLFy8mPT3dJeu0ffv20zrOmDFjmDdvHj/88AOzZ88mLCyMESNGONd/8cUXNG/enC+//NKle93TTz99Rm0G2LlzJ82bN3cuP3r0aIkszhdffMHFF1/Mu+++67I8OTmZevXqOZ9XpqJh8ddftGgRaWlpLlknR1dQR/vOhSZNmvDHH39gt9tdsk6ltcXPz48RI0YwYsQI7HY7d999N2+99RZPPfWUM+NZp04dbr75Zm6++WbS09Pp378/U6ZM4bbbbjtn70lEzl/KOImInCOOX/aL/5Kfm5vLG2+84a4mufD29mbQoEHMnTuXw4cPO5fv2rWrxLiYsvYH1/dnWZZLSenTNWzYMPLz85kxY4ZzWUFBAdOnTz+t44wcOZKgoCDeeOMNfvjhB6655hoCAgLKbftvv/3GypUrT7vNgwYNwtfXl+nTp7sc79VXXy2xrbe3d4nMzpw5czh06JDLsuDgYIBKlWEfNmwYBQUFvPbaay7LX3nlFWw2W6XHq1WFYcOGkZiYyGeffeZclp+fz/Tp0wkJCXF24zx+/LjLfl5eXs5JiXNyckrdJiQkhJYtWzrXi4hUN2WcRETOkX79+hEZGcm4ceO49957sdlsfPjhh+e0S1RFpkyZwoIFC7jwwgu56667nBfgHTt2ZP369eXu27ZtW1q0aMHDDz/MoUOHCAsL47///e9ZjZUZMWIEF154IY899hj79u2jffv2fPnll6c9/ickJISRI0c6xzkV76YHcMUVV/Dll19y9dVXM3z4cPbu3cubb75J+/btSU9PP63XcsxHNW3aNK644gqGDRvGunXr+OGHH1yySI7XnTp1KjfffDP9+vVj48aNfPzxxy6ZKoAWLVoQERHBm2++SWhoKMHBwfTp04dmzZqVeP0RI0Zw8cUX88QTT7Bv3z66dOnCggUL+Prrr7n//vtdCkFUhcWLF5OdnV1i+ciRI7njjjt46623GD9+PGvWrKFp06Z88cUXLF++nFdffdWZEbvttts4ceIEl1xyCY0aNSI+Pp7p06fTtWtX53io9u3bM3DgQHr06EGdOnVYvXo1X3zxBRMnTqzS9yMiUhYFTiIi50jdunX59ttveeihh3jyySeJjIzkxhtv5NJLL2Xw4MHubh4APXr04IcffuDhhx/mqaeeIi4ujqlTp7J169YKq/75+vryv//9j3vvvZdp06YREBDA1VdfzcSJE+nSpcsZtcfLy4tvvvmG+++/n48++gibzcaVV17JP//5T7p163ZaxxozZgyzZ88mNjaWSy65xGXd+PHjSUxM5K233mL+/Pm0b9+ejz76iDlz5rB06dLTbvff//53AgICePPNN1myZAl9+vRhwYIFDB8+3GW7xx9/nIyMDGbPns1nn31G9+7d+e6773jsscdctvP19eX9999n0qRJ3HnnneTn5zNz5sxSAyfHZzZ58mQ+++wzZs6cSdOmTXnxxRd56KGHTvu9VGTevHmlTpjbtGlTOnbsyNKlS3nsscd4//33SU1NpU2bNsycOZPx48c7t73xxht5++23eeONN0hOTiYmJoYbbriBKVOmOLv43XvvvXzzzTcsWLCAnJwcmjRpwt///nceeeSRKn9PIiKlsVme9FOniIh4pJEjR6oUtIiInNc0xklERFxkZWW5PN+5cyfff/89AwcOdE+DREREPIAyTiIi4iI2Npbx48fTvHlz4uPjmTFjBjk5Oaxbt67E3EQiIiLnC41xEhERF0OGDOGTTz4hMTERf39/+vbty7PPPqugSUREzmvKOImIiIiIiFRAY5xEREREREQqoMBJRERERESkAufdGCe73c7hw4cJDQ3FZrO5uzkiIiIiIuImlmWRlpZGgwYNnPPGleW8C5wOHz5MXFycu5shIiIiIiIe4sCBAzRq1Kjcbc67wCk0NBQwH05YWJibWyMiIiIiIu6SmppKXFycM0Yoz3kXODm654WFhSlwEhERERGRSg3hUXEIERERERGRCihwEhERERERqYACJxERERERkQqcd2OcRERERMTzWJZFfn4+BQUF7m6K1DK+vr54e3uf9XEUOImIiIiIW+Xm5pKQkEBmZqa7myK1kM1mo1GjRoSEhJzVcRQ4iYiIiIjb2O129u7di7e3Nw0aNMDPz69SFc5EKsOyLI4ePcrBgwdp1arVWWWeFDiJiIiIiNvk5uZit9uJi4sjKCjI3c2RWigqKop9+/aRl5d3VoGTikOIiIiIiNt5eemyVKpHVWUwdYaKiIiIiIhUQF313GhHUhq7j6TTtF4w7WLD3N0cEREREREpgzJObjRn9QHu+ngtc9cdcndTRERERMTNmjZtyquvvlrp7ZcuXYrNZiM5Obna2iRFFDi5UYCvGZyWnaf5CkRERERqCpvNVu5typQpZ3TcVatWcccdd1R6+379+pGQkEB4ePgZvV5lKUAz1FXPjRyBU5YCJxEREZEaIyEhwfn4s88+Y/LkyWzfvt25rPh8QZZlUVBQgI9PxZfdUVFRp9UOPz8/YmJiTmsfOXPKOLlRUcbJ7uaWiIiIiHgGy7LIzM13y82yrEq1MSYmxnkLDw/HZrM5n2/bto3Q0FB++OEHevTogb+/P7/88gu7d+/mqquuIjo6mpCQEHr16sWiRYtcjntqVz2bzcY777zD1VdfTVBQEK1ateKbb75xrj81EzRr1iwiIiKYP38+7dq1IyQkhCFDhrgEevn5+dx7771ERERQt25dHn30UcaNG8fIkSPP+Ds7efIkY8eOJTIykqCgIIYOHcrOnTud6+Pj4xkxYgSRkZEEBwfToUMHvv/+e+e+Y8aMISoqisDAQFq1asXMmTPPuC3VSRknNwrwNXGrMk4iIiIiRlZeAe0nz3fLa2+ZOpggv6q5PH7sscd46aWXaN68OZGRkRw4cIBhw4bxj3/8A39/fz744ANGjBjB9u3bady4cZnHeeaZZ3jhhRd48cUXmT59OmPGjCE+Pp46deqUun1mZiYvvfQSH374IV5eXtx44408/PDDfPzxxwA8//zzfPzxx8ycOZN27drxr3/9i7lz53LxxRef8XsdP348O3fu5JtvviEsLIxHH32UYcOGsWXLFnx9fZkwYQK5ubn89NNPBAcHs2XLFmdW7qmnnmLLli388MMP1KtXj127dpGVlXXGbalOCpzcKFBjnERERERqpalTp3LZZZc5n9epU4cuXbo4n//tb3/jq6++4ptvvmHixIllHmf8+PGMHj0agGeffZZ///vf/P777wwZMqTU7fPy8njzzTdp0aIFABMnTmTq1KnO9dOnT2fSpElcffXVALz22mvO7M+ZcARMy5cvp1+/fgB8/PHHxMXFMXfuXK677jr279/PqFGj6NSpEwDNmzd37r9//366detGz549AZN181QKnNxIxSFEREREXAX6erNl6mC3vXZVcQQCDunp6UyZMoXvvvuOhIQE8vPzycrKYv/+/eUep3Pnzs7HwcHBhIWFceTIkTK3DwoKcgZNALGxsc7tU1JSSEpKonfv3s713t7e9OjRA7v9zIaObN26FR8fH/r06eNcVrduXdq0acPWrVsBuPfee7nrrrtYsGABgwYNYtSoUc73dddddzFq1CjWrl3L5ZdfzsiRI50BmKfRGCc3CtQYJxEREREXNpuNID8ft9xsNluVvY/g4GCX5w8//DBfffUVzz77LD///DPr16+nU6dO5ObmlnscX1/fEp9PeUFOadtXduxWdbntttvYs2cPN910Exs3bqRnz55Mnz4dgKFDhxIfH88DDzzA4cOHufTSS3n44Yfd2t6yKHByI3+NcRIRERE5Lyxfvpzx48dz9dVX06lTJ2JiYti3b985bUN4eDjR0dGsWrXKuaygoIC1a9ee8THbtWtHfn4+v/32m3PZ8ePH2b59O+3bt3cui4uL48477+TLL7/koYce4j//+Y9zXVRUFOPGjeOjjz7i1Vdf5e233z7j9lQnddVzI41xEhERETk/tGrVii+//JIRI0Zgs9l46qmnzrh73Nm45557mDZtGi1btqRt27ZMnz6dkydPVirbtnHjRkJDQ53PbTYbXbp04aqrruL222/nrbfeIjQ0lMcee4yGDRty1VVXAXD//fczdOhQWrduzcmTJ1myZAnt2rUDYPLkyfTo0YMOHTqQk5PDt99+61znaRQ4uZHKkYuIiIicH15++WVuueUW+vXrR7169Xj00UdJTU095+149NFHSUxMZOzYsXh7e3PHHXcwePBgvL0rHt/Vv39/l+fe3t7k5+czc+ZM7rvvPq644gpyc3Pp378/33//vbPbYEFBARMmTODgwYOEhYUxZMgQXnnlFcDMRTVp0iT27dtHYGAgF110EZ9++mnVv/EqYLPc3enxHEtNTSU8PJyUlBTCwsLc2pZ9xzIY+NJSQvx92PSMewZBioiIiLhTdnY2e/fupVmzZgQEBLi7Oecdu91Ou3btuP766/nb3/7m7uZUi/LOsdOJDZRxciNV1RMRERGRcyk+Pp4FCxYwYMAAcnJyeO2119i7dy9/+ctf3N00j6fiEG7kGOOUb7fIK1B3PRERERGpXl5eXsyaNYtevXpx4YUXsnHjRhYtWuSx44o8iTJObuSoqgcm6+TrrThWRERERKpPXFwcy5cvd3czaiRdqbuRv48XjgImKkkuIiIiIuK5FDi5kc1mI8DHdNfLUWU9ERERERGPpcDJzQI0Ca6IiIiIiMdT4ORmmgRXRERERMTzKXByM0dJ8qxcBU4iIiIiIp5KgZObOedyytcYJxERERERT6XAyc0cY5zUVU9ERETk/DJw4EDuv/9+5/OmTZvy6quvlruPzWZj7ty5Z/3aVXWc84kCJzcL9NMYJxEREZGaZMSIEQwZMqTUdT///DM2m40//vjjtI+7atUq7rjjjrNtnospU6bQtWvXEssTEhIYOnRolb7WqWbNmkVERES1vsa5pMDJzRzlyBU4iYiIiNQMt956KwsXLuTgwYMl1s2cOZOePXvSuXPn0z5uVFQUQUFBVdHECsXExODv739OXqu2UODkZgF+Kg4hIiIi4mRZkJvhnptlVaqJV1xxBVFRUcyaNctleXp6OnPmzOHWW2/l+PHjjB49moYNGxIUFESnTp345JNPyj3uqV31du7cSf/+/QkICKB9+/YsXLiwxD6PPvoorVu3JigoiObNm/PUU0+Rl5cHmIzPM888w4YNG7DZbNhsNmebT+2qt3HjRi655BICAwOpW7cud9xxB+np6c7148ePZ+TIkbz00kvExsZSt25dJkyY4HytM7F//36uuuoqQkJCCAsL4/rrrycpKcm5fsOGDVx88cWEhoYSFhZGjx49WL16NQDx8fGMGDGCyMhIgoOD6dChA99///0Zt6UyfKr16FIhZ8ZJxSFEREREIC8Tnm3gntd+/DD4BVe4mY+PD2PHjmXWrFk88cQT2Gw2AObMmUNBQQGjR48mPT2dHj168OijjxIWFsZ3333HTTfdRIsWLejdu3eFr2G327nmmmuIjo7mt99+IyUlxWU8lENoaCizZs2iQYMGbNy4kdtvv53Q0FD+7//+jxtuuIFNmzYxb948Fi1aBEB4eHiJY2RkZDB48GD69u3LqlWrOHLkCLfddhsTJ050CQ6XLFlCbGwsS5YsYdeuXdxwww107dqV22+/vcL3U9r7cwRNy5YtIz8/nwkTJnDDDTewdOlSAMaMGUO3bt2YMWMG3t7erF+/Hl9fXwAmTJhAbm4uP/30E8HBwWzZsoWQkJDTbsfpUODkZoF+hRPgKuMkIiIiUmPccsstvPjiiyxbtoyBAwcCppveqFGjCA8PJzw8nIcffti5/T333MP8+fP5/PPPKxU4LVq0iG3btjF//nwaNDCB5LPPPltiXNKTTz7pfNy0aVMefvhhPv30U/7v//6PwMBAQkJC8PHxISYmpszXmj17NtnZ2XzwwQcEB5vA8bXXXmPEiBE8//zzREdHAxAZGclrr72Gt7c3bdu2Zfjw4SxevPiMAqfFixezceNG9u7dS1xcHAAffPABHTp0YNWqVfTq1Yv9+/fzyCOP0LZtWwBatWrl3H///v2MGjWKTp06AdC8efPTbsPpUuDkZkUZJwVOIiIiIvgGmcyPu167ktq2bUu/fv147733GDhwILt27eLnn39m6tSpABQUFPDss8/y+eefc+jQIXJzc8nJyan0GKatW7cSFxfnDJoA+vbtW2K7zz77jH//+9/s3r2b9PR08vPzCQsLq/T7cLxWly5dnEETwIUXXojdbmf79u3OwKlDhw54e3s7t4mNjWXjxo2n9VrFXzMuLs4ZNAG0b9+eiIgItm7dSq9evXjwwQe57bbb+PDDDxk0aBDXXXcdLVq0AODee+/lrrvuYsGCBQwaNIhRo0ad0biy0+HWMU7Tpk2jV69ehIaGUr9+fUaOHMn27dvL3WfWrFnOPpqOW0BAwDlqcdVzVtVTxklEREQEbDbTXc4dt8Iud5V166238t///pe0tDRmzpxJixYtGDBgAAAvvvgi//rXv3j00UdZsmQJ69evZ/DgweTm5lbZR7Vy5UrGjBnDsGHD+Pbbb1m3bh1PPPFElb5GcY5ucg42mw27vfqGm0yZMoXNmzczfPhwfvzxR9q3b89XX30FwG233caePXu46aab2LhxIz179mT69OnV1hZwc+C0bNkyJkyYwK+//srChQvJy8vj8ssvJyMjo9z9wsLCSEhIcN7i4+PPUYurnnMC3DyNcRIRERGpSa6//nq8vLyYPXs2H3zwAbfccotzvNPy5cu56qqruPHGG+nSpQvNmzdnx44dlT52u3btOHDgAAkJCc5lv/76q8s2K1asoEmTJjzxxBP07NmTVq1albgu9vPzo6Cg/B/o27Vrx4YNG1yuwZcvX46Xlxdt2rSpdJtPh+P9HThwwLlsy5YtJCcn0759e+ey1q1b88ADD7BgwQKuueYaZs6c6VwXFxfHnXfeyZdffslDDz3Ef/7zn2ppq4Nbu+rNmzfP5fmsWbOoX78+a9asoX///mXuZ7PZyu2nWZP4+xSOcVI5chEREZEaJSQkhBtuuIFJkyaRmprK+PHjnetatWrFF198wYoVK4iMjOTll18mKSnJJSgoz6BBg2jdujXjxo3jxRdfJDU1lSeeeMJlm1atWrF//34+/fRTevXqxXfffefMyDg0bdqUvXv3sn79eho1akRoaGiJMuRjxozh6aefZty4cUyZMoWjR49yzz33cNNNNzm76Z2pgoIC1q9f77LM39+fQYMG0alTJ8aMGcOrr75Kfn4+d999NwMGDKBnz55kZWXxyCOPcO2119KsWTMOHjzIqlWrGDVqFAD3338/Q4cOpXXr1pw8eZIlS5bQrl27s2prRTyqHHlKSgoAderUKXe79PR0mjRpQlxcHFdddRWbN28uc9ucnBxSU1Ndbp5EE+CKiIiI1Fy33norJ0+eZPDgwS7jkZ588km6d+/O4MGDGThwIDExMYwcObLSx/Xy8uKrr74iKyuL3r17c9ttt/GPf/zDZZsrr7ySBx54gIkTJ9K1a1dWrFjBU0895bLNqFGjGDJkCBdffDFRUVGllkQPCgpi/vz5nDhxgl69enHttddy6aWX8tprr53eh1GK9PR0unXr5nIbMWIENpuNr7/+msjISPr378+gQYNo3rw5n332GQDe3t4cP36csWPH0rp1a66//nqGDh3KM888A5iAbMKECbRr144hQ4bQunVr3njjjbNub3lsllXJgvXVzG63c+WVV5KcnMwvv/xS5nYrV65k586ddO7cmZSUFF566SV++uknNm/eTKNGjUpsP2XKFOcHXFxKSsppD5yrDv9dc5CH5mygf+soPril4gorIiIiIrVJdnY2e/fupVmzZjV63Lp4rvLOsdTUVMLDwysVG3hMxmnChAls2rSJTz/9tNzt+vbty9ixY+natSsDBgzgyy+/JCoqirfeeqvU7SdNmkRKSorzVrwfpSdQcQgREREREc/nEeXIJ06cyLfffstPP/1UataoPL6+vnTr1o1du3aVut7f379EP05PEuBrYleVIxcRERER8VxuzThZlsXEiRP56quv+PHHH2nWrNlpH6OgoICNGzcSGxtbDS2sfo6qepoAV0RERETEc7k14zRhwgRmz57N119/TWhoKImJiQCEh4cTGBgIwNixY2nYsCHTpk0DYOrUqVxwwQW0bNmS5ORkXnzxReLj47ntttvc9j7OhrMcuTJOIiIiIiIey62B04wZMwAYOHCgy/KZM2c6yznu378fL6+ixNjJkye5/fbbSUxMJDIykh49erBixYpKl3b0NIHOjJPmcRIREZHzl4fUK5NaqKrOLbcGTpV5E0uXLnV5/sorr/DKK69UU4vOPUfGKUflyEVEROQ85OvrC0BmZqazx5FIVcrNzQVMifOz4RHFIc5nzoyTAicRERE5D3l7exMREcGRI0cAM6eQzWZzc6uktrDb7Rw9epSgoCB8fM4u9FHg5GaOqnr5dou8Aju+3h5TIV5ERETknIiJiQFwBk8iVcnLy4vGjRufdUCuwMnNHF31ALLzChQ4iYiIyHnHZrMRGxtL/fr1ycvLc3dzpJbx8/NzqZlwphQ4uZm/T9GXmJ1nJ1QTZouIiMh5ytvb+6zHoYhUF6U33MxmsxVNgqtxTiIiIiIiHkmBkwdwFIhQ4CQiIiIi4pkUOHkA5yS4eZrLSURERETEEylw8gAqSS4iIiIi4tkUOHkAf3XVExERERHxaAqcPEBgYXEIZZxERERERDyTAicPEKCMk4iIiIiIR1Pg5AFUVU9ERERExLMpcPIAqqonIiIiIuLZFDh5gABV1RMRERER8WgKnDxAQGFxCHXVExERERHxTAqcPIAyTiIiIiIink2BkwdwFIfI0RgnERERERGPpMDJA6irnoiIiIiIZ1Pg5AHUVU9ERERExLMpcPIAmgBXRERERMSzKXDyAIHOjJPGOImIiIiIeCIFTh5AGScREREREc+mwMkDBPqpOISIiIiIiCdT4OQBAnyUcRIRERER8WQKnDxAgJ+q6omIiIiIeDIFTh6gKOOk4hAiIiIiIp5IgZMHCCzMOGXnKuMkIiIiIuKJFDh5gADfwuIQ+QqcREREREQ8kQInD+DoqpdXYJFfoO56IiIiIiKeRoGTB3B01QPIzlfgJCIiIiLiaRQ4eQB/n6KvQSXJRUREREQ8jwInD2Cz2ZzjnLJUIEJERERExOMocPIQAb6mu16OCkSIiIiIiHgcBU4eIrAwcMrK1RgnERERERFPo8DJQzgyTipJLiIiIiLieRQ4eYgAZ8ZJgZOIiIiIiKdR4OQhnJPgqqqeiIiIiIjHUeDkIZxjnBQ4iYiIiIh4HAVOHsJZVS9PxSFERERERDyNWwOnadOm0atXL0JDQ6lfvz4jR45k+/btFe43Z84c2rZtS0BAAJ06deL7778/B62tXso4iYiIiIh4LrcGTsuWLWPChAn8+uuvLFy4kLy8PC6//HIyMjLK3GfFihWMHj2aW2+9lXXr1jFy5EhGjhzJpk2bzmHLq56/xjiJiIiIiHgsm2VZlrsb4XD06FHq16/PsmXL6N+/f6nb3HDDDWRkZPDtt986l11wwQV07dqVN998s8T2OTk55OTkOJ+npqYSFxdHSkoKYWFhVf8mztDjX21k9m/7eWBQa+4b1MrdzRERERERqfVSU1MJDw+vVGzgUWOcUlJSAKhTp06Z26xcuZJBgwa5LBs8eDArV64sdftp06YRHh7uvMXFxVVdg8/Wuo/ho1Hwx+fqqiciIiIi4sE8JnCy2+3cf//9XHjhhXTs2LHM7RITE4mOjnZZFh0dTWJiYqnbT5o0iZSUFOftwIEDVdrus3JsB+xaBPHLVY5cRERERMSD+bi7AQ4TJkxg06ZN/PLLL1V6XH9/f/z9/av0mFWmQVdzn7CBwJYm46TASURERETE83hExmnixIl8++23LFmyhEaNGpW7bUxMDElJSS7LkpKSiImJqc4mVo/YLuY+aTOB3qYMuQInERERERHP49bAybIsJk6cyFdffcWPP/5Is2bNKtynb9++LF682GXZwoUL6du3b3U1s/pENgP/cCjIJSZnH6AxTiIiIiIinsitgdOECRP46KOPmD17NqGhoSQmJpKYmEhWVpZzm7FjxzJp0iTn8/vuu4958+bxz3/+k23btjFlyhRWr17NxIkT3fEWzo7NBrGdAYjOMPNXZWsCXBERERERj+PWwGnGjBmkpKQwcOBAYmNjnbfPPvvMuc3+/ftJSEhwPu/Xrx+zZ8/m7bffpkuXLnzxxRfMnTu33IISHq2wu169tK2AMk4iIiIiIp7IrcUhKjOF1NKlS0ssu+6667juuuuqoUVu0KAbAJEpW4AryFHgJCIiIiLicTyiOMR5rTDjFJK8DW8KlHESEREREfFACpzcrU4L8AvBuyCb5rYEjXESEREREfFACpzczcsLYkyBiE62PSpHLiIiIiLigRQ4eYLC7nodvfapq56IiIiIiAdS4OQJCgOnDl77yFFXPRERERERj6PAyRM06ApAB9s+8gryKbBXXG1QRERERETOHQVOnqBuKyyfQEJs2TSzJWqck4iIiIiIh1Hg5Am8fSDaTODb0baPlKw8NzdIRERERESKU+DkIWyO7npee9l5JN29jRERERERERcKnDxFYYGITra9bE9MdXNjRERERESkOAVOnqJYSfJtCQqcREREREQ8iQInT1G/HXabL2G2TI4f3uPu1oiIiIiISDEKnDyFty8F4Y0BsE7sIb9A8zmJiIiIiHgKBU4exKdeCwAa2BPZdzzDza0REREREREHBU4exFanGQBNbUlsTUhzc2tERERERMRBgZMnqdMcgCa2RLYnKnASEREREfEUCpw8SWHg1NSWxDYFTiIiIiIiHkOBkycpDJwa25LYnpji5saIiIiIiIiDAidPEtEYy+ZFsC2H7JOJpOfku7tFIiIiIiKCAifP4uOHLbwRoHFOIiIiIiKeRIGTp3GMc/JKUuAkIiIiIuIhFDh5mmLjnLYlprq5MSIiIiIiAgqcPI8q64mIiIiIeBwFTp4m0kyC28RmuupZluXmBomIiIiIiAInT1OYcWpmSyQlK5ek1Bw3N0hERERERBQ4eZrIpgCE2TKJIJ2tGuckIiIiIuJ2Cpw8jV8QhDYAirrriYiIiIiIeylw8kSF3fUUOImIiIiIeAYFTp6oTlPAVNbbmqCueiIiIiIi7qbAyRM5Mk5eiew+mk5egd3NDRIREREROb8pcPJEhYFTc6+j5BVY7D2W4eYGiYiIiIic3xQ4eSJHSXKvJABNhCsiIiIi4mYKnDxR4SS4EVYyIWSyTeOcRERERETcSoGTJwoIg6B6ADSxHVFlPRERERERN1Pg5KkKu+s1tiWpq56IiIiIiJspcPJUhYFTU1sSh5KzSM3Oc3ODRERERETOXwqcPFVh4NTZPwGAHco6iYiIiIi4jQInT9X4AgAuZD1e2NVdT0RERETEjdwaOP3000+MGDGCBg0aYLPZmDt3brnbL126FJvNVuKWmJh4bhp8LjXpBwERhNlT6GHboQIRIiIiIiJu5NbAKSMjgy5duvD666+f1n7bt28nISHBeatfv341tdCNvH2h9WAALvNew7ZElSQXEREREXEXnzPZ6cCBA9hsNho1agTA77//zuzZs2nfvj133HFHpY8zdOhQhg4detqvX79+fSIiIk57vxqn7XD44zMu91rN9MRULMvCZrO5u1UiIiIiIuedM8o4/eUvf2HJkiUAJCYmctlll/H777/zxBNPMHXq1CptYGm6du1KbGwsl112GcuXLy9325ycHFJTU11uNUaLS7G8/WnqlURMTjwJKdnubpGIiIiIyHnpjAKnTZs20bt3bwA+//xzOnbsyIoVK/j444+ZNWtWVbbPRWxsLG+++Sb//e9/+e9//0tcXBwDBw5k7dq1Ze4zbdo0wsPDnbe4uLhqa1+V8w/B1nwgAJd7rVZ3PRERERERNzmjwCkvLw9/f38AFi1axJVXXglA27ZtSUhIqLrWnaJNmzb89a9/pUePHvTr14/33nuPfv368corr5S5z6RJk0hJSXHeDhw4UG3tqxZthwFwmfdqVdYTEREREXGTMwqcOnTowJtvvsnPP//MwoULGTJkCACHDx+mbt26VdrAivTu3Ztdu3aVud7f35+wsDCXW43SeigWNrp67SHxwB53t0ZERERE5Lx0RoHT888/z1tvvcXAgQMZPXo0Xbp0AeCbb75xduE7V9avX09sbOw5fc1zKjSalLpdAYg6/KN72yIiIiIicp46o6p6AwcO5NixY6SmphIZGelcfscddxAUFFTp46Snp7tki/bu3cv69eupU6cOjRs3ZtKkSRw6dIgPPvgAgFdffZVmzZrRoUMHsrOzeeedd/jxxx9ZsGDBmbyNmqPtcFi+jq4Zy8nNt+Pno3mLRURERETOpTO6As/KyiInJ8cZNMXHx/Pqq6+yffv205pTafXq1XTr1o1u3boB8OCDD9KtWzcmT54MQEJCAvv373dun5uby0MPPUSnTp0YMGAAGzZsYNGiRVx66aVn8jZqjPCuVwHQx7aZ3QcPubk1IiIiIiLnH5tlWdbp7nT55ZdzzTXXcOedd5KcnEzbtm3x9fXl2LFjvPzyy9x1113V0dYqkZqaSnh4OCkpKTVqvNPRv7UmqiCJ+b3fY/CwUe5ujoiIiIhIjXc6scEZZZzWrl3LRRddBMAXX3xBdHQ08fHxfPDBB/z73/8+k0NKBdLCWgGQun+jm1siIiIiInL+OaPAKTMzk9DQUAAWLFjANddcg5eXFxdccAHx8fFV2kAxfGPaA+B1bLubWyIiIiIicv45o8CpZcuWzJ07lwMHDjB//nwuv/xyAI4cOVKjur/VJPWadwYgJieeI6nZbm6NiIiIiMj55YwCp8mTJ/Pwww/TtGlTevfuTd++fQGTfXIUepCqFdjAZJxaeR1i7f6Tbm6NiIiIiMj55YzKkV977bX86U9/IiEhwTmHE8Cll17K1VdfXWWNk2LqtQagvi2Zzbv3MaRjLZ67SkRERETEw5xR4AQQExNDTEwMBw8eBKBRo0bnfPLb84p/KJmBsQRlJXB870agr7tbJCIiIiJy3jijrnp2u52pU6cSHh5OkyZNaNKkCREREfztb3/DbrdXdRulkC2qLWAKRGTnFbi5NSIiIiIi548zyjg98cQTvPvuuzz33HNceOGFAPzyyy9MmTKF7Oxs/vGPf1RpI8UIaNAe9i+hOQfZdCiFnk3ruLtJIiIiIiLnhTMKnN5//33eeecdrrzySueyzp0707BhQ+6++24FTtXEVr8dAC1th1gdf1KBk4iIiIjIOXJGXfVOnDhB27ZtSyxv27YtJ06cOOtGSRkKu+q18jrEmnhV1hMREREROVfOKHDq0qULr732Wonlr732Gp07dz7rRkkZokxlvVjbCXbsO4hlWW5ukIiIiIjI+eGMuuq98MILDB8+nEWLFjnncFq5ciUHDhzg+++/r9IGSjEB4VihsdjSEqibtZf445k0rRcMGcch7TDEdHJ3C0VEREREaqUzyjgNGDCAHTt2cPXVV5OcnExycjLXXHMNmzdv5sMPP6zqNkoxjsp6Lb3MOCfsdvjgKnirPxzZ6ubWiYiIiIjUTmc8j1ODBg1KFIHYsGED7777Lm+//fZZN0zKENUW9iyhle0Qa+JPcG3oFkjaaNYdXAWFBSRERERERKTqnHHgJG4S1QaAVrZDfLDrGFbadGyOdcd2uK1ZIiIiIiK1mQKnmqZYZb3Qk1uxZf5UtO6oAicRERERkepwRmOcxI0KM04Nbce41+crsyyiibk/tt1NjRIRERERqd1OK+N0zTXXlLs+OTn5bNoilRFUB0KiIT2JId6rzLJhL8Ls6+FkPORlg2+Ae9soIiIiIlLLnFbgFB4eXuH6sWPHnlWDpBKi2kB6EgCr7G1p3/gSggPCITsFju+CmI5ubqCIiIiISO1yWoHTzJkzq6sdcjqi2sJeM7bp7fxhXL/nBJfVawMHfzcFIhQ4iYiIiIhUKY1xqonqtwfguH8jFtu7s2T7EYhqbdapsp6IiIiISJVTVb2aqPMNcHwXe0Mvwf5NNku3HcG6qLUpS35UBSJERERERKqaMk41kV8QDP4HHXoOxN/Hi8Mp2RzybWzWHdvp3raJiIiIiNRCCpxqsEA/by5oXheAn09GmoXHd4K9wI2tEhERERGpfRQ41XAXt4kC4Jt4H/D2h/xsSN7v5laJiIiIiNQuCpxquIFt6gOwKj6VgjotzEJ11xMRERERqVIKnGq4pvWCaV4vmHy7RZKfY5yTCkSIiIiIiFQlBU61wKXtTNZpY060WaCS5CIiIiIiVUqBUy0wqJ0JmH48Vlgg4qgCJxERERGRqqTAqRbo0SSSyCBfNuaYzBPHtoNlubdRIiIiIiK1iAKnWsDH24uL29Znt9UACxtknYTM4+5uloiIiIhIraHAqZa4vH00OfiRYCvMOh1VgQgRERERkaqiwKmWuKhVFH7eXmzPjzELVFlPRERERKTKKHCqJYL9fejXsi67rIZmgeZyEhERERGpMgqcapHL2kcXBU7qqiciIiIiUmUUONUig9pFs9VuJsG1H1oLdrubWyQiIiIiUjsocKpFosMC8GnQmUzLH6/skxrnJCIiIiJSRRQ41TIXt2/IOntL8yR+hXsbIyIiIiJSSyhwqmUu7xDDKqsNAHl7l7u5NSIiIiIitYNbA6effvqJESNG0KBBA2w2G3Pnzq1wn6VLl9K9e3f8/f1p2bIls2bNqvZ21iSto0M4ENoVUOAkIiIiIlJV3Bo4ZWRk0KVLF15//fVKbb93716GDx/OxRdfzPr167n//vu57bbbmD9/fjW3tOaw2WzEdepPnuVNUFYiJB9wd5NERERERGo8H3e++NChQxk6dGilt3/zzTdp1qwZ//znPwFo164dv/zyC6+88gqDBw+urmbWOIO6NGfzr03pattN9u5fCOgx2t1NEhERERGp0WrUGKeVK1cyaNAgl2WDBw9m5cqVZe6Tk5NDamqqy62269AgjO1+HQBI2rTEza0REREREan5alTglJiYSHR0tMuy6OhoUlNTycrKKnWfadOmER4e7rzFxcWdi6a6lc1mw7vZhQD4HfrVza0REREREan5alTgdCYmTZpESkqK83bgwPkx5qdN78sBiM2NJzvlqJtbIyIiIiJSs9WowCkmJoakpCSXZUlJSYSFhREYGFjqPv7+/oSFhbnczgcdWzZjr60RAFt+U/EMEREREZGzUaMCp759+7J48WKXZQsXLqRv375uapHnstlsnKjbA4DkbT+5uTUiIiIiIjWbWwOn9PR01q9fz/r16wFTbnz9+vXs378fMN3sxo4d69z+zjvvZM+ePfzf//0f27Zt44033uDzzz/ngQcecEfzPV6d9gMAqHt8LTn5BSU3KMiDYzvPcatERERERGoetwZOq1evplu3bnTr1g2ABx98kG7dujF58mQAEhISnEEUQLNmzfjuu+9YuHAhXbp04Z///CfvvPOOSpGXoUnXSwFozx5+2bK/5Ab/uw9e6wmb557bhomIiIiI1DA2y7IsdzfiXEpNTSU8PJyUlJTaP97Jskh9tjVheUd4PeZvTLjz3qJ1SVtgRj/AgiYXws3fu62ZUsPkZoLNC3wD3N0SERERkbNyOrFBjRrjJKfJZiO/3VUADE6YwfGUtKJ1S6cBhTFz/HI4sefct09qnoI8eKMPvHEBFOS7uzUiIiIi54wCp1quztAnSbZF0NJ2mN3fvGAWJvwBW78BbBDVzixbP9ttbZQaJPUwJO+Hk3shaaO7W2PkpLu7BSIiInIeUOBU2wVGsKnjIwB03v0W1sl4WPqcWddxFAww61g/G+ylFJCo6ZL3w8l97m5F7ZGWWPQ4fqX72uHwyyvwXBzsWebuloiIiEgtp8DpPNBpyB38brUjgByyPh4D278zY1QGPApthkNAOKQegj1Lq/7Fv30QXu/jnqxAfi68PRDe6g952ef+9WujtISix/s9IHDatxwsOxxa4+6WiIiISC2nwOk8EB7sx+Jmj5BveRF0rLB7VafrIaq1GeDf6XqzbP3HVfvClgUbPoGj29xzYXt0G2Qeh+wUSDlw7l+/NkovNgH1/pXmO3anjCPmPifVve0QERGRWk+B03niTxf2592CoQBYNm8Y8H9FK7uNMfdbv4Wsk1X3ohlHIS/TPE6Or7rjVlbSpqLHCpyqRvGMU8ZR9xcVyThm7rNT3NsOERERqfUUOJ0n+rWox5zgMfy34E9s6vQY1G1RtDK2K9TvAAU5sPGLig+WvN/MAZW0ufztio8tOumOwKlY+1IOnvvXr42Kj3EC93bXsywTvAFkK+MkIiIi1UuB03nC28vGsB4teSjvbp4/McB1pc0G3W40j1e+bqrulefXN2HNLPjoWkhLKnu74sGSOzJOicWqvilwKl9uBhxYBXZ7+ds5Mk5hDc29OwtEZKdAQa55rK56IiIiUs0UOJ1HrusZh7eXjV92HWPt/lO65HW+AYLqmjLTbw+Abx+AjOOlHyhhg7lPOwxzxpu5fUrjzoyTZZ3SVU+BU7kWPAnvDjKFQ8rjCJQ7XG3u3ZlxcnTTA3XVExERkWqnwOk8ElcniFHdTabg5QU7XFcG14U7lkGHa0yVstXvwfTuJbvjWVZRJsfLF/avgPlPlP6CxQOnc51xSks0hSEcNMapfI5g+NDa8rdzZJzaXQnY4MTu8rOO1clRGALUVU9ERESqnQKn88w9l7TC19tknX7bc0pGKSIOrpsJ47+HqLaQnQyr3nHd5uQ+yEkBbz+49l2z7Pe3YP0nJV+seOCUngR5WVX4TirgzDbZzJ0yTuVzZARP7C57m7wsc06AqcgY3cE8PvBrtTatTI7xTaCueiIiIlLtFDidZ+LqBHFDrzgA/rlgB1Zp5aSbXggXP24eH1jlus6RbarfDtpfZeaCAtO1LyvZddtTs0zJ5zDr42hno57mPuWQ+0tne6rcDMgs7PZ2vJwqeY7CED4BEBABjfua5+4a55RePOOkrnoiIiJSvRQ4nYcmXtwKPx8vft93gl92HSt9o0a9zf2RzZCTVrQ8sbBwRExncz/gMQhvDPlZRd29wEw+68jyhESb+3PZXc+RcWp1OWAzFQMzyniv57viAe2JPWUHmI45nEJjTEGRxheY5+4a51T8+8xNB3uBe9ohIiIi5wUFTuehmPAAxvRpDJSTdQqLNQGRZXedvDbhlMDJywtiCx+XmDfJAt8gaNjDLCvede9sJGyAryfCkmdN+fSEDSZQKy6xsC0NupkLfWebpITiAW1ehuskt8U5xjeFxpp7R8Yp8Q/X4PpcKT7GCdRdT0RERKqVAqfz1F0DWxDo6836A8ks2X6k9I3iepn74t31HF3gHMESFAVRicUCJ0eQFNEEIpuax1WRcbIs+OYeWPchLHse/nsrvNUf/nNxUcYhLwuO7zSPoztAeCPzWOOcSpe83/V5WZPaOrrqOTKI4Q0hojC4Priq9H2qU/ExTqDueiIiIlKtFDidp+qHBjC2XxOgnKyTo7vegd/MfcYxU4IcG0R3LNoupvBx8XmTHIFTZFMTPMHplSTPz4HjpRQq2P+ryTD5BEC3m0zWw9vfZLu2f2+2ObLVXMwH1jHZEQVO5Ts1E1ja5w4lM05QlHXat7zKm1WhU7teqrKeiIiIVCMFTuexv/ZvQbCfN5sPpzJ/c2LJDeIKA6eDhROjOsYw1W0B/iFF2zmCqKPbirrMOQOnJuYGlc84WRZ8PtaUQ9/0X9d1v75h7jvfAFe9BrfMg74TCtfNMPeOLoMxHc1YnPMpcMrPhcwTp7ePI+Pk7Wfuy8w4FRvj5NCscDLlnQtO7zWrQrq66omIiMi5o8DpPFYn2I9b/9QMgJcX7qDAfkrWKaYT+ASaEtTHdxYrDNHJdbuIxuAfDvY8OFY4P9TZZJy2/g92zDOPf3gUsk4W7b/tW/P4gruKtu91G9i8IX65Ce4cc09FF7Yz3FQRrPVjnPJzYdYw+GdbOLG38vs5AlpHsYeySpKXlnFqPRiwmXMj5dBpN/msODJO/uHmXl31REREpBopcDrP3XpRc8ICfNiRlM63fxx2XentCw27m8cHfi/qihfT2XU7m61kdz3HxXhkUxNYgQnAKrq4zc2E+YWl0L18zTiWRVPM89/fNl3wml9syqE7hDeEDiPN41/fLBpr5WjTucw4JW6ExVPN+zjXlk4z2cGCHNj9Y+X3c2Scml9s7ssqSe4Y41Q84xRcD+L6mMc7fji99p6NvGwznxiYDCioq56IiIhUKwVO57nwQF/u6N8cgH8t2kl+gd11g0aFBSIO/l5UUS/2lMAJirrrObrJFc84+YdAUN3C5RVknX552WSGwuPgL5+aZWtmwa5FsPZD8/yCu0vu16cwA7XpC0hY79qmcxk4ffsA/PxP+PX16n+t4uJXwvJXi54Xr4RYnuzUooxe84HmvqyS5KUFTgBthpr77ecwcHLMO+XlWxSYq6ueiIiIVCMFTsL4C5sRGeTLnmMZzF1/StbJMc5pz1I4vss8PjXjBEXd9xI3mgtxR2bJcVEbUYlxTsd3w/J/mceDn4WWg6D7WPP8k7+YDEPdlmb5qeJ6QcOeUJBr5vTx8oGoNmado6texhGTqagu6Ufh4Grz+I/Pz92Euzlp8NVfTTaubkuzrLKBkyPbFFjHBJo2r9JLkudmFmV4ygqc9v507sqSO8Y3BUdBgLrqiYiISPVT4CSE+Ptw5wDT3emVhTvIyMkvWumorJe8H7AgJAZC6pc8SPGueo5sU3B98As2jyMrMc5p3iQT+DS/GNqNMMsGPQNB9Uz3M4A+d5q5o0pTfNxTvTbg428eB0aa+aQAUssZh5OXdXaTqO5aCBQGS8d2uE4IXJ3mPWYC0vDG8JfPzbKj2yvXdc0ROEU0Bh+/oiDz1AIR6YXZJt8g8A9zXVevNdRpbr670+kieDYc45uC60FAYXsUOImIiEg1UuAkAIzt25SGEYEcSs7ihXnbilaEREFks6LnpXXTA4hqZwo0ZJ0wJcOhaP4mKJZx2l9iVwDiV8DO+abr1bAXzbgpgKA6JvsEJrPQZXTZb6L9VUWFC2KKlUuvTGW9fcvh2Ybwj1iYcSF8cQusfP30MiiOghZePub+j88rv++Z2rMM1n0E2ODqN814n/DGgAWH11W8v3MsWuH34xgvdGpJ8uLd9BzfjYPNBm2GmcfnqrteRikZJ3XVExERkWqkwEkACPTz5rlRprvd+yvjWbn7eNFKR3c9KFlRz8E3AOq1Mo+3Fla+c1yMF39cVle9DYXjmTrfUHQch87Xw6h34cavXMugn8rbFy5+3HQ3a3el67qKAqc1M8EqMJmtpE2mDPr8x+FfXUwAVVEXv/xc2FWYbfnTA+Z+0xdnl8GqjNXvmvueN0PTC83jRj3M/aHVFe9fPOMEJnMEJTNOjop6Iad003NwdNfbMb/63zMUTX4bUl9V9UREROScUOAkThe1imJ0b3MB/X//3VDUZc8lcCoj4wRFQdX+Fea+tIxTaV318nNhy9fmcefrSq632aDTtUUBQXm6j4Unj0K7K1yXOwKn0rrq5eeYC36A696H0Z/BoClmvFDmcRNATe9e/iSv+1dAbprJgPR/xHQPTE+CvcsqbvOZyjxRlOHpeUvR8oaFn9PBSoxzcnwfju+nTmHG6dSS5KXN4VRc3AUQEGEyjgd+r/h1z1apXfWUcRIREZHqo8BJXDw+rC0NIwI5cKJYl71GxQKnsrrqQVEVO6uwMl/xwMnxODm+ZNGE3YtNqfKQaGh60Vm0vpC3T8llYY6MUylzOe1ZZrp5hcaaTFWbISZrdPdvcOV0s2/qIfj+kbJf0xF4tRpsxlZ1uMY8r87uehu/MOOKYjq7ZgIb9jT3h1ZXXKDCmXFyBE6FGadTS5KXNodTcd4+hXM6Adu/r1z7z4azOER9ddUTERGRc0KBk7gIDfDl+VEmOHp/ZTy/7jkO0R1MwYaWl0FE07J3Lj6uCIouxqEw42ODvMyibIHDxi/MfYdrwMv7rN9Dqcrrqre1MNvV9grXwhPePiaDdcdS8/zIZsg4XmJ3oGh8kyN46HxD4bH/V31zOq3/2Nx3HeO6PLaLGW+WnlR+MQzLKuo66eiq5xjjdGpJ8rJKkRfXeoi5PxeBk6OrXnBUUbEKddUTERGRaqTASUr4U6t6zi57T87dRJ5lg7Fz4cYvyq5oByW78RXPOPn4Q1gD87j4OKfcjKIL7U6ldNOrKmUFTgX5sK3w9dufMi7KISTKFL8AiC+lu96xXSbQ8PKFFoWTyMb1NoFjbnr1BBJJm818VV6+JT83vyCIbm8el1eWPDu5KEtTvGx8aSXJK8o4gSkT7+1nytY7yspXF+cYpyh11RMREZFzQoGTlOqxIW2pG+zHriPpvPfL3srtFFLfdJ0Cc0HvCJQcnOOc9hUt2/6DyUJFNoOG3c+63WUqHjgVz6TELzfjcgLrQON+Ze/f9E/mft8vJdc5sk1N/wT+oeaxzWaKWgAs+QfMfwJ+/48ZJ1UV8zutn23u2wyB4Lol1zu66x0sViAi4Q9YMR0K8sxzx/im4CgTbEHZJckdQVRodNltCggzY8MAFk4urPZXTOYJSNpigtWzVTzjpK56IiIicg4ocJJShQf5MmmYybL8a/FOElKyKrejo7teROOS3e4cWY3iGaeNc8x9p2tLlrmuSmENzX1eppmg12HrN+a+7fDSx0Y5OCrWlRc4ObrpOXT+s+kyd2IPrHwNvn8YZg2DH/92Zu/BoSAP/vjMPO56Y+nbOApEODJOaYnw4UhY8CT8+oZZdur4JgfnOKdiBSKcXfXKyTgB9J0AF95nHn9zr8nmHd0O39wD/2wLM/rC803g/SthybOwfR6kHDq9YNJuL1Ycon5RV738bFPoQ0TOTG4mfD0B1n3s7paIiHgkBU5Spmu6NaRnk0gycwv4+7dbK7eTo0BEZJOS6xzLjm43F8qZJ2DXIrOsOrvpgSmX7siGOQpE2O1FpdPbX1X+/k0KM05HNpt2O2SnwP6V5vGpgVO9lnD7Yhj2EvSdCK0LS3b//PLZTRS7c6HJuATXN93jStOoMON0eJ0JtObeZSoEgsk65WaWLEXuUHycE0BOelE2p7wxTg6DnjEBnVUAn98Er/eGtR+YUu8+gab74t5lsOx5+OQGeKU9vNAcPh1Tue52WSfNscFU1XNk+UDd9UTOxvqPTaZ48VR3t0RExCMpcJIyeXnZmHpVR7xs8N3GBH7acbTinTqMNBf0Ha4uuc6RyfjjM1Pee+5dYM+H6E4Q1aZK216qU8c5HVwF6YkmY9FsQPn7hkRBVFvzuPg4p03/Ne+hXpui91dcg27Q+3YY/A/4y6fQ42bAgi//WlQZ7nScjIcV/zaPu9xQdpasXmvwCzUZtm8fMIGaT4DJGGUchTWzSk5+63BqSXJHNz2/ENcgpSw2G4z4l5kU154P2EzhjVsWwOOH4M7lMPxlk5Gr36Fo4uRt38Kq/1R8fMfkt4GRZu4uL++irJO664mcGcuCVe+Yx+mJZvypiIi4UOAk5WrfIIxx/ZoC8PhXG9l1JL38HRr2gId3mGp0p2o73FSA8w0y2QxHF7dO11Zto8viCJyO74L0o0Xd3VoPMWN7KtLklO56lgW/vW0e9xhfuTYMmQb125uL/6/uNFmv8liWqeS3Z6nJyPy7q8lwefmU3U0PTDDRoKt5vO5Dc3/532HgJPN4+asm8wclM06nliSvTEW9U3n7wHWzzMTF96yBP38MjfuYdsV0hF63wjVvwd0r4PHDMPQFs99vb5Xsbrf0OdPN7+gO87z4+CYHZ2W95Mq3UUSKxK+Ao9uKnp+o5NhWEZHziAInqdADl7WmYUQgB09mcdVrv/DNhsPl71DWWCX/UBj5Bjy8E675jylv3rgvdLup6htdGkfRg4WT4aWWsPpd87ysanqnchaIKMw47f0Jjm4F32DoNqbs/YrzDYRr3zNd1nYvLsoeFVeQD9//H7zSCf5eH15sDh9cZTIylt2Uhr9pLtRvW/5rObrrgZlfqtdt0GW0+RyKT857auDkyP4lbYSvJxZlnkJOI3ACU0mx07VFXf/K4htgJvANbWDa5Rj3BpC40XTpS0so9mt4sTmcHFRZr/bIyzY/bHiCvGwzDi8nzd0tqX6Of18OJxU4iYicSoGTVCgswJevJvSjb/O6ZOQWcO8n63j6603k5Bec2QH9Q0zFuRu/gFvmlV4Vrjq0uMRU+3PwCYS4C0wAVxmOwClpkxnn9Ntb5nnXvxRVdquM+u1g6HPm8eJniuaxApNh+u5B+P0tSNlvJrgFk13pMR7u/tWUhm9WiYmCHVUCg6PgqtdNQOvjZyb3Le7UubnqNIOLnwBsJlv1beH2p5NxOl3evnDBnebxitfM52BZ8MOjRRMqb/7KBJXOwhD1ivZXZb3a4/Ox8EoHOLKt4m2r056lMKOfGYf36ZiqqYbpqdKSigrlOKZeUMZJRKSEcsqIiRSpHxrAh7f25pVFO3h9yW7eXxnP+gPJvD6mO40ig9zdvMppNQgeKyyI4BNQ/pxUpQmpb8YOHdsBGz4pmp+p9x2n35bu4+DwelgzE768wwQO7a+Cn16Cte+buZSunG7GXoVEV64r4alaXQZXvWHmlAop1q2t243mddIKM4eOLozFDfg/aHyBGYvl2K46AycwgeGyF00Wb9diyEkx48l8As37zzgC8b8Um8OpWMZJk+DWDke2wc755vHmL6H+4+e+DelHYcETRV15wWRndy4oWQCmtlj3gRmP2Kg3NB9g/g0Wn45AREQAZZzkNPh4e/HI4La8N74n4YG+bDiYwvB//8KP25Iq3tlT+AWZ2+kGTQ6OrNPivwGWyWJFtT7949hspkBC1zGmQtwXt8C3D8KSv5v1Q18wAU5E3JkFTY7X6DYG6rVyXe7jX5R1CmtousqVpll/uGu5Cehs3kVjvKpLQDj0GGce//wSLHjKPP7TA0XFRjZ+UVQcovgYJ3XVqx3WflD02DEGsrgVr8G/u5sJoKtaaoKZb+1fXQqDJhv0/mvRDyMLnqqaOciqSkE+bPnGVL082+OsnmUe97rNzKkH6qonIlIKBU5y2i5pG8139/6JLo3CScnK45ZZq3lx/jbs9lrclcXBETjlF85r1efOMz+WV2FWqeO15tdex5irC+83lfiqU4/xZr6lIc+Vv11QHbj+A1MNr+2w6m0TmM/T5m0KYKQegvDGcOG95jMC050otTAD5hI4qaue2+2YDy+1ga3/O7P987Jhw+yi5wkbir5rx/qfXjBj7r65t+LCKpWVfsR0R/1XZzPfWl4GxHaF2xbDsBfgkifNBNnHtpts8JnKOmnmNZv/BLx7Ocx7/Oy6/634tyn3/7/7zvwYYDJ8qQfNe2x/VVFxGHXVExEpQYGTnJFGkUF8fmdfxvU1paxfX7Kbez5ZR3beGY57qikc8zmB+WW2suOjyuLlDVe/VTSPVOcb4NKnz+6YleHjB5dNrXxhDN/A6m2PQ0QcdLym6Pngf5jXbtLPFKfITjFjT6CMqnrqqucW+blmguf0RFg05cyCmm3fmuAirBE06G6W7VxQtH7n/KLv99BqWDvr7NpsWSaD+XpvWP2eGU8YdwGM+QLuWAqNCieRDggvqka5dNqZZTV//w883ww+HW2CswO/wa+vw5J/nHnbHdUyN39ppik4k2Ns+hK+e8g8736TyT7XKcw4pRww36uIiDh5ROD0+uuv07RpUwICAujTpw+///57mdvOmjULm83mcgsIKKOrkVQrfx9vnrmqI6/c0AVfbxvfbUzgxnd+42RGLf6fbWi0GecE0OevZ97lrzhvH7jufZjwuwmiquKYNdmfHjAl69sMg3YjzDIv76KAyl7YXUpd9TzHmllFEyof31U0sfXpHgNMF1VHdnPH/KL1Gz41946uZIumnHn1vbQk+OxG+O+tJliL6QTjv4db55uxgadWBu15s5nfLOMoLP/X6b1WdiosegawoG5LM77xwsIs0U8vmglnT9ehNUVjkCw7/Drj9PY/ut1U6vziZlOxMrIZXHC3WRcSbf79WfaiycJFRATwgMDps88+48EHH+Tpp59m7dq1dOnShcGDB3PkSNmTg4aFhZGQkOC8xcefwa9tUmWu7taI92/pTWiAD6vjTzJqxgrWxJ/Eqq1VqK6cbn6B7nlL1R3TZjNlwMsq5X4+ie4Aj+yC6z90/Tw6jnLdLqSUrnrKOJ17uRkmAICioObXN07vGMd3w76fTVGUbjea8vlgsot5WWYuM0f26c8fm0AnOwUWPnV6r5OdAkumwfQeJsPl5QsXPwm3L4Gm5Yzh8/Y1GVowGaOT+yr/mus+gtw0M0n2xNVw5b/NsS562Kz/332wewkkbYElz8KMC+HlDvD2QPj4etONMPmUAMYRRDomq177gQkAK2PzV+Y19i4Db38Y8BjcvbKo+IvNVvQ9qkCEiIgLtwdOL7/8Mrfffjs333wz7du358033yQoKIj33nuvzH1sNhsxMTHOW3R09DlssZSmX4t6/PeufjSMCGTPsQxGzVjBZa/8xFvLdnMkLdvdzatajS+AgY+ZIgtSPfyCTSauuIY9ILJp0fPSuuppjNO59+sMU7Ajshnc+F8T/OxZAke2Vv4YjrFDLQeZ7poxncy8XnmZZsLpTf81mcbYriawvuJVwGaqWzompC5PThr8/DK82hmWPWcCmQbd4K/LYMAjJjCqSNvh0PQiyM+Gr+4CeyW6JdsLzNQCYDLUxX8IuOTJovGNH10DM/qaOcuSNpkxR4fXme6Jq9+Dr+8uGg+Vn2s+DzBFZOp3MOOyVs+suD0bv4AvbgV7nvmsJ/wGF08q2RXX0V1P45xERFy4NXDKzc1lzZo1DBo0yLnMy8uLQYMGsXLlyjL3S09Pp0mTJsTFxXHVVVexeXPZFZZycnJITU11uUn1aB0dyld39+Oa7g0J8PVi15F0pv2wjb7TfuS291cxb1MiuflVNKBbzj82W1HWyScQ/EKK1p0vGaej22HWFaZse15W9bzGkW3mwrysCnLFM8mZJ2B54STOFz9hJjtue4V5Xrz72ME1Zn6mb+6BNe9D4iYTzKQcNI/XFxaF6F5YVdFmKyr9vWMe/FGYYenyZ3PfqKcpcALwyV/gx3+Ytpwq47jJ4rzS0cyZlp1sMj/XfwC3/WiCsMqy2Uy22S8E9q+oXFZtx3yTnQoIL2p78eONfMPMt2bZwdsPWg813XVv+xFGf2oqb/oEmMm2N39l9tu1CLJOmC51zQdCv3vM8t/egvycorFb7w421Tq3z4OCPNjwGXx5u6ni2XUM/OXzogDpVI7lqqwnIuLCrfM4HTt2jIKCghIZo+joaLZtK33ywzZt2vDee+/RuXNnUlJSeOmll+jXrx+bN2+mUaOS89FMmzaNZ555plraLyXVDwvg5eu7MuXKDnz3RwJzVh9g7f5kFm09wqKtR6gb7MeYC5pw20XNCAuoxK+8IsV1/rMpSR3TyfXX+/MlcFo81XRp2/ezydIMmmKCybPt4mkvMF3hfnuzqPjGnx4wxy/u1xmw4EnTRaxhD/N556RAdMeioPaCu031wz8+M4VOdi4w3dEKcsz64iXHiwuJdp0nqfUQM8/ZxjnmdWzeRdUVAQY9DYfXmup7P71gApkuo022MjfdBFI75pmsFZg2D3gUOl1rxsydiTrNTMGS/91npiRoOchMaF2W3wqDx+7jTLtO5eMPN30J8StMMFjaRNoZR01RigVPQqvLi4LITteZrGzHUea8SDsMK6bDwVVFpdwPYILgwEjISgYs6D4WrvhX+WMp1VVPRKRUNsuNA1EOHz5Mw4YNWbFiBX379nUu/7//+z+WLVvGb7/9VuEx8vLyaNeuHaNHj+Zvf/tbifU5OTnk5OQ4n6emphIXF0dKSgphYWFV80akXLuOpDFnzUG+XHuIo2nmuwgP9OWvA5ozvl9Tgvw0D7OchuO7ISACgusWLTuyDd7oYy4QH93nrpZVHcsqGQylHIJXO5rsRGisGdQPENcHBk8rqgLnkJtpgobgeuW/1q5F8P3/mTLfYLraWXYTqPx1mQlSwQQo/7mkqDhHcX/5vCjosSwzPidhvQmokjaZ5a2HmCDj0Bo4tM50l/PyMd9ZYB3o/wh0vs61/S80M13jwIx7GvO56+va7SZI++klSNpY+vuL6QwXPQjtrjzzgKk4y4LZ15uAMKYzjHrXdE3cucBk0S6421TJPLIFZvQzn+N9G0wXxDORl2Uq/yXvh563mjFTBTnw158htrPZZvm/YOHkon28fE0Z/9xMEzg55j7reQsM+2fFBWh2/wgfXl04LqvsYk0iIrVBamoq4eHhlYoN3HrFWq9ePby9vUlKcp1ANSkpiZiYmEodw9fXl27durFr165S1/v7++Pvr7Eo7tSyfiiThrbjkcvbMG9zIq8u2smuI+m8MG877/y8l1HdG3J9zzhaRYe6u6lSE9RtUXJZ8ap6pQUdNcmvM+CXV2Hoc0UT/4KpOmfZTUn8G78wmbdfXjalrd+5xGTjLp1s5j5a96EpNV2QA6M/MZmRU6UfhfmTTEYHTDDaY5yZBHX+EyYg+eYeM59RQR58dacJmtoMN9sdWgOH1pqiJq0uLzquzQZ9J5huYY6gacCjpgiB44LdbjdzofkGlf1d+QVBswFmnA+U7OoG5ngdRppAZcc8EwR6+5vsjn8IxHYxx6jK88HRZe+NCyDxD3i9l+v6A7+ZsVCODFO7K848aAIz/mjIc/DpX4rmeqvfviigBdNt8aeXzBi/Rr1M+xyZsMv/bgpBZKeY86kyn4VjLqeT+8x3db5X+hQRKeTWjBNAnz596N27N9OnTwfAbrfTuHFjJk6cyGOPPVbh/gUFBXTo0IFhw4bx8ssvV7j96USVUj0K7BZfrz/Eq4t2sv9EpnN517gIru8ZxxVdYtWNT05PbgY828A8nnTIXDTXRL/OgHmFf/cCIkwVtpAoE7i80gHSk+Da94q6xaUmmG5ajoljHdmi4vxCTZnt4uN5/vgcvn/EjPmxeZmJhy9+HPwLf7xIS4TXeptueIOfNc9X/NsU5Lj714qzWPm58PYAkyUZOaPy84WdatW78N2DpvjHwzvO3XxilbHla/h8nMliNe5rgsecNPM5ObJkADfPgyZ9yz5OZVgWfHxtUZn3QVNMV8riEv6A5HhTxv9sM2sF+fCPaBMoP7AZwkt2gxcRqS1OJzZwe+D02WefMW7cON566y169+7Nq6++yueff862bduIjo5m7NixNGzYkGnTpgEwdepULrjgAlq2bElycjIvvvgic+fOZc2aNbRv377C11Pg5DnyCuws3X6Uz1cf4MdtRyiwm1MxwNeLYR1jub5XHH2a1cFWk7MHcm5YFkytawa+P7gVwhq4u0Wn7/f/mElkwYx1yU4xY3auftMUBpgzHoLrmwtZHz/XfQ+tgXmPw4FfTeGM9ldB19GmiET8L2ZS2dsXm8Do+0dg/cdmv5jOMOJf0LB7yfasmWXG8vgEmKIDWPDn2aa6XGXkZpoL+LOpPpmTBl9PMN30uo058+NUl5P7TFfD4mOTTsabMulbvoYmF8L476om43V8t8lyWZbp+hfe8OyPWZ5/dzNjnMZ9C80uqt7XEhFxoxrTVQ/ghhtu4OjRo0yePJnExES6du3KvHnznAUj9u/fj1exbgInT57k9ttvJzExkcjISHr06MGKFSsqFTSJZ/H19uKy9tFc1j6aI2nZzF13iM9WHWD30Qy+XHeIL9cdokndIK7r0YhRPRoRG+5BvzaLZ7HZTHe9rJOmu547AqeCPJNtaNAdWlxc/ra7FpvS04F1zK/5Nq+istV/esBUpntnkCm33XWMybyA6SJ3atAEplDDLfPg2E4zSbPjQj6mM7x7mZmU9uPrTAB0bLt5vQGPwUUPlSz77tBtrMlMxS83z7veWPmgCUxXu7PlH2oq4Hmq4uXxncuamDYn7zffb1X98FO3Bdy6wGSDqjtoAtNd78Qec1PgJCICeEDG6VxTxsmzWZbFugPJzFl9gP9tSCA9xwxE97JBm5gwmtULonGdYJpHBTOgdRTRYQFubrF4jFc7m65KtyyAxn3O/euvfMOMGfL2N0FMaVkcMIP9p/eA1EMl1/WdaMak2Gxm4tPV75lsUepBE+zcv/H0u02d2GOCsMzj5nloLIx6B5r+qeJ9j+00hR6Co+CvPxWNJZPa77uHYdV/Sq+uKCJSi9SojJNIcTabje6NI+neOJKnrmjP9xsT+Xz1AX7fe4KtCalsTUgtti10bxzJ0I4xdGwYjo+XDS8vG4G+3rSJDsXLS138ziuOLIs7JsHNPGEySGAKMnx2k6lIV9pYoF9nmKApPM5clKYeMgUdGvU0VdMcGYpLJ8PW/5mgCcwcP2cy1qROc/jzJ2YepYbdTeGAisYoOdRrBff9YbJc/irecl5xFIjQJLgiIk4KnMRjBfn5cG2PRlzboxEHT2ayPTGN+OOZxB/PYMPBFNYfSGZN/EnWxJ8ssW/jOkH8pU9jruvRiLohqqp4XnDnXE4/vWgKLUS1g4JcU9r7i1vgxi9du8JlHIdfXjGPL3kKutxQ9jEDI+Hyf8BXd5jnvW458/Y17gMPbTuzbmPFy77L+cMxCa7mchIRcVLgJDVCo8ggGkW6jplISMli/qZEFmxJIjE1G7vdosCyOJ6ey/4TmTz3wzZeXrCD/q2j6NY4gq5xEXRqFK6KfbWVv6Mk+TkOnI7tgt/fNo+HPGu6wv3nUlMC+sepcNnUom1/esFkxGI6mwlMK9L5elPyOi8Lml9ydu1UkRU5HY5JcE/uq/kl/kVEqogCJ6mxYsMDGX9hM8Zf2MxleWZuPt9uSOCj3+L542AKi7YmsWirmSvMZoN2MWH8qVU9+rWoS8+mdQjx1z+DWsFdXfUWPW3KNre8DFoUBjcjXzdV8Jb/yxQJ6P9/prrcqnfM+sv/Vrm5cWw2GPyPamu6SJkimwI28+8p84QyjyIiKHCSWijIz4fre8Vxfa84Nh1KYeXu46w/mMyGA8kcPJnFloRUtiSk8vZPpgtKg/AAWtQPoUVUCG1jQmnfIIzW0aEE+J7lXChybhWfBPdc2fcLbPsWbN6mqINDh6vhyDZY9pwpJb75KwhrWBhgDYLmA89dG0XOhG+AqU6Zesh011PgJCKiwElqt44Nw+nYsGiOlSOp2azcc5zlu46xfNdxDiVncTglm8Mp2fy885hzO28vG03rmu6BDSMDaRAeQG6+neMZuRxPz8XXx4s/94qjX4u6mmfKU1RVV728bHOxmH4EMo9BxjHw8jFV6BzjPjKOw4p/mbmXwJQJr9/W9TgXTzITvy573szpk3oIsLl23RPxZJHNzHn76WiTTW1xCTS/2JS8L+7oDlM6Pz/HlC5vcqEqMIpIraRy5HJeO5mRy55j6ew+ksHOI2lsTUhj8+EUTmbmVWr/Tg3DuXNACy5rH42fTyW6Xkn1WfEaLHjCjB0a9U7l9zu5zwRA+3+FlAOQnlT2tpFNoUE32LkQctPNska94S+fQVCdsvdL2mxeI7YL9Ly58m0TcacNn5my+HkZrsujO5m5yuo0h41ziub6crB5mwqOzQZA8wEQ1we8/SDlICSsh6PbTIn7uq2gbktTtTFpCxzZYkrgB4RBRBMzJ1ZoA9PN1XHzC61cN1cRkUo6ndhAgZPIKSzLIjE1m91HMjiUnMmhkyYrFeDrRd1gf+qG+LH7SDqfrT5Adp7duV+9EH9iwwOICPIlPSef1Kw8UrPziQzypUODcDo0CKN9bBgto0OICvFXpqqqrf0AvrkHWg2GMZ+7rsvNgJ0LIP2o6XIUVA/sebB6Jmz/Hiy76/a+wRBS31zcBdczE+seXGW62jnEdoGLn4BWl2vgvNRe+Tlw4HfY/SPsXgwJG0puY/My/+5Co2HvTyUr8fkEgF9w0VxiZ8PLB0JizGsFR4G3r1nm7QcRjaFRL2jY0/w7tyzzbzc9CXLSID/bZJTt+ebfdWgMhESbgOxMZZ2EhD/gyFbzHiPiTDuC6kJupvnbk5tuuuqGRJ39+xf3s9sheZ+ZY6+0CcmlxlHgVA4FTlJVjqfn8P7KeD76NZ4TGbmntW9ogA8t64cQ4u9Dek4+6dn5FNgthnaK4dY/NadOsPljnJNfwNfrDrN0xxG6N45kTJ8mBPpp7FWpNs+FOeMg7gK4dT7YC2DXIvOL+LbvS/5qXlyLS6HrX6BuC/NLd2BkyWAoJ938sn5oDcR2hTZDFTDJ+SfjGOxZagKpYzvMmL1uN0F4w6JtkvfDnmUmiNq7rCiL6+UD9dtB/fYmiDq202yLBeGNIbo91GttAo2T8WZC6/QjJngryDm9dgbXN912K7NfYB1TDTM02tyHRBc9z8+FI5tNRuzYDtNWnwATqGUnF7a/ksIbQ8NuZtoCe35hIJdp2pl53BThyMs0f4PqFWbj/IIhK9lsk11473juHwpxvUxGL6YznNxr/j4dWms+syZ9TdavbsuSf6vsBabtJ3abH4rCG5r3bPM23TOP7TABcGisyRqezjxulmWOb883N8tu9i/t76XdbpZX59/SvCzz+WYcM/f2fPDyLgy4/SEstjCzWUEQlHIQ1n0M6z6ClP2mB8Lgaaf//4LsVDj4O8SvNMes3870ZIjtYrqc56SaW36OOTcDI08vy2ovMP9u0hIgLdH8MNCkH/gGFm1TkA8HfjOv0+KSkj8eHN1hMsDRHc3/Fyvz/izL/Fs/uc/M/3Zyn7n5+Jv31qAr1O9gzofcdPNjRm66eQ0v917XKHAqhwInqWqWZXEiI5eElGwSU7JJzsojxN+H8EBfQgN8SErNZvPhVDYfTmFbYhoHTmRiL+dfXZCfNzdd0ITwIF9mLd/HkbSi//HXC/Hj9ouaM7hDDHuPZbAtMY39JzLo1DCCEV1iCT2fS63vXgIfjjQXZVe9broYJawvWh/RBGI7Q+ZJyDhqfgluNQj63FVyfJKIVA3LMhfhuRnm36ZvgOv6vGyT/a3owtyyzIVk5jFzMZiWWHgRnGcuFPNz4Oh2c0F6bIfrvgEREBhhAh6fAHORlnHUHKPg9H70KlVkU3Pxl5dluvsmH4D8LJOJ8ws17zn9COCmy62QaBNIevuYYCE3A47vLiWotJmL3Pxs18VevubCu0k/sy7rpOst86S5AC/ILQqWTuXlazKEIVHmsSNQzEkpel1vf9NGy26+bzCZf0e3Tf9QE2gkHzD3Xt6mmmpAuFnn7Vd48zXtSincLqvkXI8l2UwGMqiuOYaPv/msCnJNMJuXZT6z0r7DloPggrvM+XRspwlG05LMOZZ53HzevkEmCPbxN+06tZdD8Xac+ho2b9Mu/xDTJi+fosDPccvPNgF11snCcb6nHMM3yBQlatYfEjfC9h8g64RZF1TPjNPtPtZklFe9Y370cAiMhIY9TODleI3cdPNvyTcQfALNspP7zHl/uv5vb/ld3c8BBU7lUOAk7padV0D88Ux2HUknO6+A0AAfQgJ8OJ6ey5vLdrP5sGtVuJiwAEZ0iWXe5kQOnCj7j1KgrzfDO8cypEMMgX7e2ABs4GWzYQNsNhuBvt60rB9SO7NWh9bAfy4x/xOxFwCW+fWu61/MuKeGPZQhEjkfZJ00v3gH1TVBw6nBmoOjK19aIqQXBmNpCeai1/FrvZd3UZYsqq25qC7IMZko3wCI7mAuLE89bn62ubB0/M3JTjU/5BxaY9rmE2D29wk0F/5BdUx7ffzN+uM7zRxxBbkm6AsINwGgI1AICDfB2IFfYf9vkHbYrG/YAxr1NBfb+3423SzLyrp5+5txavlZkHq4KIj08jXL6zQ349FO7j3778TdvHxN98zAOiazZM83/5/Iy4TUhMpnNJteZAKMFpfAytfM2Fp75cZEu4hoYgLROs3NGNjD60yG1cHb35wjOWdY7MjmXZg5jTFZoNRDJbcJiDCBT1pCKft7mXP+2M7Ty/bavCC8kfkxIbKZCXhz0s25f3h9UbAG4BdibncsMRU83UiBUzkUOIknsyyLH7cd4Z2f95KTX8CYPk0Y0aUBfj5e5BXYmbvuEDOW7mb/iUyaRwXTOjqUhhGBLNqaxO6j5XRFK8bLBi2iQujQIAxvLy+OpudwNC2HtOw8An29CfL3IdjPmyZ1g+jboh59m9clKrT8MQCWZZGalc+xjBwK7BYRgb6EB/ni71O5AC2/wE5ugZ0gv7Mo9HlsF7zWo+h5p+vg8n+UrAAmIlLbZCWbYOrUH4fyskwWITejKBvk7Q/1WkJ4XFEXKbvdZPNy081y78LeC5ZlMi07F0DSJvNjVGCk6y0oEvzDTUDi5WOCFC/vovFnYDIv6UdMFqYgzwSJQXVNUGhZRcGoPc9cfNu8TFYmLcF02zy5z7yH8IZmDFl4nNnP0YUxN93sX5BrjhUQYbYJb2T28Q8r+4czyzLtSj5gApX8wmMU5JlA1pFViWhsxrAVd3y3mcsvYYMJFOq1MkVPwhoUjpGNMpmmvMLxbnmZhe1qWLIdmSdMMBcQVtR1riDPdDHMOGK+S0fA53KfbwL6wEjzeQZGms/W8d1alvnuts+D/StNd9i2w6BxP7N++/dmEvd9Pxdln3rcbN5rfq7Z9/A68304vnO/EPPjQH62eV8BYeb9RzQuOndK+5wzT5jzxDfYo4q8KHAqhwInqQ0K7BbeXkX/E7AsizXxJ/ls1QE2HkrBssDCKrwHe+GD5Ky80x6PBdC0bhD+Pt4UWBZ2u0WBZZFfYGG3LPIKLJIzc8kvpf9hkJ837WPD6NWsDr2b1aFDbBhB/j4E+Hhht2D5rmP8sCmBhVuSSM7K48IW9RjZrSFDOsaUOjFxZm4+h5OzqBPs7xwH5pSfC29eaP6HO/R5zZUkIiJSWZknTEB0Hha8UOBUDgVOcj6zLIsjaTlsPpzC1oQ0vGw26oX4US/Un/BAX7LzCsjMKSAtJ49Nh1JZufs4WxIqP6FsqL8P3t42UrPyyh3HBebHv7L++gT4etG4ThB+Pl74+3iTb7c4dDKTY+lFQV+LqGB6N6tDvxb1GNYp1gSSdrtH/YolIiIink2BUzkUOImcnpMZuWxNSMXCjJfy9ip2K3weGexLnWA/Z9c8u90iLSefo2nZrN2fzO97T7Bq3wnij2e6HDsq1J8hHWIY2imG2PBAvt1wmK/WHWLPsbK7HToqERb34rWdua5nXBl7iIiIiJROgVM5FDiJuI9lWeTk28nKLSC3wE5UiD9eXrYS2+xISud4eg45+XZy8u3YbNAwIpC4OkGEB/pyMiOXVftO8OGv8fy88xjX92zEC9d2cdO7EhERkZrqdGKDsxiJLSJyemw2GwG+3gT4ll00wmaz0SYmFCi7PHFksB+Xd4jBAn7eeYxNhyrfnVBERETkTGgwgIjUWB0bhgOwIymN7LwCN7dGREREajMFTiJSYzUID6BOsB/5dovtiWnubo6IiIjUYgqcRKTGstlszqzTxkNnOFGgiIiISCUocBKRGq1TQzOQc5MCJxEREalGCpxEpEbrpIyTiIiInAMKnESkRiteICInXwUiREREpHoocBKRGq1hRCARQb7kFahAhIiIiFQfBU4iUqPZbDZndz3N5yQiIiLVRYGTiNR4qqwnIiIi1U2Bk4jUeEUZJwVOIiIiUj0UOIlIjecInLYnppGbb3dza0RERKQ2UuAkIjVeo8hAwgN9yS2wsyNJBSJERESk6ilwEpEar3iBCI1zEhERkeqgwElEaoUODcMABU4iIiJSPRQ4iUitoAIRIiIiUp0UOIlIrdClUQQAmw+nsjVB8zmJiIhI1VLgJCK1QlydIAZ3iKbAbjHpy40U2C13N0lERERqEQVOIlJrPHNlR0L9fVh/IJkPV+5zd3NERESkFlHgJCK1Rkx4AP83tC0AL87fzuHkLDe3yH3sdovkzFwycvLJK7BjWaefgTuensPK3cdZu/8kmbn5Jdan5+STkVNyeVWwLIsDJzL5YWMC8zcnsib+BPHHM6rt9SrTnoSULPYdyzijz7Isu46k8c2Gw5zIyK2yY5amrDZn5RZwPD2nWl9bRKS28HF3A0REqtKY3o2Zu+4Qa+JPMvnrTfxnbE9sNpu7m3XWsvMKOJKaw9H0bPIKLBzXwYF+3sSGB1AvxB8vG/xxMIVvNhzm2z8Ok5RadEFss4Gftxd+Pl74+3g5Hztvzufe5OXb2XkkjWPpuS77N6sbTJO6QRxLz+XAyUySM/MAiAzypVFkENFhAeTkF5CWnU9adh5Bfj50ahROl0bhtIsNIzElm+2JaWxLSuNYWg4+3jZ8vLzwLbz39rbh62UjOSuPPw6mlBlMBPp6UzfEj3oh/liWRWp2PqlZeRRYFj0aR3JRq3pc1DqKyCA/9p/IZP+JTI6m5VA/1J+4OkHERQZyPCOXlbuPs3L3cTYcTCYrr4CCAot8u0WgnzeN6wTRpG4QMeEB7DmawYYDyRxJM59ns3rBDO4Qw+AO0YT4+3AsPZfjGTlk5hYQHuhLZJAfYYE+xB/PZMOBZDYcTCYxJZv2DcLp0TiCLnERbD6cypw1B9lwIBmAAF8vru8Zx21/ak5MeAAbDiazcvdxNh9OIdjPh8hgP+oE+xEVUvge6gQSExZARk4BR9KyOZqWQ77dIjY8gNiIQAJ9vVm7/yQLNieyYEsSh05mEVf4nhrXCeJoWg7bEtPYdzwDy4LujSO4unsjRnSOJSLIrxrOYBGRms9mVeVPZzVAamoq4eHhpKSkEBYW5u7miEg12JGUxvB//0xegcWNFzTmnktaER0W4O5mVVp2XgFr95/k193HWbnnOLuOpHOyMEgpi4+XjdAAnwq3Ox02G8RFBpGZW8AxN2QlfL1ttI0Jw9vLxrH0HI6l55CdZz/n7XDw9rLhbbORW1B1bfD2shEXGci+45kAeNnAz8frrN+nn7fXGbXT19vGX3o35skr2uPrrU4pIlL7nU5soMBJRGql15fs4sX52wFzITq6Vxy3929Oo8ggN7fMlWVZbDqUyqp9J9iakMrWxFR2JKaXetHr7+NF/TB//AovaG02Gxk5+SSlZuOohRHg68WgdtFc2aUBF7WKwsIiN99ubgWn3Bfecgrs5BVbbrNB83ohtIoOIcjPdEw4mpbD1oRU9p/ILMrc1AnCsiwOJWdx8EQWianZBPp6ExrgQ2iAL8mZuaw/mMyGA8nsSEonOiyAtjGhtIkJpUFEIHa7RV6BnQK7RZ7dIr/ATn6BRYCvFx0bmixVgK+3y2eQkZNfGETlciw9B2+bjbBAX8ICfcjNt7N813F+3nmU1ftOkltgJyYsgMZ1gqgX6sfRtBwOnMgiKS0bfx8vejapQ98WdendrA6RQb54e3nh42UjNTuPAycyiT+eSUJKNo0iA+kaF0GHBuEUWBZLtx9h3qZElu04ipfNZrJfwf4E+nmTkpVHcmYuyVl5RIcG0DUugq6NI4gJD2DTwRTW7D/JHwdTiArx59oejRjZrSH1QvxYuec4by3bw7IdRwGoG+zHBc3r0q1xBPl2i5OZuZxIzyUxNZuDJ7M4eDKTvALzpYcF+FA/LABvm43DKVmkZZvujKEBPlzatj6DO8TQsWE4B05msu+YycDVC/GjbUwYbWJCsSyLr9cf5st1h5wVKfu1qMuMMT0ID/Kt+pNeRMSD1LjA6fXXX+fFF18kMTGRLl26MH36dHr37l3m9nPmzOGpp55i3759tGrViueff55hw4ZV6rUUOImcHyzL4pddx/j34p2s2nfSubxjwzAubRvNpe3q0y42zG2/qidn5vLVukN8tuoA2xLTSqyvH+pP3xZ16du8Lp0bRdAgIoDwQN9Sux3mF9g5kmYyMi2iQgj2Vy/s7LwCgBKBF0BOfgFeNptHZlTij2eQm2+nZf2QcruYFtgtTmTkEhrgU+I9pufkcywthwYRgfj5nN57XLQlifs+XUdGbgHNo4KZOb4XTeoGn9F7ERGpCWpU4PTZZ58xduxY3nzzTfr06cOrr77KnDlz2L59O/Xr1y+x/YoVK+jfvz/Tpk3jiiuuYPbs2Tz//POsXbuWjh07Vvh6CpxEzi+WZbFyz3Fe+3EXK/ccp/hfPB8vG03qBtGyfggNI4II8PUiwNebQF9vQgJ8CA3wISzA15lBCQs0z/19vEpc1BbYLdJz8s0tO5/0nDzSsouep2Xnk5iazf4TmRw4kcmeoxnOrJK/jxd/almPDg3CaN8gjHaxYTSuE1QrxmZJzbPlcCq3vr+KhJRsIoJ8ub5nHJe3j6Zb40i8vXROikjtUqMCpz59+tCrVy9ee+01AOx2O3Fxcdxzzz089thjJba/4YYbyMjI4Ntvv3Uuu+CCC+jatStvvvlmha+nwEnk/HUsPYcl246weOsRlu86RtoZVmjzsuEMsLy8THe5zNyC0z5Ou9gwRveO46ouDdUlSjzKkdRsbvtgNX8cTHEuqxfiR7vYMAJ9vZ3nf4CvFwF+5rGvtxc2G3jZbHgV3tuKPfayUfjchuM3ARsUe2wzC3DeYbPZij123fbU3xVKbFv4TL8/1Bzu7wNVMYsa0EhqxmcJcFn76FJ7BpxLpxMbuLU/R25uLmvWrGHSpEnOZV5eXgwaNIiVK1eWus/KlSt58MEHXZYNHjyYuXPnlrp9Tk4OOTlFg5pTU1PPvuEiUiPVC/Hnup5xXNczrrC8dDa7jqSz60g6R9JyyM4rICe/gKzcAtJz8p3V2tKy80nNziM9Jx/LArsFmbkFpQZLfj5ehPj7FN0CfAj19yHY34f6of40qWvGBjWrF6ysknis+mEBfHFnPxZuSWLhlkQWbzvCsfRcft55zN1NE5Fa5PcnLnV74HQ63Bo4HTt2jIKCAqKjo12WR0dHs23btlL3SUxMLHX7xMTEUrefNm0azzzzTNU0WERqDZvNRoOIQBpEBNK/dVSl9rHbLTJy88nKLSA7z05WXgH5djuh/r6EBPgQ7O+Nv0/N+R+ASHn8fLwY3jmW4Z1jySuws2rfCRJTsp3nfnbhLSu3wPxbKLCwWxZ2y3SRdTy2W1bhDw5mWVHdk6Ky+o4fxy3LKva4aN2pnWMsq+iXf+d2pS1z/Ee/T9QY5+tXdb7+hubngWNNy1PrRxBPmjTJJUOVmppKXFycG1skIjWVl5eN0ABfQgPUrU7OL77eXvRrUc/dzRARcSu3Bk716tXD29ubpKQkl+VJSUnExMSUuk9MTMxpbe/v74+/v3/VNFhERERERM5Lbs2P+fn50aNHDxYvXuxcZrfbWbx4MX379i11n759+7psD7Bw4cIytxcRERERETlbbu+q9+CDDzJu3Dh69uxJ7969efXVV8nIyODmm28GYOzYsTRs2JBp06YBcN999zFgwAD++c9/Mnz4cD799FNWr17N22+/7c63ISIiIiIitZjbA6cbbriBo0ePMnnyZBITE+natSvz5s1zFoDYv38/Xl5FibF+/foxe/ZsnnzySR5//HFatWrF3LlzKzWHk4iIiIiIyJlw+zxO55rmcRIRERERETi92KBm1QAUERERERFxAwVOIiIiIiIiFVDgJCIiIiIiUgEFTiIiIiIiIhVQ4CQiIiIiIlIBBU4iIiIiIiIVcPs8Tueao/p6amqqm1siIiIiIiLu5IgJKjND03kXOKWlpQEQFxfn5paIiIiIiIgnSEtLIzw8vNxtzrsJcO12O4cPHyY0NBSbzebu5pCamkpcXBwHDhzQhLzVQJ9v9dNnXL30+VY/fcbVS59v9dNnXL30+VY/d37GlmWRlpZGgwYN8PIqfxTTeZdx8vLyolGjRu5uRglhYWH6x1iN9PlWP33G1Uufb/XTZ1y99PlWP33G1Uufb/Vz12dcUabJQcUhREREREREKqDASUREREREpAIKnNzM39+fp59+Gn9/f3c3pVbS51v99BlXL32+1U+fcfXS51v99BlXL32+1a+mfMbnXXEIERERERGR06WMk4iIiIiISAUUOImIiIiIiFRAgZOIiIiIiEgFFDiJiIiIiIhUQIGTG73++us0bdqUgIAA+vTpw++//+7uJtVI06ZNo1evXoSGhlK/fn1GjhzJ9u3bXbYZOHAgNpvN5XbnnXe6qcU1z5QpU0p8fm3btnWuz87OZsKECdStW5eQkBBGjRpFUlKSG1tcszRt2rTE52uz2ZgwYQKg8/dM/PTTT4wYMYIGDRpgs9mYO3euy3rLspg8eTKxsbEEBgYyaNAgdu7c6bLNiRMnGDNmDGFhYURERHDrrbeSnp5+Dt+F5yrv883Ly+PRRx+lU6dOBAcH06BBA8aOHcvhw4ddjlHaef/cc8+d43fiuSo6h8ePH1/i8xsyZIjLNjqHy1bR51va32SbzcaLL77o3EbncNkqc21WmWuH/fv3M3z4cIKCgqhfvz6PPPII+fn55/KtuFDg5CafffYZDz74IE8//TRr166lS5cuDB48mCNHjri7aTXOsmXLmDBhAr/++isLFy4kLy+Pyy+/nIyMDJftbr/9dhISEpy3F154wU0trpk6dOjg8vn98ssvznUPPPAA//vf/5gzZw7Lli3j8OHDXHPNNW5sbc2yatUql8924cKFAFx33XXObXT+np6MjAy6dOnC66+/Xur6F154gX//+9+8+eab/PbbbwQHBzN48GCys7Od24wZM4bNmzezcOFCvv32W3766SfuuOOOc/UWPFp5n29mZiZr167lqaeeYu3atXz55Zds376dK6+8ssS2U6dOdTmv77nnnnPR/BqhonMYYMiQIS6f3yeffOKyXudw2Sr6fIt/rgkJCbz33nvYbDZGjRrlsp3O4dJV5tqsomuHgoIChg8fTm5uLitWrOD9999n1qxZTJ482R1vybDELXr37m1NmDDB+bygoMBq0KCBNW3aNDe2qnY4cuSIBVjLli1zLhswYIB13333ua9RNdzTTz9tdenSpdR1ycnJlq+vrzVnzhznsq1bt1qAtXLlynPUwtrlvvvus1q0aGHZ7XbLsnT+ni3A+uqrr5zP7Xa7FRMTY7344ovOZcnJyZa/v7/1ySefWJZlWVu2bLEAa9WqVc5tfvjhB8tms1mHDh06Z22vCU79fEvz+++/W4AVHx/vXNakSRPrlVdeqd7G1RKlfcbjxo2zrrrqqjL30TlceZU5h6+66irrkksucVmmc7jyTr02q8y1w/fff295eXlZiYmJzm1mzJhhhYWFWTk5Oef2DRRSxskNcnNzWbNmDYMGDXIu8/LyYtCgQaxcudKNLasdUlJSAKhTp47L8o8//ph69erRsWNHJk2aRGZmpjuaV2Pt3LmTBg0a0Lx5c8aMGcP+/fsBWLNmDXl5eS7nc9u2bWncuLHO5zOQm5vLRx99xC233ILNZnMu1/lbdfbu3UtiYqLLORseHk6fPn2c5+zKlSuJiIigZ8+ezm0GDRqEl5cXv/322zlvc02XkpKCzWYjIiLCZflzzz1H3bp16datGy+++KJbu+DUREuXLqV+/fq0adOGu+66i+PHjzvX6RyuOklJSXz33XfceuutJdbpHK6cU6/NKnPtsHLlSjp16kR0dLRzm8GDB5OamsrmzZvPYeuL+LjlVc9zx44do6CgwOVEAIiOjmbbtm1ualXtYLfbuf/++7nwwgvp2LGjc/lf/vIXmjRpQoMGDfjjjz949NFH2b59O19++aUbW1tz9OnTh1mzZtGmTRsSEhJ45plnuOiii9i0aROJiYn4+fmVuCCKjo4mMTHRPQ2uwebOnUtycjLjx493LtP5W7Uc52Vpf4Md6xITE6lfv77Leh8fH+rUqaPz+jRlZ2fz6KOPMnr0aMLCwpzL7733Xrp3706dOnVYsWIFkyZNIiEhgZdfftmNra05hgwZwjXXXEOzZs3YvXs3jz/+OEOHDmXlypV4e3vrHK5C77//PqGhoSW6oOscrpzSrs0qc+2QmJhY6t9pxzp3UOAktcqECRPYtGmTy/gbwKVPd6dOnYiNjeXSSy9l9+7dtGjR4lw3s8YZOnSo83Hnzp3p06cPTZo04fPPPycwMNCNLat93n33XYYOHUqDBg2cy3T+Sk2Vl5fH9ddfj2VZzJgxw2Xdgw8+6HzcuXNn/Pz8+Otf/8q0adPw9/c/102tcf785z87H3fq1InOnTvTokULli5dyqWXXurGltU+7733HmPGjCEgIMBluc7hyinr2qwmUlc9N6hXrx7e3t4lKockJSURExPjplbVfBMnTuTbb79lyZIlNGrUqNxt+/TpA8CuXbvORdNqnYiICFq3bs2uXbuIiYkhNzeX5ORkl210Pp+++Ph4Fi1axG233Vbudjp/z47jvCzvb3BMTEyJYj35+fmcOHFC53UlOYKm+Ph4Fi5c6JJtKk2fPn3Iz89n375956aBtUzz5s2pV6+e8++CzuGq8fPPP7N9+/YK/y6DzuHSlHVtVplrh5iYmFL/TjvWuYMCJzfw8/OjR48eLF682LnMbrezePFi+vbt68aW1UyWZTFx4kS++uorfvzxR5o1a1bhPuvXrwcgNja2mltXO6Wnp7N7925iY2Pp0aMHvr6+Lufz9u3b2b9/v87n0zRz5kzq16/P8OHDy91O5+/ZadasGTExMS7nbGpqKr/99pvznO3bty/JycmsWbPGuc2PP/6I3W53Bq5SNkfQtHPnThYtWkTdunUr3Gf9+vV4eXmV6F4mlXPw4EGOHz/u/Lugc7hqvPvuu/To0YMuXbpUuK3O4SIVXZtV5tqhb9++bNy40eUHAMePMO3btz83b+RUbilJIdann35q+fv7W7NmzbK2bNli3XHHHVZERIRL5RCpnLvuussKDw+3li5daiUkJDhvmZmZlmVZ1q5du6ypU6daq1evtvbu3Wt9/fXXVvPmza3+/fu7ueU1x0MPPWQtXbrU2rt3r7V8+XJr0KBBVr169awjR45YlmVZd955p9W4cWPrxx9/tFavXm317dvX6tu3r5tbXbMUFBRYjRs3th599FGX5Tp/z0xaWpq1bt06a926dRZgvfzyy9a6deucVd2ee+45KyIiwvr666+tP/74w7rqqqusZs2aWVlZWc5jDBkyxOrWrZv122+/Wb/88ovVqlUra/To0e56Sx6lvM83NzfXuvLKK61GjRpZ69evd/m77KiEtWLFCuuVV16x1q9fb+3evdv66KOPrKioKGvs2LFufmeeo7zPOC0tzXr44YetlStXWnv37rUWLVpkde/e3WrVqpWVnZ3tPIbO4bJV9DfCsiwrJSXFCgoKsmbMmFFif53D5avo2syyKr52yM/Ptzp27Ghdfvnl1vr166158+ZZUVFR1qRJk9zxlizLsiwFTm40ffp0q3Hjxpafn5/Vu3dv69dff3V3k2okoNTbzJkzLcuyrP3791v9+/e36tSpY/n7+1stW7a0HnnkESslJcW9Da9BbrjhBis2Ntby8/OzGjZsaN1www3Wrl27nOuzsrKsu+++24qMjLSCgoKsq6++2kpISHBji2ue+fPnW4C1fft2l+U6f8/MkiVLSv27MG7cOMuyTEnyp556yoqOjrb8/f2tSy+9tMRnf/z4cWv06NFWSEiIFRYWZt18881WWlqaG96N5ynv8927d2+Zf5eXLFliWZZlrVmzxurTp48VHh5uBQQEWO3atbOeffZZl4v+8115n3FmZqZ1+eWXW1FRUZavr6/VpEkT6/bbby/x46vO4bJV9DfCsizrrbfesgIDA63k5OQS++scLl9F12aWVblrh3379llDhw61AgMDrXr16lkPPfSQlZeXd47fTRGbZVlWNSWzREREREREagWNcRIREREREamAAicREREREZEKKHASERERERGpgAInERERERGRCihwEhERERERqYACJxERERERkQoocBIREREREamAAicREREREZEKKHASEREph81mY+7cue5uhoiIuJkCJxER8Vjjx4/HZrOVuA0ZMsTdTRMRkfOMj7sbICIiUp4hQ4Ywc+ZMl2X+/v5uao2IiJyvlHESERGP5u/vT0xMjMstMjISMN3oZsyYwdChQwkMDKR58+Z88cUXLvtv3LiRSy65hMDAQOrWrcsdd9xBenq6yzbvvfceHTp0wN/fn9jYWCZOnOiy/tixY1x99dUEBQXRqlUrvvnmG+e6kydPMmbMGKKioggMDKRVq1YlAj0REan5FDiJiEiN9tRTTzFq1Cg2bNjAmDFj+POf/8zWrVsByMjIYPDgwURGRrJq1SrmzJnDokWLXAKjGTNmMGHCBO644w42btzIN998Q8uWLV1e45lnnuH666/njz/+YNiwYYwZM4YTJ044X3/Lli388MMPbN26lRkzZlCvXr1z9wGIiMg5YbMsy3J3I0REREozfvx4PvroIwICAlyWP/744zz++OPYbDbuvPNOZsyY4Vx3wQUX0L17d9544w3+85//8Oijj3LgwAGCg4MB+P777xkxYgSHDx8mOjqahg0bcvPNN/P3v/+91DbYbDaefPJJ/va3vwEmGAsJCeGHH35gyJAhXHnlldSrV4/33nuvmj4FERHxBBrjJCIiHu3iiy92CYwA6tSp43zct29fl3V9+/Zl/fr1AGzdupUuXbo4gyaACy+8ELvdzvbt27HZbBw+fJhLL7203DZ07tzZ+Tg4OJiwsDCOHDkCwF133cWoUaNYu3Ytl19+OSNHjqRfv35n9F5FRMRzKXASERGPFhwcXKLrXFUJDAys1Ha+vr4uz202G3a7HYChQ4cSHx/P999/z8KFC7n00kuZMGECL730UpW3V0RE3EdjnEREpEb79ddfSzxv164dAO3atWPDhg1kZGQ41y9fvhwvLy/atGlDaGgoTZs2ZfHixWfVhqioKMaNG8dHH33Eq6++yttvv31WxxMREc+jjJOIiHi0nJwcEhMTXZb5+Pg4CzDMmTOHnj178qc//YmPP/6Y33//nXfffReAMWPG8PTTTzNu3DimTJnC0aNHueeee7jpppuIjo4GYMqUKdx5553Ur1+foUOHkpaWxvLly7nnnnsq1b7JkyfTo0cPOnToQE5ODt9++60zcBMRkdpDgZOIiHi0efPmERsb67KsTZs2bNu2DTAV7z799FPuvvtuYmNj+eSTT2jfvj0AQUFBzJ8/n/vuu49evXoRFBTEqFGjePnll53HGjduHNnZ2bzyyis8/PDD1KtXj2uvvbbS7fPz82PSpEns27ePwMBALrroIj799NMqeOciIuJJVFVPRERqLJvNxldffcXIkSPd3RQREanlNMZJRERERETk/9u3gxMAABAGYvtv7Qj3EwrJFoc1CCcAAIDgxwmAWdbmAHxxcQIAAAjCCQAAIAgnAACAIJwAAACCcAIAAAjCCQAAIAgnAACAIJwAAADCAQjBGidCySoJAAAAAElFTkSuQmCC",
            "text/plain": [
              "<Figure size 1000x400 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test set Metrics:\n",
            " Average loss: 0.3299 \n",
            " F1 Score: 0.9065\n",
            "Accuracy: 90.67%\n",
            "Uncertainties: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.0, 9: 0.0}\n",
            "ECE: 0.0491\n",
            "Mean AUPR: 0.9643\n",
            "Mean AUC: 0.9939\n",
            "Mean FPR at 95% Recall: 0.0268\n",
            "Number of Parameters: 23520842\n"
          ]
        }
      ],
      "source": [
        "resnet50_cifr10 = ResNet(Bottleneck, [3, 4, 6, 3], num_classes=10)\n",
        "resnet50_cifr10.to(device)\n",
        "summary(resnet50_cifr10, (3, 32, 32))\n",
        "\n",
        "# Training the model\n",
        "train_losses, val_losses = train_model(\n",
        "    model=resnet50_cifr10, \n",
        "    train_loader=trainloader10, \n",
        "    val_loader=validloader10,\n",
        "    epochs=200, \n",
        "    learning_rate=0.1, \n",
        "    gamma_lr=0.2,\n",
        "    milestones=[60, 120, 160], \n",
        "    save_path='./resnet50_cifr10_dropout.pth', \n",
        "    Weight_decay=5e-4,\n",
        "    Momentum=0.9, \n",
        "    Optimizer_type='SGD',  \n",
        "    Loss_fn='CrossEntropyLoss',\n",
        "    Num_classes=10,\n",
        "    BNL_enable=False,\n",
        "    BNL_load_path='./resnet50_cifr10_dropout.pth'\n",
        ")\n",
        "\n",
        "# Testing the model with metrics\n",
        "test_model_with_metrics(\n",
        "    loss_fn=nn.CrossEntropyLoss(), \n",
        "    model=resnet50_cifr10, \n",
        "    test_loader=testloader10, \n",
        "    load_path=\"./resnet50_cifr10_dropout.pth\",\n",
        "    calculate_uncert=True, \n",
        "    calculate_nll_loss=True, \n",
        "    calculate_ece_error=True,\n",
        "    calculate_auprc=True, \n",
        "    calculate_auc_roc=True, \n",
        "    calculate_fpr_95=True, \n",
        "    count_params=True,\n",
        "    plot_uncert=False, \n",
        "    predict_uncert=False, \n",
        "    model_class=resnet50_cifr10.__class__, \n",
        "    models=[torch.load('./resnet50_cifr10_dropout.pth')],\n",
        "    num_samples=10, \n",
        "    num_classes=10\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ResNet50 on CIFAR100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "resnet50_cifr100 = ResNet(Bottleneck, [3, 4, 6, 3], num_classes=100)\n",
        "resnet50_cifr100.to(device)\n",
        "summary(resnet50_cifr100, (3, 32, 32))\n",
        "\n",
        "# Training the model\n",
        "train_losses, val_losses = train_model(\n",
        "    model=resnet50_cifr10, \n",
        "    train_loader=trainloader100, \n",
        "    val_loader=validloader100,\n",
        "    epochs=200, \n",
        "    learning_rate=0.1, \n",
        "    gamma_lr=0.2,\n",
        "    milestones=[60, 120, 160], \n",
        "    save_path='./resnet50_cifr100_dropout.pth', \n",
        "    Weight_decay=5e-4,\n",
        "    Momentum=0.9, \n",
        "    Optimizer_type='SGD',  \n",
        "    Loss_fn='CrossEntropyLoss',\n",
        "    Num_classes=100,\n",
        "    BNL_enable=False,\n",
        "    BNL_load_path='./resnet50_cifr100_dropout.pth'\n",
        ")\n",
        "\n",
        "# Testing the model with metrics\n",
        "test_model_with_metrics(\n",
        "    loss_fn=nn.CrossEntropyLoss(), \n",
        "    model=resnet50_cifr10, \n",
        "    test_loader=testloader10, \n",
        "    load_path=\"./resnet50_cifr100_dropout.pth\",\n",
        "    calculate_uncert=True, \n",
        "    calculate_nll_loss=True, \n",
        "    calculate_ece_error=True,\n",
        "    calculate_auprc=True, \n",
        "    calculate_auc_roc=True, \n",
        "    calculate_fpr_95=True, \n",
        "    count_params=True,\n",
        "    plot_uncert=False, \n",
        "    predict_uncert=False, \n",
        "    model_class=resnet50_cifr10.__class__, \n",
        "    models=[torch.load('./resnet50_cifr100_dropout.pth')],\n",
        "    num_samples=100, \n",
        "    num_classes=100\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ResNet50 on CIFAR10 with ABNLL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 64, 32, 32]           1,728\n",
            "               BNL-2           [-1, 64, 32, 32]               0\n",
            "            Conv2d-3           [-1, 64, 32, 32]           4,096\n",
            "               BNL-4           [-1, 64, 32, 32]               0\n",
            "            Conv2d-5           [-1, 64, 32, 32]          36,864\n",
            "               BNL-6           [-1, 64, 32, 32]               0\n",
            "            Conv2d-7          [-1, 256, 32, 32]          16,384\n",
            "               BNL-8          [-1, 256, 32, 32]               0\n",
            "            Conv2d-9          [-1, 256, 32, 32]          16,384\n",
            "              BNL-10          [-1, 256, 32, 32]               0\n",
            "       Bottleneck-11          [-1, 256, 32, 32]               0\n",
            "           Conv2d-12           [-1, 64, 32, 32]          16,384\n",
            "              BNL-13           [-1, 64, 32, 32]               0\n",
            "           Conv2d-14           [-1, 64, 32, 32]          36,864\n",
            "              BNL-15           [-1, 64, 32, 32]               0\n",
            "           Conv2d-16          [-1, 256, 32, 32]          16,384\n",
            "              BNL-17          [-1, 256, 32, 32]               0\n",
            "       Bottleneck-18          [-1, 256, 32, 32]               0\n",
            "           Conv2d-19           [-1, 64, 32, 32]          16,384\n",
            "              BNL-20           [-1, 64, 32, 32]               0\n",
            "           Conv2d-21           [-1, 64, 32, 32]          36,864\n",
            "              BNL-22           [-1, 64, 32, 32]               0\n",
            "           Conv2d-23          [-1, 256, 32, 32]          16,384\n",
            "              BNL-24          [-1, 256, 32, 32]               0\n",
            "       Bottleneck-25          [-1, 256, 32, 32]               0\n",
            "           Conv2d-26          [-1, 128, 32, 32]          32,768\n",
            "              BNL-27          [-1, 128, 32, 32]               0\n",
            "           Conv2d-28          [-1, 128, 16, 16]         147,456\n",
            "              BNL-29          [-1, 128, 16, 16]               0\n",
            "           Conv2d-30          [-1, 512, 16, 16]          65,536\n",
            "              BNL-31          [-1, 512, 16, 16]               0\n",
            "           Conv2d-32          [-1, 512, 16, 16]         131,072\n",
            "              BNL-33          [-1, 512, 16, 16]               0\n",
            "       Bottleneck-34          [-1, 512, 16, 16]               0\n",
            "           Conv2d-35          [-1, 128, 16, 16]          65,536\n",
            "              BNL-36          [-1, 128, 16, 16]               0\n",
            "           Conv2d-37          [-1, 128, 16, 16]         147,456\n",
            "              BNL-38          [-1, 128, 16, 16]               0\n",
            "           Conv2d-39          [-1, 512, 16, 16]          65,536\n",
            "              BNL-40          [-1, 512, 16, 16]               0\n",
            "       Bottleneck-41          [-1, 512, 16, 16]               0\n",
            "           Conv2d-42          [-1, 128, 16, 16]          65,536\n",
            "              BNL-43          [-1, 128, 16, 16]               0\n",
            "           Conv2d-44          [-1, 128, 16, 16]         147,456\n",
            "              BNL-45          [-1, 128, 16, 16]               0\n",
            "           Conv2d-46          [-1, 512, 16, 16]          65,536\n",
            "              BNL-47          [-1, 512, 16, 16]               0\n",
            "       Bottleneck-48          [-1, 512, 16, 16]               0\n",
            "           Conv2d-49          [-1, 128, 16, 16]          65,536\n",
            "              BNL-50          [-1, 128, 16, 16]               0\n",
            "           Conv2d-51          [-1, 128, 16, 16]         147,456\n",
            "              BNL-52          [-1, 128, 16, 16]               0\n",
            "           Conv2d-53          [-1, 512, 16, 16]          65,536\n",
            "              BNL-54          [-1, 512, 16, 16]               0\n",
            "       Bottleneck-55          [-1, 512, 16, 16]               0\n",
            "           Conv2d-56          [-1, 256, 16, 16]         131,072\n",
            "              BNL-57          [-1, 256, 16, 16]               0\n",
            "           Conv2d-58            [-1, 256, 8, 8]         589,824\n",
            "              BNL-59            [-1, 256, 8, 8]               0\n",
            "           Conv2d-60           [-1, 1024, 8, 8]         262,144\n",
            "              BNL-61           [-1, 1024, 8, 8]               0\n",
            "           Conv2d-62           [-1, 1024, 8, 8]         524,288\n",
            "              BNL-63           [-1, 1024, 8, 8]               0\n",
            "       Bottleneck-64           [-1, 1024, 8, 8]               0\n",
            "           Conv2d-65            [-1, 256, 8, 8]         262,144\n",
            "              BNL-66            [-1, 256, 8, 8]               0\n",
            "           Conv2d-67            [-1, 256, 8, 8]         589,824\n",
            "              BNL-68            [-1, 256, 8, 8]               0\n",
            "           Conv2d-69           [-1, 1024, 8, 8]         262,144\n",
            "              BNL-70           [-1, 1024, 8, 8]               0\n",
            "       Bottleneck-71           [-1, 1024, 8, 8]               0\n",
            "           Conv2d-72            [-1, 256, 8, 8]         262,144\n",
            "              BNL-73            [-1, 256, 8, 8]               0\n",
            "           Conv2d-74            [-1, 256, 8, 8]         589,824\n",
            "              BNL-75            [-1, 256, 8, 8]               0\n",
            "           Conv2d-76           [-1, 1024, 8, 8]         262,144\n",
            "              BNL-77           [-1, 1024, 8, 8]               0\n",
            "       Bottleneck-78           [-1, 1024, 8, 8]               0\n",
            "           Conv2d-79            [-1, 256, 8, 8]         262,144\n",
            "              BNL-80            [-1, 256, 8, 8]               0\n",
            "           Conv2d-81            [-1, 256, 8, 8]         589,824\n",
            "              BNL-82            [-1, 256, 8, 8]               0\n",
            "           Conv2d-83           [-1, 1024, 8, 8]         262,144\n",
            "              BNL-84           [-1, 1024, 8, 8]               0\n",
            "       Bottleneck-85           [-1, 1024, 8, 8]               0\n",
            "           Conv2d-86            [-1, 256, 8, 8]         262,144\n",
            "              BNL-87            [-1, 256, 8, 8]               0\n",
            "           Conv2d-88            [-1, 256, 8, 8]         589,824\n",
            "              BNL-89            [-1, 256, 8, 8]               0\n",
            "           Conv2d-90           [-1, 1024, 8, 8]         262,144\n",
            "              BNL-91           [-1, 1024, 8, 8]               0\n",
            "       Bottleneck-92           [-1, 1024, 8, 8]               0\n",
            "           Conv2d-93            [-1, 256, 8, 8]         262,144\n",
            "              BNL-94            [-1, 256, 8, 8]               0\n",
            "           Conv2d-95            [-1, 256, 8, 8]         589,824\n",
            "              BNL-96            [-1, 256, 8, 8]               0\n",
            "           Conv2d-97           [-1, 1024, 8, 8]         262,144\n",
            "              BNL-98           [-1, 1024, 8, 8]               0\n",
            "       Bottleneck-99           [-1, 1024, 8, 8]               0\n",
            "          Conv2d-100            [-1, 512, 8, 8]         524,288\n",
            "             BNL-101            [-1, 512, 8, 8]               0\n",
            "          Conv2d-102            [-1, 512, 4, 4]       2,359,296\n",
            "             BNL-103            [-1, 512, 4, 4]               0\n",
            "          Conv2d-104           [-1, 2048, 4, 4]       1,048,576\n",
            "             BNL-105           [-1, 2048, 4, 4]               0\n",
            "          Conv2d-106           [-1, 2048, 4, 4]       2,097,152\n",
            "             BNL-107           [-1, 2048, 4, 4]               0\n",
            "      Bottleneck-108           [-1, 2048, 4, 4]               0\n",
            "          Conv2d-109            [-1, 512, 4, 4]       1,048,576\n",
            "             BNL-110            [-1, 512, 4, 4]               0\n",
            "          Conv2d-111            [-1, 512, 4, 4]       2,359,296\n",
            "             BNL-112            [-1, 512, 4, 4]               0\n",
            "          Conv2d-113           [-1, 2048, 4, 4]       1,048,576\n",
            "             BNL-114           [-1, 2048, 4, 4]               0\n",
            "      Bottleneck-115           [-1, 2048, 4, 4]               0\n",
            "          Conv2d-116            [-1, 512, 4, 4]       1,048,576\n",
            "             BNL-117            [-1, 512, 4, 4]               0\n",
            "          Conv2d-118            [-1, 512, 4, 4]       2,359,296\n",
            "             BNL-119            [-1, 512, 4, 4]               0\n",
            "          Conv2d-120           [-1, 2048, 4, 4]       1,048,576\n",
            "             BNL-121           [-1, 2048, 4, 4]               0\n",
            "      Bottleneck-122           [-1, 2048, 4, 4]               0\n",
            "          Linear-123                   [-1, 10]          20,490\n",
            "================================================================\n",
            "Total params: 23,467,722\n",
            "Trainable params: 23,467,722\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 66.13\n",
            "Params size (MB): 89.52\n",
            "Estimated Total Size (MB): 155.66\n",
            "----------------------------------------------------------------\n",
            "BNL model loaded from ./resnet50_cifr10_dropout.pth\n",
            "Model weights loaded.\n",
            "Epoch 1/20, Train Loss: 418.7775, Val Loss: 320.5757\n",
            "Epoch 2/20, Train Loss: 236.2800, Val Loss: 172.4849\n",
            "Epoch 3/20, Train Loss: 126.6855, Val Loss: 92.6447\n"
          ]
        }
      ],
      "source": [
        "resnet50_cifr10_bnl = ResNet(Bottleneck, [3, 4, 6, 3], num_classes=10,norm_layer=ABNN.BNL)\n",
        "resnet50_cifr10_bnl.to(device)\n",
        "summary(resnet50_cifr10_bnl, (3, 32, 32))\n",
        "\n",
        "# Training the model\n",
        "train_losses, val_losses = train_model(\n",
        "    model=resnet50_cifr10_bnl, \n",
        "    train_loader=trainloader10, \n",
        "    val_loader=validloader10,\n",
        "    epochs=20, \n",
        "    learning_rate=0.1, \n",
        "    gamma_lr=0.2,\n",
        "    milestones=[60, 120, 160], \n",
        "    save_path='./resnet50_cifr10_bnl_dropout.pth', \n",
        "    Weight_decay=5e-4,\n",
        "    Momentum=0.9, \n",
        "    Optimizer_type='SGD',  \n",
        "    Loss_fn='CustomMAPLoss',\n",
        "    Num_classes=10,\n",
        "    BNL_enable=True,\n",
        "    BNL_load_path='./resnet50_cifr10_dropout.pth'\n",
        ")\n",
        "\n",
        "# Testing the model with metrics\n",
        "test_model_with_metrics(\n",
        "    loss_fn='CustomMAPLoss', \n",
        "    model=resnet50_cifr10_bnl, \n",
        "    test_loader=testloader10, \n",
        "    load_path=\"./resnet50_cifr10_bnl_dropout.pth\",\n",
        "    calculate_uncert=True, \n",
        "    calculate_nll_loss=True, \n",
        "    calculate_ece_error=True,\n",
        "    calculate_auprc=True, \n",
        "    calculate_auc_roc=True, \n",
        "    calculate_fpr_95=True, \n",
        "    count_params=True,\n",
        "    plot_uncert=False, \n",
        "    predict_uncert=False, \n",
        "    model_class=resnet50_cifr10.__class__, \n",
        "    models=[torch.load('./resnet50_cifr10_bnl_dropout.pth')],\n",
        "    num_samples=10, \n",
        "    num_classes=10,\n",
        "    Weight_decay=5e-4\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ResNet50 on CIFAR100 with ABNLL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "resnet50_cifr10_bnl = ResNet(Bottleneck, [3, 4, 6, 3], num_classes=10,norm_layer=ABNN.BNL)\n",
        "resnet50_cifr10_bnl.to(device)\n",
        "summary(resnet50_cifr10_bnl, (3, 32, 32))\n",
        "\n",
        "# Training the model\n",
        "train_losses, val_losses = train_model(\n",
        "    model=resnet50_cifr10_bnl, \n",
        "    train_loader=trainloader10, \n",
        "    val_loader=validloader10,\n",
        "    epochs=200, \n",
        "    learning_rate=0.1, \n",
        "    gamma_lr=0.2,\n",
        "    milestones=[60, 120, 160], \n",
        "    save_path='./resnet50_cifr100_bnl_dropout.pth', \n",
        "    Weight_decay=5e-4,\n",
        "    Momentum=0.9, \n",
        "    Optimizer_type='SGD',  \n",
        "    Loss_fn=ABNN.CustomMAPLoss,\n",
        "    Num_classes=100,\n",
        "    BNL_enable=True,\n",
        "    BNL_load_path='./resnet50_cifr100_dropout.pth'\n",
        ")\n",
        "\n",
        "# Testing the model with metrics\n",
        "test_model_with_metrics(\n",
        "    loss_fn=nn.CrossEntropyLoss(), \n",
        "    model=resnet50_cifr10, \n",
        "    test_loader=testloader10, \n",
        "    load_path=\"./resnet50_cifr100_bnl_dropout.pth\",\n",
        "    calculate_uncert=True, \n",
        "    calculate_nll_loss=True, \n",
        "    calculate_ece_error=True,\n",
        "    calculate_auprc=True, \n",
        "    calculate_auc_roc=True, \n",
        "    calculate_fpr_95=True, \n",
        "    count_params=True,\n",
        "    plot_uncert=False, \n",
        "    predict_uncert=False, \n",
        "    model_class=resnet50_cifr10.__class__, \n",
        "    models=[torch.load('./resnet50_cifr100_bnl_dropout.pth')],\n",
        "    num_samples=100, \n",
        "    num_classes=100\n",
        ")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "jBXHs2m0APxl",
        "NW9xd8Xc3UqB",
        "AoXdUp79d8CL",
        "XFwNb7Gf3UqR",
        "AL7gMgK93UqT",
        "cS3UXMDZwmM4",
        "OhKKLx5JAnzq",
        "6S-mVSFzeH0b",
        "a1WXC6ajA2jg",
        "5CcSbEhOiTst",
        "IpQxUPVV6VPH",
        "YAas1qBvIovm",
        "IPppy_FS52vd",
        "HYV-mZuBIqJa",
        "f8W_F8G-IzbY",
        "to_mOzrTI2VV",
        "FpQrqv19I5gC",
        "DsCg3LDgI9zK",
        "BV9SutwYJAKV",
        "QGjM8OySfpR1"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
