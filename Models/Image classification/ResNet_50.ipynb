{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NW9xd8Xc3UqB"
      },
      "source": [
        "## Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wvc77o82xGwt"
      },
      "outputs": [],
      "source": [
        "!pip install netcal\n",
        "!pip install torchsummary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "REotBNCV3UqO"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import numpy as np\n",
        "from torchsummary import summary\n",
        "from torch.nn.functional import softmax\n",
        "import torch.nn.functional as F\n",
        "import netcal.metrics as metrics\n",
        "from netcal.metrics import ECE\n",
        "from sklearn.metrics import precision_recall_curve, auc, roc_auc_score, roc_curve\n",
        "from torch.utils.data import random_split\n",
        "import multiprocessing\n",
        "multiprocessing.set_start_method('spawn', force=True)\n",
        "from sklearn.metrics import f1_score\n",
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lJy7vTo0RM5K",
        "outputId": "d13a67e0-41d7-4103-e3af-8132fe7145ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AoXdUp79d8CL"
      },
      "source": [
        "##  Prepare the data and the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFwNb7Gf3UqR"
      },
      "source": [
        "### Load and Augment CIFAR-10 and CIFAR100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uyQVyB3oCizi"
      },
      "source": [
        "Similar to the paper, I only use Horizontal Flip for data augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mSmx1nli3UqS",
        "outputId": "b76a3846-459b-4d96-c25e-37a2941a3d83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Training set size: 40000\n",
            "Validation set size: 10000\n",
            "Test set size: 10000\n"
          ]
        }
      ],
      "source": [
        "# Define the transform\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "# Load the train set\n",
        "full_trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                             download=True, transform=transform_train)\n",
        "\n",
        "# Split the dataset into training and validation sets\n",
        "train_size = int(0.8 * len(full_trainset))  # 80% for training\n",
        "valid_size = len(full_trainset) - train_size  # 20% for validation\n",
        "train_subset, valid_subset = random_split(full_trainset, [train_size, valid_size])\n",
        "\n",
        "# Create DataLoaders\n",
        "trainloader = torch.utils.data.DataLoader(train_subset, batch_size=128,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "validloader = torch.utils.data.DataLoader(valid_subset, batch_size=128,\n",
        "                                          shuffle=False, num_workers=2)\n",
        "\n",
        "# Load the test set\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform_test)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=128,\n",
        "                                         shuffle=False, num_workers=2)\n",
        "\n",
        "# Verify the sizes of the datasets\n",
        "print(f'Training set size: {len(train_subset)}')\n",
        "print(f'Validation set size: {len(valid_subset)}')\n",
        "print(f'Test set size: {len(testset)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ZJhCvRnRM5M",
        "outputId": "1206dcdf-9d03-4ffa-8584-1f583332255d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Training set size: 40000\n",
            "Validation set size: 10000\n",
            "Test set size: 10000\n"
          ]
        }
      ],
      "source": [
        "# Define the transform\n",
        "transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "# Load the CIFAR100 dataset\n",
        "full_trainset100 = torchvision.datasets.CIFAR100(root='./data', train=True,\n",
        "                                                 download=True, transform=transform)\n",
        "\n",
        "# Split the dataset into training and validation sets\n",
        "train_size100 = int(0.8 * len(full_trainset100))  # 80% for training\n",
        "valid_size100 = len(full_trainset100) - train_size100  # 20% for validation\n",
        "train_subset100, valid_subset100 = random_split(full_trainset100, [train_size100, valid_size100])\n",
        "\n",
        "# Create DataLoaders\n",
        "trainloader100 = torch.utils.data.DataLoader(train_subset100, batch_size=128,\n",
        "                                             shuffle=True, num_workers=2)\n",
        "validloader100 = torch.utils.data.DataLoader(valid_subset100, batch_size=128,\n",
        "                                             shuffle=False, num_workers=2)\n",
        "\n",
        "# Load the test set\n",
        "testset100 = torchvision.datasets.CIFAR100(root='./data', train=False,\n",
        "                                           download=True, transform=transform)\n",
        "testloader100 = torch.utils.data.DataLoader(testset100, batch_size=128,\n",
        "                                            shuffle=False, num_workers=2)\n",
        "\n",
        "# Verify the sizes of the datasets\n",
        "print(f'Training set size: {len(train_subset100)}')\n",
        "print(f'Validation set size: {len(valid_subset100)}')\n",
        "print(f'Test set size: {len(testset100)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AL7gMgK93UqT"
      },
      "source": [
        "### Define the ResNet50 model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "a4UNu1Ei4qr4"
      },
      "outputs": [],
      "source": [
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=stride, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, self.expansion *\n",
        "                               planes, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = F.relu(self.bn2(self.conv2(out)))\n",
        "        out = self.bn3(self.conv3(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = F.avg_pool2d(out, 4)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0tLyO2fhd3VK"
      },
      "outputs": [],
      "source": [
        "resnet50 = ResNet(Bottleneck, [3, 4, 6, 3], num_classes=10)\n",
        "resnet50.to(device)\n",
        "summary(resnet50, (3, 32, 32))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define organized train and test functions for the model"
      ],
      "metadata": {
        "id": "nfrxL4uBTMXP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "gXnevADeRM5Q"
      },
      "outputs": [],
      "source": [
        "def train_model(model: nn.Module, train_loader: DataLoader, val_loader: DataLoader,\n",
        "                epochs: int = 10, learning_rate: float = 0.005, gamma_lr: float = 0.1,\n",
        "                milestones: list = [5, 15], save_path: str = 'model.pth',Weight_decay: float = 5e-4) -> (list, list):\n",
        "    \"\"\"\n",
        "    Trains the model and evaluates it on the validation set after each epoch.\n",
        "\n",
        "    Parameters:\n",
        "        model (nn.Module): The neural network model to train.\n",
        "        train_loader (DataLoader): DataLoader for the training dataset.\n",
        "        val_loader (DataLoader): DataLoader for the validation dataset.\n",
        "        epochs (int): The number of epochs to train the model.\n",
        "        learning_rate (float): The learning rate for the optimizer.\n",
        "        gamma_lr (float): Factor by which the learning rate will be multiplied at each milestone.\n",
        "        milestones (list): List of epoch indices at which to adjust the learning rate.\n",
        "        save_path (str): Path to save the trained model state.\n",
        "\n",
        "    Returns:\n",
        "        train_losses (tuple): A tuple containing lists of training losses per epoch.\n",
        "        val_losses (tuple): A tuple containing lists of validation losses per epoch.\n",
        "    \"\"\"\n",
        "    optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=Weight_decay)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=milestones, gamma=gamma_lr)\n",
        "\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()  # Set model to training mode\n",
        "        train_loss = 0.0\n",
        "\n",
        "        # Training loop\n",
        "        for data, target in train_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            optimizer.zero_grad()  # Zero the parameter gradients\n",
        "            output = model(data)  # Forward pass\n",
        "            loss = criterion(output, target)  # Loss calculation\n",
        "            loss.backward()  # Backward pass (backpropagation)\n",
        "            optimizer.step()  # Optimize model parameters\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        # Store average training loss\n",
        "        train_losses.append(train_loss / len(train_loader))\n",
        "\n",
        "        # Validation loop\n",
        "        val_loss = 0.0\n",
        "        model.eval()  # Set model to evaluation mode\n",
        "        with torch.no_grad():\n",
        "            for data, target in val_loader:\n",
        "                data, target = data.to(device), target.to(device)\n",
        "                output = model(data)\n",
        "                val_loss += criterion(output, target).item()\n",
        "\n",
        "        # Store average validation loss\n",
        "        val_losses.append(val_loss / len(val_loader))\n",
        "\n",
        "        # Print epoch summary\n",
        "        print(f'Epoch {epoch + 1}/{epochs}, Train Loss: {train_losses[-1]:.4f}, Val Loss: {val_losses[-1]:.4f}')\n",
        "\n",
        "        scheduler.step()  # Adjust learning rate\n",
        "\n",
        "    # Save the trained model state\n",
        "    torch.save(model.state_dict(), save_path)\n",
        "\n",
        "    # Plot training and validation losses\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    plt.plot(train_losses, label='Training Loss')\n",
        "    plt.plot(val_losses, label='Validation Loss')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    return train_losses, val_losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "-JQ2XBx9RM5R"
      },
      "outputs": [],
      "source": [
        "def test_model(model: nn.Module, test_loader: DataLoader, load_path: str = 'vit_mnist.pth') -> None:\n",
        "    \"\"\"\n",
        "    Evaluates the model on the test dataset.\n",
        "\n",
        "    Parameters:\n",
        "        model (nn.Module): The neural network model to be evaluated.\n",
        "        test_loader (DataLoader): DataLoader for the test dataset.\n",
        "        load_path (str): Path to the file from which the model state is loaded.\n",
        "    \"\"\"\n",
        "    # Load the saved model state\n",
        "    model.load_state_dict(torch.load(load_path))\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    all_preds = []\n",
        "    all_targets = []\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    with torch.no_grad():  # No gradient calculation for inference\n",
        "        for data, target in test_loader:\n",
        "            # Move data and target to the same device as the model\n",
        "            data, target = data.to(device), target.to(device)\n",
        "\n",
        "            # Forward pass and loss calculation\n",
        "            output = model(data)\n",
        "            test_loss += criterion(output, target).item()\n",
        "\n",
        "            # Prediction and accuracy calculation\n",
        "            pred = output.argmax(dim=1, keepdim=True).squeeze()\n",
        "            correct += pred.eq(target).sum().item()\n",
        "\n",
        "            # Storing all predictions and targets for F1 score calculation\n",
        "            all_preds.extend(pred.tolist())\n",
        "            all_targets.extend(target.tolist())\n",
        "\n",
        "    # Calculate average test loss and accuracy\n",
        "    test_loss /= len(test_loader)\n",
        "    accuracy = correct / len(test_loader.dataset)\n",
        "\n",
        "    # Calculate F1 score\n",
        "    f1 = f1_score(all_targets, all_preds, average='weighted')\n",
        "\n",
        "    # Print results\n",
        "    print(f'\\n\\n Test set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} ({accuracy}%), F1 Score: {f1:.4f}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gSUg9KgaRM5S"
      },
      "source": [
        "## CIFAR10"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_losses, val_losses = train_model(resnet50, trainloader, validloader,\n",
        "                                       epochs=200, learning_rate=0.1, gamma_lr=0.2,\n",
        "                                       milestones=[60, 120,160], save_path='./resnet50_cifar10.pth', Weight_decay=5e-4)"
      ],
      "metadata": {
        "id": "NBEsh6KrWnBP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_model(resnet50, testloader , load_path=\"./resnet50_cifar10.pth\")"
      ],
      "metadata": {
        "id": "5UlMrxfuWo6s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omdKD4blRM5T"
      },
      "source": [
        "## CIFAR100"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_losses, val_losses = train_model(resnet50, trainloader100, validloader100,\n",
        "                                       epochs=200, learning_rate=0.1, gamma_lr=0.2,\n",
        "                                       milestones=[60, 120,160], save_path='./resnet50_cifar100.pth', Weight_decay=5e-4)"
      ],
      "metadata": {
        "id": "bLulr0VHS8xU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_model(resnet50, testloader100 , load_path=\"./resnet50_cifar100.pth\")"
      ],
      "metadata": {
        "id": "ZFRVUyATVILM"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "jBXHs2m0APxl",
        "NW9xd8Xc3UqB",
        "AoXdUp79d8CL",
        "XFwNb7Gf3UqR",
        "AL7gMgK93UqT",
        "cS3UXMDZwmM4",
        "OhKKLx5JAnzq",
        "6S-mVSFzeH0b",
        "a1WXC6ajA2jg",
        "5CcSbEhOiTst",
        "IpQxUPVV6VPH",
        "YAas1qBvIovm",
        "IPppy_FS52vd",
        "HYV-mZuBIqJa",
        "f8W_F8G-IzbY",
        "to_mOzrTI2VV",
        "FpQrqv19I5gC",
        "DsCg3LDgI9zK",
        "BV9SutwYJAKV",
        "QGjM8OySfpR1"
      ],
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}